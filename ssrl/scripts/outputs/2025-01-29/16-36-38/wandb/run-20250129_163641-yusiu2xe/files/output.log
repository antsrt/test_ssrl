run_name: null
algo: ssrl
gpus: '0'
seed: 0
env: ant2
ssrl_forces_in_q_coords: true
ssrl_dynamics_fn: contact_integrate_only
ssrl_model_probabilistic: true
env_ant2:
  backend: generalized
  terminate_when_unhealthy: true
env_hopper2:
  backend: generalized
  terminate_when_unhealthy: true
env_walker2d2:
  backend: generalized
  terminate_when_unhealthy: true
ssrl:
  episode_length: 1000
  policy_repeat: 1
  num_epochs: 80
  model_trains_per_epoch: 1
  training_steps_per_model_train: 1
  env_steps_per_training_step: 1000
  model_rollouts_per_hallucination_update: 400
  sac_grad_updates_per_hallucination_update: 20
  init_exploration_steps: 1000
  clear_model_buffer_after_model_train: true
  action_repeat: 1
  obs_history_length: 1
  num_envs: 1
  num_evals: 81
  num_eval_envs: 1
  policy_normalize_observations: false
  model_learning_rate: 0.001
  model_training_batch_size: 250
  model_training_max_epochs: 2000
  model_training_convergence_criteria: 0.01
  model_training_consec_converged_epochs: 6
  model_training_abs_criteria: null
  model_training_test_ratio: 0.2
  model_training_weight_decay: true
  model_training_stop_gradient: false
  model_loss_horizon: 1
  model_check_done_condition: true
  max_env_buffer_size: null
  max_model_buffer_size: null
  sac_learning_rate: 0.0003
  sac_discounting: 0.99
  sac_batch_size: 256
  real_ratio: 0.06
  sac_reward_scaling: 1.0
  sac_tau: 0.005
  sac_fixed_alpha: 0.2
  seed: ${seed}
  deterministic_in_env: false
  deterministic_eval: false
  hallucination_max_std: -1.0
  zero_final_layer_of_policy: false
ssrl_linear_threshold_fn:
  start_epoch: 20
  end_epoch: 150
  start_model_horizon: 1
  end_model_horizon: 15
ssrl_hupts_fn:
  start_epoch: 0
  end_epoch: 9
  start_hupts: 100
  end_hupts: 1000
wandb:
  entity: an-tsaritsin-itmo-university
  log: true
[2025-01-29 16:37:10,295][absl][INFO] - {'eval/walltime': 10.618884801864624, 'eval/episode_distance_from_origin': Array(44.507618, dtype=float32), 'eval/episode_forward_reward': Array(-14.939314, dtype=float32), 'eval/episode_reward': Array(-90.88228, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-126.58382, dtype=float32), 'eval/episode_reward_forward': Array(-14.939314, dtype=float32), 'eval/episode_reward_survive': Array(55., dtype=float32), 'eval/episode_x_position': Array(-4.207349, dtype=float32), 'eval/episode_x_velocity': Array(-14.939314, dtype=float32), 'eval/episode_y_position': Array(-2.4676032, dtype=float32), 'eval/episode_y_velocity': Array(-9.95795, dtype=float32), 'eval/avg_episode_length': Array(56., dtype=float32), 'eval/epoch_eval_time': 10.618884801864624, 'eval/sps': 94.17184748293012}
0 {'eval/walltime': 10.618884801864624, 'eval/episode_distance_from_origin': Array(44.507618, dtype=float32), 'eval/episode_forward_reward': Array(-14.939314, dtype=float32), 'eval/episode_reward': Array(-90.88228, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-126.58382, dtype=float32), 'eval/episode_reward_forward': Array(-14.939314, dtype=float32), 'eval/episode_reward_survive': Array(55., dtype=float32), 'eval/episode_x_position': Array(-4.207349, dtype=float32), 'eval/episode_x_velocity': Array(-14.939314, dtype=float32), 'eval/episode_y_position': Array(-2.4676032, dtype=float32), 'eval/episode_y_velocity': Array(-9.95795, dtype=float32), 'eval/avg_episode_length': Array(56., dtype=float32), 'eval/epoch_eval_time': 10.618884801864624, 'eval/sps': 94.17184748293012, 'steps': 0}
[2025-01-29 16:37:25,473][absl][INFO] - env buffer size after init exploration 1000
Model epoch 0: train total loss 199.50967407226562, train mean loss 18.1616268157959, test mean loss [nan nan nan nan nan nan nan]
Model epoch 1: train total loss 201.35935974121094, train mean loss 18.94137191772461, test mean loss [nan nan nan nan nan nan nan]
Model epoch 2: train total loss 178.01882934570312, train mean loss 18.661191940307617, test mean loss [nan nan nan nan nan nan nan]
Model epoch 3: train total loss 146.39425659179688, train mean loss 18.85413932800293, test mean loss [nan nan nan nan nan nan nan]
Model epoch 4: train total loss 119.95386505126953, train mean loss 18.08356285095215, test mean loss [nan nan nan nan nan nan nan]
Model epoch 5: train total loss 105.13086700439453, train mean loss 18.75701141357422, test mean loss [nan nan nan nan nan nan nan]
Model trained in 6 epochs with 1000 transitions.
[2025-01-29 16:38:31,716][absl][INFO] - {'eval/walltime': 13.88317084312439, 'training/sps': 15.879046386923362, 'training/walltime': 62.97607398033142, 'training/model_train_time': 36.14334797859192, 'training/other_time': 26.817777395248413, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 100, 'training/env_buffer_size': Array(2000, dtype=int32), 'model/train_total_loss': Array(105.13087, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(18.757011, dtype=float32), 'model/test_total_loss': Array(nan, dtype=float32), 'model/test_mean_loss': Array(nan, dtype=float32), 'model/train_epochs': 6, 'model/sec_per_epoch': 5.798494259516398, 'sac/actor_loss': Array(-0.29818133, dtype=float32), 'sac/alpha': Array(0.20000002, dtype=float32), 'sac/alpha_loss': Array(1.7752802, dtype=float32), 'sac/buffer_current_size': Array(21182., dtype=float32), 'sac/critic_loss': Array(1.0358174, dtype=float32), 'eval/episode_distance_from_origin': Array(17.450472, dtype=float32), 'eval/episode_forward_reward': Array(-7.075601, dtype=float32), 'eval/episode_reward': Array(-18.190481, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-35.562878, dtype=float32), 'eval/episode_reward_forward': Array(-7.075601, dtype=float32), 'eval/episode_reward_survive': Array(24., dtype=float32), 'eval/episode_x_position': Array(-4.201076, dtype=float32), 'eval/episode_x_velocity': Array(-7.075601, dtype=float32), 'eval/episode_y_position': Array(-4.5917087, dtype=float32), 'eval/episode_y_velocity': Array(-4.059867, dtype=float32), 'eval/avg_episode_length': Array(25., dtype=float32), 'eval/epoch_eval_time': 3.2642860412597656, 'eval/sps': 306.3457023558132}
2000.0 {'eval/walltime': 13.88317084312439, 'training/sps': 15.879046386923362, 'training/walltime': 62.97607398033142, 'training/model_train_time': 36.14334797859192, 'training/other_time': 26.817777395248413, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 100, 'training/env_buffer_size': Array(2000, dtype=int32), 'model/train_total_loss': Array(105.13087, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(18.757011, dtype=float32), 'model/test_total_loss': Array(nan, dtype=float32), 'model/test_mean_loss': Array(nan, dtype=float32), 'model/train_epochs': 6, 'model/sec_per_epoch': 5.798494259516398, 'sac/actor_loss': Array(-0.29818133, dtype=float32), 'sac/alpha': Array(0.20000002, dtype=float32), 'sac/alpha_loss': Array(1.7752802, dtype=float32), 'sac/buffer_current_size': Array(21182., dtype=float32), 'sac/critic_loss': Array(1.0358174, dtype=float32), 'eval/episode_distance_from_origin': Array(17.450472, dtype=float32), 'eval/episode_forward_reward': Array(-7.075601, dtype=float32), 'eval/episode_reward': Array(-18.190481, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-35.562878, dtype=float32), 'eval/episode_reward_forward': Array(-7.075601, dtype=float32), 'eval/episode_reward_survive': Array(24., dtype=float32), 'eval/episode_x_position': Array(-4.201076, dtype=float32), 'eval/episode_x_velocity': Array(-7.075601, dtype=float32), 'eval/episode_y_position': Array(-4.5917087, dtype=float32), 'eval/episode_y_velocity': Array(-4.059867, dtype=float32), 'eval/avg_episode_length': Array(25., dtype=float32), 'eval/epoch_eval_time': 3.2642860412597656, 'eval/sps': 306.3457023558132, 'steps': Array(2000., dtype=float32)}
Model horizon updated to 1.
Hallucination updates per training step updated to 200.
SAC buffer resized to 80000 samples.
Model epoch 0: train total loss 78.06838989257812, train mean loss 17.558094024658203, test mean loss [18.073368 18.303543 18.206598 18.122692 18.244053 18.063229 18.354921]
Model epoch 1: train total loss 71.15892791748047, train mean loss 17.403093338012695, test mean loss [17.895191 18.117125 18.02347  17.870337 18.015574 17.83096  18.199049]
Model epoch 2: train total loss 68.09168243408203, train mean loss 17.20763397216797, test mean loss [17.657963 17.93649  17.806099 17.659683 17.755644 17.658009 18.025473]
Model epoch 3: train total loss 65.66629791259766, train mean loss 17.005958557128906, test mean loss [17.357805 17.75609  17.52751  17.48093  17.455412 17.367868 17.844725]
Model epoch 4: train total loss 63.775177001953125, train mean loss 16.705692291259766, test mean loss [16.92805  17.53731  17.172352 17.182022 17.119081 17.036352 17.655277]
Model epoch 5: train total loss 61.14923095703125, train mean loss 16.372493743896484, test mean loss [16.384604 17.301939 16.725971 16.81853  16.769093 16.549353 17.475271]
Model epoch 6: train total loss 58.824378967285156, train mean loss 15.954078674316406, test mean loss [15.787664 17.03083  16.183216 16.367315 16.276081 15.894824 17.271818]
Model epoch 7: train total loss 56.16267776489258, train mean loss 15.375846862792969, test mean loss [14.940541 16.715359 15.522599 15.72803  15.624207 15.051673 17.041723]
Model epoch 8: train total loss 53.75897216796875, train mean loss 14.882448196411133, test mean loss [13.784443 16.289965 14.673858 14.842787 14.801876 13.90427  16.847427]
Model epoch 9: train total loss 50.16065979003906, train mean loss 14.007533073425293, test mean loss [12.88044  15.812743 13.658924 13.842039 13.847854 13.049491 16.613987]
Model epoch 10: train total loss 49.307071685791016, train mean loss 13.773823738098145, test mean loss [12.379812 15.180612 12.790429 13.045495 12.99415  12.549829 16.31658 ]
Model epoch 11: train total loss 47.368499755859375, train mean loss 13.421411514282227, test mean loss [12.37277  14.409328 12.458171 12.765375 12.585895 12.214524 15.976803]
Model epoch 12: train total loss 45.79194259643555, train mean loss 13.148721694946289, test mean loss [12.00447   13.631129  12.265178  12.46653   12.4270115 12.125407
 15.566612 ]
Model epoch 13: train total loss 45.42020034790039, train mean loss 13.191827774047852, test mean loss [12.004841 13.059826 12.060381 12.29325  12.477511 11.990297 15.068219]
Model epoch 14: train total loss 44.09894943237305, train mean loss 12.805682182312012, test mean loss [11.8605175 12.636959  11.946962  12.108275  12.302832  11.952368
 14.498711 ]
Model epoch 15: train total loss 43.13470458984375, train mean loss 12.669022560119629, test mean loss [11.855997 12.388459 11.884162 12.126391 12.01146  11.793    13.978878]
Model epoch 16: train total loss 42.66206741333008, train mean loss 12.426411628723145, test mean loss [11.760435 12.182618 11.811238 12.062951 11.898868 11.976245 13.352843]
Model epoch 17: train total loss 41.87236785888672, train mean loss 12.425286293029785, test mean loss [11.685838 12.104423 11.75672  11.942237 11.847955 11.778053 12.916805]
Model epoch 18: train total loss 41.59550857543945, train mean loss 12.367852210998535, test mean loss [11.714741 11.982724 11.748935 11.798585 11.834911 11.723918 12.649159]
Model epoch 19: train total loss 41.22132873535156, train mean loss 12.297698020935059, test mean loss [11.67029  11.873701 11.771886 11.779599 11.82132  11.688963 12.429967]
Model epoch 20: train total loss 41.092628479003906, train mean loss 12.221243858337402, test mean loss [11.665832 11.825944 11.702667 11.755641 11.797652 11.764347 12.302208]
Model epoch 21: train total loss 41.34356689453125, train mean loss 12.234251022338867, test mean loss [11.620029 11.861463 11.716329 11.747223 11.752752 11.69288  12.170435]
Model epoch 22: train total loss 41.49842071533203, train mean loss 12.253178596496582, test mean loss [11.614597  11.873062  11.699226  11.7490425 11.732401  11.664524
 12.073833 ]
Model epoch 23: train total loss 40.39098358154297, train mean loss 12.147077560424805, test mean loss [11.592879 11.745668 11.721672 11.757336 11.669042 11.769404 11.967724]
Model epoch 24: train total loss 40.373958587646484, train mean loss 12.136048316955566, test mean loss [11.585207 11.746869 11.632259 11.7542   11.652565 11.813678 11.941508]
Model epoch 25: train total loss 40.369171142578125, train mean loss 12.204144477844238, test mean loss [11.603395  11.7601385 11.66732   11.720383  11.637917  11.830486
 11.843117 ]
Model epoch 26: train total loss 40.27056884765625, train mean loss 12.169584274291992, test mean loss [11.580609  11.758625  11.6729145 11.703657  11.628028  11.750178
 11.949527 ]
Model epoch 27: train total loss 39.919578552246094, train mean loss 12.117646217346191, test mean loss [11.616284 11.666555 11.617547 11.692411 11.652993 11.727199 11.890587]
Model epoch 28: train total loss 39.80360794067383, train mean loss 12.10595989227295, test mean loss [11.578673 11.682072 11.656512 11.732208 11.597664 11.72468  11.778847]
Model epoch 29: train total loss 39.6470947265625, train mean loss 12.084859848022461, test mean loss [11.582314 11.687294 11.652016 11.733976 11.605226 11.750531 11.712727]
Model epoch 30: train total loss 39.60606002807617, train mean loss 12.090075492858887, test mean loss [11.607967 11.617434 11.603564 11.732275 11.665072 11.737877 11.719443]
Model epoch 31: train total loss 39.466007232666016, train mean loss 12.082486152648926, test mean loss [11.6762085 11.639373  11.622565  11.7457285 11.601435  11.634754
 11.721082 ]
Model epoch 32: train total loss 39.34653091430664, train mean loss 12.04821491241455, test mean loss [11.6421795 11.615488  11.657601  11.7154    11.611824  11.625109
 11.676866 ]
Model epoch 33: train total loss 39.38277053833008, train mean loss 12.06029987335205, test mean loss [11.610786 11.618843 11.612713 11.727555 11.637739 11.655457 11.696278]
Model epoch 34: train total loss 39.33894348144531, train mean loss 12.069300651550293, test mean loss [11.605266 11.583366 11.655629 11.712803 11.665908 11.751106 11.656686]
Model epoch 35: train total loss 39.336700439453125, train mean loss 12.064749717712402, test mean loss [11.626229 11.5885   11.698196 11.696162 11.669718 11.638968 11.624621]
Model trained in 36 epochs with 2000 transitions.
[2025-01-29 16:39:04,927][absl][INFO] - {'eval/walltime': 17.159133672714233, 'training/sps': 33.45872799279852, 'training/walltime': 92.8636417388916, 'training/model_train_time': 7.095257997512817, 'training/other_time': 22.788010835647583, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 200, 'training/env_buffer_size': Array(3000, dtype=int32), 'model/train_total_loss': Array(39.3367, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(12.06475, dtype=float32), 'model/test_total_loss': Array(37.562122, dtype=float32), 'model/test_mean_loss': Array(11.648914, dtype=float32), 'model/train_epochs': 36, 'model/sec_per_epoch': 0.14721942610210842, 'sac/actor_loss': Array(-17.519108, dtype=float32), 'sac/alpha': Array(0.20000003, dtype=float32), 'sac/alpha_loss': Array(0.45199484, dtype=float32), 'sac/buffer_current_size': Array(41191., dtype=float32), 'sac/critic_loss': Array(14.399756, dtype=float32), 'eval/episode_distance_from_origin': Array(50.695217, dtype=float32), 'eval/episode_forward_reward': Array(-15.266896, dtype=float32), 'eval/episode_reward': Array(-128.51991, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-154.59996, dtype=float32), 'eval/episode_reward_forward': Array(-15.266896, dtype=float32), 'eval/episode_reward_survive': Array(45., dtype=float32), 'eval/episode_x_position': Array(7.5884457, dtype=float32), 'eval/episode_x_velocity': Array(-15.266896, dtype=float32), 'eval/episode_y_position': Array(31.280785, dtype=float32), 'eval/episode_y_velocity': Array(21.985239, dtype=float32), 'eval/avg_episode_length': Array(46., dtype=float32), 'eval/epoch_eval_time': 3.2759628295898438, 'eval/sps': 305.2537687447454}
3000.0 {'eval/walltime': 17.159133672714233, 'training/sps': 33.45872799279852, 'training/walltime': 92.8636417388916, 'training/model_train_time': 7.095257997512817, 'training/other_time': 22.788010835647583, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 200, 'training/env_buffer_size': Array(3000, dtype=int32), 'model/train_total_loss': Array(39.3367, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(12.06475, dtype=float32), 'model/test_total_loss': Array(37.562122, dtype=float32), 'model/test_mean_loss': Array(11.648914, dtype=float32), 'model/train_epochs': 36, 'model/sec_per_epoch': 0.14721942610210842, 'sac/actor_loss': Array(-17.519108, dtype=float32), 'sac/alpha': Array(0.20000003, dtype=float32), 'sac/alpha_loss': Array(0.45199484, dtype=float32), 'sac/buffer_current_size': Array(41191., dtype=float32), 'sac/critic_loss': Array(14.399756, dtype=float32), 'eval/episode_distance_from_origin': Array(50.695217, dtype=float32), 'eval/episode_forward_reward': Array(-15.266896, dtype=float32), 'eval/episode_reward': Array(-128.51991, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-154.59996, dtype=float32), 'eval/episode_reward_forward': Array(-15.266896, dtype=float32), 'eval/episode_reward_survive': Array(45., dtype=float32), 'eval/episode_x_position': Array(7.5884457, dtype=float32), 'eval/episode_x_velocity': Array(-15.266896, dtype=float32), 'eval/episode_y_position': Array(31.280785, dtype=float32), 'eval/episode_y_velocity': Array(21.985239, dtype=float32), 'eval/avg_episode_length': Array(46., dtype=float32), 'eval/epoch_eval_time': 3.2759628295898438, 'eval/sps': 305.2537687447454, 'steps': Array(3000., dtype=float32)}
Model horizon updated to 1.
Hallucination updates per training step updated to 300.
SAC buffer resized to 120000 samples.
Model epoch 0: train total loss 35.19817352294922, train mean loss 11.037934303283691, test mean loss [10.791322 10.933453 10.866415 10.874586 10.920057 10.875599 10.952871]
Model epoch 1: train total loss 33.31565475463867, train mean loss 10.61652946472168, test mean loss [10.812273  10.907739  10.83239   10.921282  10.8984165 10.852188
 10.902044 ]
Model epoch 2: train total loss 34.0223388671875, train mean loss 10.80675983428955, test mean loss [10.861511  10.906607  10.8605175 10.875756  11.063361  10.923376
 10.890908 ]
Model epoch 3: train total loss 33.69422149658203, train mean loss 10.73486042022705, test mean loss [10.823343 10.927883 10.829172 10.87027  11.003524 10.89448  10.894632]
Model epoch 4: train total loss 32.88080978393555, train mean loss 10.57844352722168, test mean loss [10.860356 10.891441 10.874886 10.902344 10.958195 10.892065 10.929958]
Model epoch 5: train total loss 33.99921417236328, train mean loss 10.87236213684082, test mean loss [10.891996  10.924916  10.9248905 10.889698  10.9628525 10.892755
 10.879145 ]
Model epoch 6: train total loss 35.73796463012695, train mean loss 11.217013359069824, test mean loss [10.846735 10.875543 10.87112  10.962486 10.985964 10.921131 10.882622]
Model trained in 7 epochs with 3000 transitions.
[2025-01-29 16:39:36,747][absl][INFO] - {'eval/walltime': 20.43622922897339, 'training/sps': 35.08780911821725, 'training/walltime': 121.3635687828064, 'training/model_train_time': 4.846778154373169, 'training/other_time': 23.649479150772095, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 300, 'training/env_buffer_size': Array(4000, dtype=int32), 'model/train_total_loss': Array(35.737965, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(11.217013, dtype=float32), 'model/test_total_loss': Array(34.235916, dtype=float32), 'model/test_mean_loss': Array(10.906515, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.37964037486485075, 'sac/actor_loss': Array(-6485.4253, dtype=float32), 'sac/alpha': Array(0.2, dtype=float32), 'sac/alpha_loss': Array(-6.318407, dtype=float32), 'sac/buffer_current_size': Array(61194., dtype=float32), 'sac/critic_loss': Array(1884065.4, dtype=float32), 'eval/episode_distance_from_origin': Array(53.903107, dtype=float32), 'eval/episode_forward_reward': Array(3.5046506, dtype=float32), 'eval/episode_reward': Array(-191.51941, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-254.70016, dtype=float32), 'eval/episode_reward_forward': Array(3.5046506, dtype=float32), 'eval/episode_reward_survive': Array(64., dtype=float32), 'eval/episode_x_position': Array(23.603905, dtype=float32), 'eval/episode_x_velocity': Array(3.5046506, dtype=float32), 'eval/episode_y_position': Array(-5.2860923, dtype=float32), 'eval/episode_y_velocity': Array(-0.90491635, dtype=float32), 'eval/avg_episode_length': Array(65., dtype=float32), 'eval/epoch_eval_time': 3.2770955562591553, 'eval/sps': 305.1482579108899}
4000.0 {'eval/walltime': 20.43622922897339, 'training/sps': 35.08780911821725, 'training/walltime': 121.3635687828064, 'training/model_train_time': 4.846778154373169, 'training/other_time': 23.649479150772095, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 300, 'training/env_buffer_size': Array(4000, dtype=int32), 'model/train_total_loss': Array(35.737965, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(11.217013, dtype=float32), 'model/test_total_loss': Array(34.235916, dtype=float32), 'model/test_mean_loss': Array(10.906515, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.37964037486485075, 'sac/actor_loss': Array(-6485.4253, dtype=float32), 'sac/alpha': Array(0.2, dtype=float32), 'sac/alpha_loss': Array(-6.318407, dtype=float32), 'sac/buffer_current_size': Array(61194., dtype=float32), 'sac/critic_loss': Array(1884065.4, dtype=float32), 'eval/episode_distance_from_origin': Array(53.903107, dtype=float32), 'eval/episode_forward_reward': Array(3.5046506, dtype=float32), 'eval/episode_reward': Array(-191.51941, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-254.70016, dtype=float32), 'eval/episode_reward_forward': Array(3.5046506, dtype=float32), 'eval/episode_reward_survive': Array(64., dtype=float32), 'eval/episode_x_position': Array(23.603905, dtype=float32), 'eval/episode_x_velocity': Array(3.5046506, dtype=float32), 'eval/episode_y_position': Array(-5.2860923, dtype=float32), 'eval/episode_y_velocity': Array(-0.90491635, dtype=float32), 'eval/avg_episode_length': Array(65., dtype=float32), 'eval/epoch_eval_time': 3.2770955562591553, 'eval/sps': 305.1482579108899, 'steps': Array(4000., dtype=float32)}
Model horizon updated to 1.
Hallucination updates per training step updated to 400.
SAC buffer resized to 160000 samples.
Model epoch 0: train total loss 37.018402099609375, train mean loss 11.652960777282715, test mean loss [12.713912 12.765083 12.702166 12.718677 12.677875 12.828878 12.766563]
Model epoch 1: train total loss 38.59793472290039, train mean loss 12.037993431091309, test mean loss [12.479991 12.508003 12.488804 12.467471 12.441414 12.484656 12.536541]
Model epoch 2: train total loss 37.32142639160156, train mean loss 11.713109970092773, test mean loss [12.264387 12.297881 12.28551  12.245264 12.265387 12.267375 12.361944]
Model epoch 3: train total loss 36.40847396850586, train mean loss 11.56404972076416, test mean loss [12.062695 12.125792 12.101497 12.055094 12.077415 12.101772 12.189367]
Model epoch 4: train total loss 34.58546829223633, train mean loss 11.189606666564941, test mean loss [11.95095  11.959243 11.982341 11.88797  11.93323  11.951087 12.039012]
Model epoch 5: train total loss 35.93415069580078, train mean loss 11.490575790405273, test mean loss [11.860573 11.804537 11.857593 11.786629 11.824625 11.831813 11.922804]
Model epoch 6: train total loss 33.07077407836914, train mean loss 10.868070602416992, test mean loss [11.762388  11.724401  11.749409  11.682409  11.7348585 11.7361355
 11.818793 ]
Model epoch 7: train total loss 33.924076080322266, train mean loss 11.052338600158691, test mean loss [11.706749 11.592989 11.671703 11.62694  11.650848 11.669134 11.73514 ]
Model epoch 8: train total loss 32.70551300048828, train mean loss 10.791616439819336, test mean loss [11.651646 11.491901 11.577079 11.57902  11.584496 11.595074 11.685017]
Model epoch 9: train total loss 35.071685791015625, train mean loss 11.294283866882324, test mean loss [11.570704 11.353392 11.504089 11.51958  11.518295 11.534659 11.621298]
Model epoch 10: train total loss 30.40518569946289, train mean loss 10.259767532348633, test mean loss [11.497638 11.19714  11.409895 11.463453 11.420652 11.407896 11.566592]
Model epoch 11: train total loss 33.203678131103516, train mean loss 10.878819465637207, test mean loss [11.3932495 10.930308  11.284294  11.399501  11.298368  11.30443
 11.493941 ]
Model epoch 12: train total loss 32.923763275146484, train mean loss 10.873520851135254, test mean loss [11.26494  10.539726 11.145846 11.259719 11.137785 11.079611 11.413742]
Model epoch 13: train total loss 30.61335563659668, train mean loss 10.282907485961914, test mean loss [11.070906 10.059025 10.904506 11.076727 10.844996 10.79557  11.290973]
Model epoch 14: train total loss 27.665863037109375, train mean loss 9.58979606628418, test mean loss [10.760788  9.588832 10.497841 10.824827 10.420769 10.331698 11.0823  ]
Model epoch 15: train total loss 27.933513641357422, train mean loss 9.669967651367188, test mean loss [10.270077  9.153981  9.799235 10.462906  9.616088  9.708212 10.85688 ]
Model epoch 16: train total loss 21.748258590698242, train mean loss 8.213197708129883, test mean loss [ 9.483897  8.869235  9.083307  9.979666  8.882872  8.948879 10.559123]
Model epoch 17: train total loss 23.25944709777832, train mean loss 8.557929039001465, test mean loss [ 8.87462   8.690626  8.625505  9.392769  8.509798  8.568873 10.173847]
Model epoch 18: train total loss 21.53479766845703, train mean loss 8.212186813354492, test mean loss [8.59064   8.574842  8.470943  9.005524  8.43964   8.5142145 9.642565 ]
Model epoch 19: train total loss 21.343597412109375, train mean loss 8.165587425231934, test mean loss [8.480472  8.495897  8.503105  8.6957    8.4650345 8.472912  9.192167 ]
Model epoch 20: train total loss 21.801183700561523, train mean loss 8.324198722839355, test mean loss [8.420947 8.490887 8.460508 8.56786  8.430194 8.42417  8.904089]
Model epoch 21: train total loss 21.083864212036133, train mean loss 8.18610668182373, test mean loss [8.416336 8.449996 8.461573 8.527647 8.399243 8.409801 8.711256]
Model epoch 22: train total loss 19.787385940551758, train mean loss 7.917535781860352, test mean loss [8.389326  8.4357    8.41974   8.48331   8.398262  8.378155  8.6312475]
Model epoch 23: train total loss 18.453065872192383, train mean loss 7.61374568939209, test mean loss [8.372746 8.384643 8.478828 8.431267 8.373344 8.355586 8.570166]
Model epoch 24: train total loss 21.759279251098633, train mean loss 8.371908187866211, test mean loss [8.358793  8.40107   8.3906975 8.397336  8.364532  8.374033  8.534811 ]
Model epoch 25: train total loss 21.813791275024414, train mean loss 8.386838912963867, test mean loss [8.321292 8.363636 8.404429 8.360328 8.377064 8.342143 8.490407]
Model epoch 26: train total loss 19.98203468322754, train mean loss 8.04365062713623, test mean loss [8.346753 8.357292 8.413708 8.366064 8.33135  8.329964 8.427746]
Model epoch 27: train total loss 20.238216400146484, train mean loss 8.112979888916016, test mean loss [8.339942 8.358764 8.382106 8.357403 8.347185 8.333587 8.390636]
Model epoch 28: train total loss 19.897933959960938, train mean loss 8.09793758392334, test mean loss [8.338743 8.336679 8.315937 8.3203   8.36251  8.323627 8.386581]
Model epoch 29: train total loss 19.38517189025879, train mean loss 7.947482109069824, test mean loss [8.294495  8.3210125 8.342872  8.336063  8.334063  8.293143  8.370341 ]
Model epoch 30: train total loss 18.039501190185547, train mean loss 7.669951438903809, test mean loss [8.264395 8.306978 8.334942 8.304955 8.300108 8.325369 8.371687]
Model epoch 31: train total loss 18.44901466369629, train mean loss 7.793084621429443, test mean loss [8.2871275 8.338966  8.285479  8.293171  8.281565  8.306896  8.358839 ]
Model epoch 32: train total loss 19.015348434448242, train mean loss 7.8836188316345215, test mean loss [8.217799  8.298839  8.3280525 8.291065  8.32605   8.289045  8.361242 ]
Model epoch 33: train total loss 16.489498138427734, train mean loss 7.3750200271606445, test mean loss [8.218197  8.291712  8.313434  8.276645  8.322492  8.2987175 8.306506 ]
Model epoch 34: train total loss 18.289493560791016, train mean loss 7.76196813583374, test mean loss [8.252425 8.323626 8.306063 8.258291 8.292019 8.289683 8.282822]
Model epoch 35: train total loss 19.264114379882812, train mean loss 7.968750476837158, test mean loss [8.218441 8.271816 8.296183 8.263508 8.259194 8.274268 8.274588]
Model epoch 36: train total loss 18.261974334716797, train mean loss 7.786362171173096, test mean loss [8.215519  8.27      8.296801  8.25997   8.2756405 8.245646  8.270172 ]
Model epoch 37: train total loss 18.576263427734375, train mean loss 7.832767963409424, test mean loss [8.187439  8.255403  8.310505  8.262771  8.295126  8.2256155 8.246413 ]
Model epoch 38: train total loss 17.18024444580078, train mean loss 7.594972133636475, test mean loss [8.178841 8.253693 8.270271 8.257767 8.257475 8.245209 8.296892]
Model epoch 39: train total loss 16.952457427978516, train mean loss 7.5462446212768555, test mean loss [8.163407 8.305072 8.277916 8.231701 8.247916 8.240051 8.245594]
Model epoch 40: train total loss 17.702823638916016, train mean loss 7.687383651733398, test mean loss [8.203386  8.239014  8.248646  8.295105  8.250608  8.2267685 8.243178 ]
Model epoch 41: train total loss 19.280838012695312, train mean loss 8.029097557067871, test mean loss [8.168362 8.256304 8.253861 8.220405 8.237485 8.274714 8.204357]
Model epoch 42: train total loss 17.278310775756836, train mean loss 7.605961799621582, test mean loss [8.138763 8.234954 8.234259 8.233181 8.235674 8.220083 8.259302]
Model epoch 43: train total loss 18.183439254760742, train mean loss 7.823378562927246, test mean loss [8.147487 8.252977 8.226455 8.219496 8.193937 8.234655 8.199232]
Model epoch 44: train total loss 18.428857803344727, train mean loss 7.933117389678955, test mean loss [8.149689  8.2005825 8.29149   8.214719  8.185431  8.212701  8.218437 ]
Model epoch 45: train total loss 17.493452072143555, train mean loss 7.681225776672363, test mean loss [8.107585 8.246248 8.220198 8.214661 8.196961 8.209379 8.183281]
Model epoch 46: train total loss 18.874128341674805, train mean loss 7.964597702026367, test mean loss [8.094314 8.204153 8.234832 8.241331 8.184436 8.212229 8.180037]
Model epoch 47: train total loss 18.81694793701172, train mean loss 8.01819896697998, test mean loss [8.084456 8.19322  8.192504 8.201384 8.196749 8.203888 8.187308]
Model epoch 48: train total loss 17.61789894104004, train mean loss 7.696316242218018, test mean loss [8.029989 8.178449 8.211434 8.160111 8.171986 8.237815 8.18965 ]
Model epoch 49: train total loss 17.128190994262695, train mean loss 7.640458583831787, test mean loss [7.9901257 8.145629  8.200272  8.192438  8.1827965 8.182144  8.19031  ]
Model epoch 50: train total loss 18.547603607177734, train mean loss 7.9322710037231445, test mean loss [7.953096 8.175938 8.176585 8.15396  8.18058  8.174834 8.183173]
Model epoch 51: train total loss 16.65188217163086, train mean loss 7.509504318237305, test mean loss [7.876236 8.127184 8.164547 8.154186 8.182508 8.193195 8.17856 ]
Model epoch 52: train total loss 15.710579872131348, train mean loss 7.316259860992432, test mean loss [7.8258834 8.153309  8.155646  8.105276  8.212739  8.1555395 8.164417 ]
Model epoch 53: train total loss 16.841650009155273, train mean loss 7.556213855743408, test mean loss [7.7299285 8.122455  8.160688  8.085848  8.152862  8.150652  8.156295 ]
Model epoch 54: train total loss 16.53000259399414, train mean loss 7.542743682861328, test mean loss [7.651408  8.08398   8.166715  8.094645  8.159504  8.148715  8.1404505]
Model epoch 55: train total loss 16.239587783813477, train mean loss 7.443055152893066, test mean loss [7.609718 8.041834 8.206226 8.072296 8.103196 8.137718 8.130876]
Model epoch 56: train total loss 14.5861234664917, train mean loss 7.054742813110352, test mean loss [7.5778303 8.02825   8.162964  8.00901   8.096554  8.11107   8.197704 ]
Model epoch 57: train total loss 14.968188285827637, train mean loss 7.156365871429443, test mean loss [7.5073414 8.013183  8.160144  7.9735575 8.085535  8.169923  8.127554 ]
Model epoch 58: train total loss 14.214659690856934, train mean loss 6.975618839263916, test mean loss [7.470763  7.9397497 8.147323  7.9342804 8.117656  8.122547  8.11988  ]
Model epoch 59: train total loss 17.29347038269043, train mean loss 7.693163871765137, test mean loss [7.4665923 7.900264  8.11305   7.8529677 8.104996  8.092077  8.135946 ]
Model epoch 60: train total loss 16.252700805664062, train mean loss 7.477053642272949, test mean loss [7.4142094 7.8167686 8.112522  7.7324486 8.067234  8.112726  8.115933 ]
Model epoch 61: train total loss 16.612504959106445, train mean loss 7.560817718505859, test mean loss [7.378026  7.6827254 8.128573  7.6169715 8.040841  8.116782  8.107645 ]
Model epoch 62: train total loss 14.86540699005127, train mean loss 7.190835952758789, test mean loss [7.382179  7.5271864 8.146759  7.546779  8.032334  8.009333  8.124318 ]
Model epoch 63: train total loss 16.201265335083008, train mean loss 7.436350345611572, test mean loss [7.3428974 7.3659797 8.140441  7.4546638 8.014979  8.005526  8.085125 ]
Model epoch 64: train total loss 17.764633178710938, train mean loss 7.823349475860596, test mean loss [7.32903   7.260452  8.070345  7.367389  7.9667654 7.977817  8.102667 ]
Model epoch 65: train total loss 15.142483711242676, train mean loss 7.23180627822876, test mean loss [7.286588  7.206472  8.058404  7.3297057 7.975036  7.912243  8.085665 ]
Model epoch 66: train total loss 14.427947044372559, train mean loss 7.081128120422363, test mean loss [7.2682395 7.137394  8.12344   7.258383  7.936944  7.846963  8.082422 ]
Model epoch 67: train total loss 14.295964241027832, train mean loss 7.065079689025879, test mean loss [7.2550135 7.1493034 8.000126  7.27281   7.841766  7.789096  8.068337 ]
Model epoch 68: train total loss 13.933326721191406, train mean loss 6.969776153564453, test mean loss [7.2064676 7.085251  8.01141   7.267785  7.8567214 7.6605105 8.062748 ]
Model epoch 69: train total loss 14.147422790527344, train mean loss 7.024600028991699, test mean loss [7.1562376 7.067786  7.979316  7.205386  7.711712  7.5951915 8.085575 ]
Model epoch 70: train total loss 12.46381664276123, train mean loss 6.636235237121582, test mean loss [7.1352305 7.0533156 7.8528724 7.172353  7.649671  7.5737295 8.023356 ]
Model epoch 71: train total loss 13.936516761779785, train mean loss 7.006821155548096, test mean loss [7.012989  7.0578785 7.7591987 7.164077  7.534627  7.433893  8.018921 ]
Model epoch 72: train total loss 13.728324890136719, train mean loss 6.967398643493652, test mean loss [6.9555016 7.050466  7.6503    7.0861425 7.446086  7.373691  8.000183 ]
Model epoch 73: train total loss 12.100035667419434, train mean loss 6.552619457244873, test mean loss [6.8252363 6.9955115 7.4712806 7.034616  7.3013434 7.3057137 8.011978 ]
Model epoch 74: train total loss 13.574043273925781, train mean loss 6.905864715576172, test mean loss [6.7117133 6.992706  7.2666273 7.0718017 7.101957  7.223417  7.965572 ]
Model epoch 75: train total loss 11.76016902923584, train mean loss 6.519570827484131, test mean loss [6.5951443 6.9691634 7.166874  7.0353985 6.8399825 7.1579657 7.971456 ]
Model epoch 76: train total loss 13.334185600280762, train mean loss 6.851614475250244, test mean loss [6.398351  6.95444   7.047273  7.0007486 6.654375  7.0639277 7.956517 ]
Model epoch 77: train total loss 10.576728820800781, train mean loss 6.2350873947143555, test mean loss [6.2288055 6.9590297 6.982624  6.898223  6.397545  6.991094  7.976635 ]
Model epoch 78: train total loss 10.679109573364258, train mean loss 6.224014759063721, test mean loss [6.1386976 6.8913817 6.926463  6.8238096 6.1872897 6.877275  7.87124  ]
Model epoch 79: train total loss 9.642829895019531, train mean loss 6.045155048370361, test mean loss [6.0637007 6.861912  6.84308   6.731518  6.008262  6.793353  7.7934647]
Model epoch 80: train total loss 9.90351390838623, train mean loss 6.013688564300537, test mean loss [6.035946  6.905079  6.740239  6.613189  5.909538  6.616295  7.7353706]
Model epoch 81: train total loss 9.647648811340332, train mean loss 5.979317665100098, test mean loss [5.9689097 6.8155127 6.621499  6.524496  5.858962  6.466135  7.6620383]
Model epoch 82: train total loss 9.469452857971191, train mean loss 6.010186672210693, test mean loss [5.9349527 6.7220078 6.534362  6.3066115 5.751142  6.2581043 7.552541 ]
Model epoch 83: train total loss 8.254950523376465, train mean loss 5.726873397827148, test mean loss [5.970781  6.7234383 6.3896065 6.1084404 5.6860614 6.1300993 7.4483604]
Model epoch 84: train total loss 8.287983894348145, train mean loss 5.657645225524902, test mean loss [5.9339705 6.655357  6.32081   6.02972   5.6182    6.0023246 7.3026843]
Model epoch 85: train total loss 6.530077934265137, train mean loss 5.317746162414551, test mean loss [5.834983  6.608184  6.2227874 5.9142084 5.62717   5.986044  7.182874 ]
Model epoch 86: train total loss 6.463070869445801, train mean loss 5.267067909240723, test mean loss [5.792215  6.5607424 6.1902084 5.854065  5.5581045 5.918069  7.021536 ]
Model epoch 87: train total loss 7.125794410705566, train mean loss 5.399206161499023, test mean loss [5.797178  6.4546776 6.1266685 5.803521  5.5672417 5.877029  6.951353 ]
Model epoch 88: train total loss 6.511954307556152, train mean loss 5.289257049560547, test mean loss [5.756892  6.3959646 5.995102  5.744466  5.5882215 5.825101  6.890104 ]
Model epoch 89: train total loss 7.708237171173096, train mean loss 5.589139938354492, test mean loss [5.763172  6.3058887 5.9797664 5.7299757 5.51842   5.8387365 6.8329616]
Model epoch 90: train total loss 5.5530900955200195, train mean loss 5.1444268226623535, test mean loss [5.7751746 6.2597575 5.8865347 5.6482763 5.503113  5.744699  6.6990166]
Model epoch 91: train total loss 5.8950018882751465, train mean loss 5.179532051086426, test mean loss [5.6600914 6.143799  5.842069  5.768467  5.4582815 5.7097073 6.6221523]
Model epoch 92: train total loss 5.122500896453857, train mean loss 5.010904788970947, test mean loss [5.6450233 6.0786147 5.8186064 5.673251  5.46182   5.643648  6.4565096]
Model epoch 93: train total loss 4.902512073516846, train mean loss 4.963422775268555, test mean loss [5.651578  5.948945  5.8166933 5.538414  5.430708  5.6648345 6.291931 ]
Model epoch 94: train total loss 4.288154602050781, train mean loss 4.854147434234619, test mean loss [5.604905  5.829508  5.7666454 5.4081116 5.4456763 5.6820135 6.190664 ]
Model epoch 95: train total loss 4.540593147277832, train mean loss 4.8864641189575195, test mean loss [5.5949125 5.7070155 5.752081  5.387185  5.3852706 5.647777  5.938669 ]
Model epoch 96: train total loss 5.128387928009033, train mean loss 5.002688884735107, test mean loss [5.594388  5.63656   5.74561   5.319489  5.3631086 5.6535525 5.7962265]
Model epoch 97: train total loss 3.4502251148223877, train mean loss 4.6532816886901855, test mean loss [5.571926  5.604561  5.7390738 5.2645016 5.347213  5.616336  5.668003 ]
Model epoch 98: train total loss 4.264024257659912, train mean loss 4.723474025726318, test mean loss [5.4974785 5.523299  5.7005897 5.2383337 5.3133593 5.5759344 5.605976 ]
Model epoch 99: train total loss 4.310731887817383, train mean loss 4.846942901611328, test mean loss [5.4872565 5.4545517 5.6612797 5.2169805 5.326168  5.537683  5.56689  ]
Model epoch 100: train total loss 3.282883882522583, train mean loss 4.597089767456055, test mean loss [5.441812  5.4162197 5.69254   5.0984564 5.27792   5.5401454 5.5605145]
Model epoch 101: train total loss 3.2173311710357666, train mean loss 4.5773820877075195, test mean loss [5.469306  5.3538504 5.713859  5.0164795 5.2608566 5.504265  5.5560837]
Model epoch 102: train total loss 2.055809736251831, train mean loss 4.3722028732299805, test mean loss [5.4191337 5.2469707 5.6178665 5.0396996 5.2909307 5.4965835 5.4877205]
Model epoch 103: train total loss 2.036229372024536, train mean loss 4.396480083465576, test mean loss [5.35576   5.175829  5.621715  5.0129995 5.2206583 5.441284  5.446695 ]
Model epoch 104: train total loss 1.9941240549087524, train mean loss 4.447181701660156, test mean loss [5.3105507 5.1290026 5.574522  5.005509  5.1303215 5.4267254 5.358775 ]
Model epoch 105: train total loss 1.799070954322815, train mean loss 4.343759536743164, test mean loss [5.222247  5.020342  5.585581  4.982397  5.1266203 5.320291  5.299581 ]
Model epoch 106: train total loss 1.259022831916809, train mean loss 4.263984203338623, test mean loss [5.1626425 5.0571227 5.5669346 4.922305  4.9926505 5.357271  5.249456 ]
Model epoch 107: train total loss 1.5553518533706665, train mean loss 4.311258316040039, test mean loss [5.066227  4.9228835 5.5443983 4.9012556 4.983936  5.210966  5.139648 ]
Model epoch 108: train total loss 1.3517789840698242, train mean loss 4.238500595092773, test mean loss [5.017195  4.8314257 5.4735775 4.8482723 4.9654975 5.1275516 5.1649404]
Model epoch 109: train total loss 0.11787956953048706, train mean loss 4.022357940673828, test mean loss [4.976257  4.762996  5.4637823 4.8227944 4.9180603 5.012792  5.1022625]
Model epoch 110: train total loss 1.248308777809143, train mean loss 4.198331832885742, test mean loss [4.9190216 4.7018194 5.4324417 4.834997  4.8219986 5.031844  5.015872 ]
Model epoch 111: train total loss 0.17501318454742432, train mean loss 3.9550881385803223, test mean loss [4.8487163 4.6375065 5.416894  4.793403  4.7723055 4.991574  5.0221024]
Model epoch 112: train total loss 0.08643066138029099, train mean loss 3.8896188735961914, test mean loss [4.811983  4.5687904 5.3509054 4.8028746 4.7386484 4.9735117 4.9934216]
Model epoch 113: train total loss 0.48358437418937683, train mean loss 4.035975456237793, test mean loss [4.772966  4.491913  5.3358526 4.798621  4.643793  4.901701  4.96051  ]
Model epoch 114: train total loss -0.16224965453147888, train mean loss 3.9279253482818604, test mean loss [4.6647053 4.476652  5.2423983 4.7506695 4.6446934 4.915329  4.9035587]
Model epoch 115: train total loss -1.0302343368530273, train mean loss 3.733713388442993, test mean loss [4.6234503 4.44671   5.2555733 4.7777123 4.554219  4.887853  4.7987156]
Model epoch 116: train total loss -1.1966196298599243, train mean loss 3.7202839851379395, test mean loss [4.5610666 4.363723  5.1960387 4.665784  4.488574  4.848724  4.807716 ]
Model epoch 117: train total loss -1.6061749458312988, train mean loss 3.5851500034332275, test mean loss [4.5458145 4.300947  5.0881653 4.5965443 4.5081434 4.823615  4.7372165]
Model epoch 118: train total loss -1.953060507774353, train mean loss 3.559213638305664, test mean loss [4.413456  4.29594   5.042843  4.5509977 4.4778185 4.801542  4.698575 ]
Model epoch 119: train total loss -1.6863011121749878, train mean loss 3.6665031909942627, test mean loss [4.254438  4.2240057 5.0006256 4.534929  4.3576407 4.819202  4.639034 ]
Model epoch 120: train total loss -2.3456826210021973, train mean loss 3.4946131706237793, test mean loss [4.1938148 4.1282845 4.9000225 4.4409204 4.3293104 4.7767363 4.5286283]
Model epoch 121: train total loss -1.9166359901428223, train mean loss 3.561272621154785, test mean loss [4.133884  4.0349283 4.8562813 4.4284525 4.272062  4.759704  4.4887104]
Model epoch 122: train total loss -2.7015464305877686, train mean loss 3.440514326095581, test mean loss [4.107444  3.9976482 4.807143  4.3814454 4.164145  4.7611065 4.4850187]
Model epoch 123: train total loss -3.4217302799224854, train mean loss 3.326392412185669, test mean loss [4.1206374 3.9293442 4.7620583 4.3077846 4.119711  4.709479  4.3847218]
Model epoch 124: train total loss -3.3695757389068604, train mean loss 3.324964761734009, test mean loss [4.0228076 3.9051604 4.649663  4.228867  4.0035586 4.7471857 4.3676186]
Model epoch 125: train total loss -3.2440600395202637, train mean loss 3.3750064373016357, test mean loss [4.0344286 3.8817973 4.5547233 4.1487956 3.9808793 4.7079844 4.263373 ]
Model epoch 126: train total loss -3.6072235107421875, train mean loss 3.331282615661621, test mean loss [4.028121  3.8548374 4.5315933 4.068519  3.9010754 4.6669984 4.2929506]
Model epoch 127: train total loss -3.273864269256592, train mean loss 3.3461930751800537, test mean loss [4.0079536 3.8451838 4.468706  3.9673278 3.8349872 4.7055125 4.173078 ]
Model epoch 128: train total loss -4.622861862182617, train mean loss 3.1114070415496826, test mean loss [3.9948192 3.8231897 4.358822  3.9835997 3.7899044 4.6099434 4.192375 ]
Model epoch 129: train total loss -4.4257659912109375, train mean loss 3.0683388710021973, test mean loss [3.9757965 3.8175774 4.271064  3.911357  3.8409724 4.602096  4.0818796]
Model epoch 130: train total loss -5.473381996154785, train mean loss 2.907405376434326, test mean loss [4.0010285 3.8158813 4.260666  3.8828456 3.7711306 4.581047  4.034272 ]
Model epoch 131: train total loss -4.967145919799805, train mean loss 3.0446503162384033, test mean loss [3.9415872 3.7662513 4.227908  3.8650756 3.7330737 4.5621767 4.030992 ]
Model epoch 132: train total loss -4.743961811065674, train mean loss 3.0464065074920654, test mean loss [3.9354713 3.8038712 4.1277204 3.8772237 3.737539  4.5131407 3.9580393]
Model epoch 133: train total loss -5.322648525238037, train mean loss 2.881704568862915, test mean loss [3.907848  3.821759  4.0711107 3.8181796 3.7344108 4.4943514 3.9518313]
Model epoch 134: train total loss -4.9979376792907715, train mean loss 2.9940171241760254, test mean loss [3.9134078 3.793675  4.024167  3.7839665 3.709518  4.40483   3.9197197]
Model epoch 135: train total loss -5.661333084106445, train mean loss 2.899183511734009, test mean loss [3.9035945 3.7164164 3.98321   3.7848308 3.7320564 4.400396  3.9004574]
Model epoch 136: train total loss -5.548584938049316, train mean loss 2.90626859664917, test mean loss [3.974258  3.7011151 3.893712  3.841109  3.689762  4.3376412 3.8608158]
Model epoch 137: train total loss -6.102927207946777, train mean loss 2.822723865509033, test mean loss [3.888102  3.6736794 3.9588013 3.7753353 3.6702821 4.2444754 3.9019132]
Model epoch 138: train total loss -5.844521522521973, train mean loss 2.850450038909912, test mean loss [3.8407645 3.7399182 3.8883052 3.746655  3.7024038 4.1654043 3.845851 ]
Model epoch 139: train total loss -6.472073554992676, train mean loss 2.8060829639434814, test mean loss [3.8558922 3.6704626 3.8677883 3.8246276 3.682531  4.0394406 3.8334947]
Model epoch 140: train total loss -6.46475076675415, train mean loss 2.759673833847046, test mean loss [3.8225384 3.6994514 3.8994694 3.7677438 3.6723354 4.0041943 3.8015494]
Model epoch 141: train total loss -6.540680408477783, train mean loss 2.774601459503174, test mean loss [3.7951152 3.7517333 3.8333538 3.7515793 3.6857753 3.9237976 3.8018665]
Model epoch 142: train total loss -6.4716267585754395, train mean loss 2.779005527496338, test mean loss [3.7862365 3.704211  3.8066993 3.7346153 3.634615  3.9011285 3.7864063]
Model epoch 143: train total loss -7.08865213394165, train mean loss 2.6575024127960205, test mean loss [3.8403473 3.6819267 3.7926831 3.6894488 3.6477726 3.8183036 3.8094108]
Model epoch 144: train total loss -6.914000511169434, train mean loss 2.7298836708068848, test mean loss [3.822584  3.7591076 3.747796  3.7085338 3.6666193 3.79403   3.7489138]
Model epoch 145: train total loss -7.447752475738525, train mean loss 2.6114158630371094, test mean loss [3.8062637 3.6599176 3.7950892 3.7728605 3.6959043 3.747956  3.7388644]
Model epoch 146: train total loss -7.51306676864624, train mean loss 2.6068410873413086, test mean loss [3.7867403 3.6521118 3.733651  3.772476  3.6241064 3.7478218 3.714065 ]
Model epoch 147: train total loss -7.664467811584473, train mean loss 2.546330451965332, test mean loss [3.776223  3.6842363 3.7258906 3.7078958 3.632866  3.7336664 3.712814 ]
Model epoch 148: train total loss -7.776371955871582, train mean loss 2.5834860801696777, test mean loss [3.7879398 3.6483436 3.6983008 3.7524476 3.645618  3.7557425 3.7288082]
Model epoch 149: train total loss -8.01088809967041, train mean loss 2.5055618286132812, test mean loss [3.7394187 3.7121186 3.6945143 3.650212  3.624731  3.7355013 3.818884 ]
Model epoch 150: train total loss -8.174734115600586, train mean loss 2.4839930534362793, test mean loss [3.8083303 3.7046237 3.7242506 3.6881366 3.6079884 3.7033718 3.7244976]
Model epoch 151: train total loss -7.71950101852417, train mean loss 2.597703695297241, test mean loss [3.74962   3.714857  3.676726  3.6852698 3.5906925 3.706162  3.700027 ]
Model epoch 152: train total loss -8.165291786193848, train mean loss 2.4985740184783936, test mean loss [3.7440357 3.6664963 3.7070127 3.6782577 3.6255746 3.6642568 3.6802278]
Model epoch 153: train total loss -7.8673906326293945, train mean loss 2.565333127975464, test mean loss [3.7822795 3.6606827 3.6380172 3.6530647 3.6651988 3.6616378 3.7286098]
Model epoch 154: train total loss -8.435663223266602, train mean loss 2.460797071456909, test mean loss [3.6976295 3.6493442 3.7749414 3.660671  3.630696  3.648501  3.6921406]
Model epoch 155: train total loss -8.202534675598145, train mean loss 2.4962992668151855, test mean loss [3.7044868 3.632574  3.6891372 3.6462543 3.6600733 3.6373558 3.7105079]
Model epoch 156: train total loss -9.117101669311523, train mean loss 2.380040168762207, test mean loss [3.660628  3.6314468 3.7256312 3.642981  3.6142015 3.6481552 3.67899  ]
Model epoch 157: train total loss -8.830565452575684, train mean loss 2.430924654006958, test mean loss [3.6971061 3.6488156 3.6654906 3.6496258 3.634945  3.6119766 3.6532772]
Model epoch 158: train total loss -8.349324226379395, train mean loss 2.4743168354034424, test mean loss [3.7191458 3.6318328 3.638394  3.631137  3.6118286 3.647664  3.6736972]
Model epoch 159: train total loss -8.730953216552734, train mean loss 2.4424569606781006, test mean loss [3.6895175 3.7448833 3.6427906 3.6004786 3.7036579 3.629732  3.622607 ]
Model epoch 160: train total loss -9.019964218139648, train mean loss 2.36946964263916, test mean loss [3.7256246 3.637878  3.6656399 3.6331754 3.623012  3.6436803 3.6401772]
Model epoch 161: train total loss -9.149439811706543, train mean loss 2.3564019203186035, test mean loss [3.67724   3.613338  3.6290827 3.593608  3.6175032 3.6251855 3.6474693]
Model epoch 162: train total loss -9.153749465942383, train mean loss 2.3297836780548096, test mean loss [3.6920574 3.5967202 3.6345718 3.593789  3.6445713 3.5982862 3.6732693]
Model epoch 163: train total loss -9.422989845275879, train mean loss 2.3023252487182617, test mean loss [3.6693478 3.6207683 3.6394594 3.5761602 3.64622   3.6402307 3.5907364]
Model epoch 164: train total loss -8.988947868347168, train mean loss 2.3807036876678467, test mean loss [3.6585853 3.6097076 3.613796  3.5337892 3.667564  3.619842  3.6130962]
Model epoch 165: train total loss -9.327549934387207, train mean loss 2.29343581199646, test mean loss [3.7152488 3.6046767 3.6229725 3.5071743 3.618209  3.6024501 3.6302707]
Model epoch 166: train total loss -9.530967712402344, train mean loss 2.2936816215515137, test mean loss [3.6725583 3.6219587 3.6270492 3.5515516 3.6740856 3.603258  3.5811841]
Model epoch 167: train total loss -9.794882774353027, train mean loss 2.253913402557373, test mean loss [3.688319  3.6185431 3.623506  3.489469  3.615976  3.5732055 3.5995739]
Model epoch 168: train total loss -9.55158519744873, train mean loss 2.2971105575561523, test mean loss [3.6511693 3.5549002 3.644635  3.5009265 3.611221  3.5558405 3.588297 ]
Model epoch 169: train total loss -10.235340118408203, train mean loss 2.1792211532592773, test mean loss [3.6765115 3.572746  3.6544693 3.4288375 3.54524   3.5951765 3.5976834]
Model epoch 170: train total loss -10.479307174682617, train mean loss 2.110110282897949, test mean loss [3.682611  3.513294  3.6182878 3.391903  3.6264074 3.5976562 3.5484428]
Model epoch 171: train total loss -10.098167419433594, train mean loss 2.206446647644043, test mean loss [3.6561332 3.475441  3.6481147 3.3726578 3.6118717 3.606307  3.6332855]
Model epoch 172: train total loss -10.386027336120605, train mean loss 2.158698320388794, test mean loss [3.6392927 3.5684297 3.6159167 3.344527  3.5640953 3.5956218 3.6533885]
Model epoch 173: train total loss -9.940828323364258, train mean loss 2.2053372859954834, test mean loss [3.5689373 3.4618626 3.6175485 3.386175  3.575006  3.5987728 3.5831583]
Model epoch 174: train total loss -10.343865394592285, train mean loss 2.1331114768981934, test mean loss [3.5896783 3.3790863 3.6703815 3.3635128 3.6500068 3.5384426 3.5595303]
Model epoch 175: train total loss -10.547562599182129, train mean loss 2.0942225456237793, test mean loss [3.5652606 3.424024  3.6310117 3.3257987 3.5950484 3.5939937 3.5350091]
Model epoch 176: train total loss -10.436870574951172, train mean loss 2.1278164386749268, test mean loss [3.4794614 3.4055116 3.6189468 3.311158  3.5079372 3.5927703 3.5177524]
Model epoch 177: train total loss -10.510403633117676, train mean loss 2.0723626613616943, test mean loss [3.4652038 3.4345844 3.6244044 3.3094764 3.521391  3.6170633 3.4959254]
Model epoch 178: train total loss -10.400660514831543, train mean loss 2.116511583328247, test mean loss [3.512574  3.3028095 3.5842838 3.2614918 3.519903  3.6011815 3.527045 ]
Model epoch 179: train total loss -11.079033851623535, train mean loss 2.010964870452881, test mean loss [3.490034  3.278485  3.561575  3.2416987 3.562122  3.6201625 3.4751868]
Model epoch 180: train total loss -10.876593589782715, train mean loss 2.042891502380371, test mean loss [3.4057348 3.2795687 3.5575395 3.2767704 3.570435  3.5793579 3.4854877]
Model epoch 181: train total loss -11.387099266052246, train mean loss 1.979088544845581, test mean loss [3.3930166 3.2257671 3.511734  3.260061  3.507037  3.581994  3.5270119]
Model epoch 182: train total loss -11.566683769226074, train mean loss 1.9554082155227661, test mean loss [3.436432  3.3298612 3.5098672 3.2929695 3.4642062 3.5491161 3.4358633]
Model epoch 183: train total loss -11.603205680847168, train mean loss 1.9261938333511353, test mean loss [3.3981185 3.2695107 3.4756684 3.2757537 3.439944  3.541333  3.3769464]
Model epoch 184: train total loss -11.733244895935059, train mean loss 1.8940520286560059, test mean loss [3.3317935 3.3087559 3.4903495 3.2618117 3.4219737 3.506873  3.3553205]
Model epoch 185: train total loss -11.918659210205078, train mean loss 1.8781214952468872, test mean loss [3.3539016 3.2493742 3.4476185 3.2330227 3.3882785 3.5523622 3.314561 ]
Model epoch 186: train total loss -11.616851806640625, train mean loss 1.905569076538086, test mean loss [3.2743387 3.3017492 3.4810178 3.2296438 3.4085567 3.5117607 3.321928 ]
Model epoch 187: train total loss -12.173895835876465, train mean loss 1.8084299564361572, test mean loss [3.2891345 3.2166736 3.427057  3.2365084 3.3437319 3.518362  3.3202379]
Model epoch 188: train total loss -12.640223503112793, train mean loss 1.7805627584457397, test mean loss [3.2778735 3.2552295 3.3798656 3.214851  3.3551006 3.482993  3.3587089]
Model epoch 189: train total loss -12.558857917785645, train mean loss 1.7705389261245728, test mean loss [3.263042  3.1916213 3.347331  3.2141914 3.3373523 3.4698958 3.301692 ]
Model epoch 190: train total loss -12.293693542480469, train mean loss 1.8052505254745483, test mean loss [3.3048303 3.2358987 3.3171544 3.2075882 3.2928448 3.471303  3.3180356]
Model epoch 191: train total loss -12.370316505432129, train mean loss 1.7967416048049927, test mean loss [3.2857533 3.2213583 3.3158617 3.2560346 3.2213697 3.4336848 3.3043277]
Model epoch 192: train total loss -12.716526985168457, train mean loss 1.7035632133483887, test mean loss [3.27043   3.246653  3.3132598 3.2704415 3.2099466 3.4344954 3.210977 ]
Model epoch 193: train total loss -12.698161125183105, train mean loss 1.708559274673462, test mean loss [3.2941241 3.1764896 3.3080573 3.2321496 3.2185373 3.3739667 3.2714043]
Model epoch 194: train total loss -12.914280891418457, train mean loss 1.675192952156067, test mean loss [3.2672439 3.2539382 3.287386  3.2136436 3.2244077 3.3594244 3.249889 ]
Model epoch 195: train total loss -12.730691909790039, train mean loss 1.6890329122543335, test mean loss [3.2756066 3.2568595 3.3092518 3.2480638 3.1749492 3.3411565 3.2374382]
Model epoch 196: train total loss -12.970727920532227, train mean loss 1.653249979019165, test mean loss [3.2465363 3.2430024 3.2763138 3.2922332 3.154267  3.311438  3.224243 ]
Model epoch 197: train total loss -13.432022094726562, train mean loss 1.6119657754898071, test mean loss [3.3283257 3.269866  3.2663016 3.308012  3.1161914 3.2528207 3.1984367]
Model epoch 198: train total loss -13.64368724822998, train mean loss 1.587597370147705, test mean loss [3.2541206 3.200746  3.2965474 3.2799392 3.1345184 3.26169   3.188015 ]
Model epoch 199: train total loss -13.71330738067627, train mean loss 1.5237761735916138, test mean loss [3.3227308 3.156021  3.3105946 3.3419025 3.1047306 3.2072225 3.202899 ]
Model epoch 200: train total loss -13.251863479614258, train mean loss 1.630547285079956, test mean loss [3.2349854 3.2132888 3.2843914 3.270835  3.1532736 3.2223775 3.2290206]
Model epoch 201: train total loss -13.793170928955078, train mean loss 1.5354540348052979, test mean loss [3.2404964 3.1887755 3.3136582 3.2367363 3.1936035 3.1985235 3.2297258]
Model epoch 202: train total loss -13.59382152557373, train mean loss 1.587583065032959, test mean loss [3.286852  3.2419453 3.2734153 3.2530046 3.135066  3.1871831 3.2185898]
Model epoch 203: train total loss -13.63131046295166, train mean loss 1.558326005935669, test mean loss [3.2676876 3.1578064 3.2464197 3.2760913 3.115053  3.222207  3.2640944]
Model epoch 204: train total loss -13.918749809265137, train mean loss 1.4890763759613037, test mean loss [3.3062844 3.246394  3.2638338 3.2704368 3.1429622 3.1555438 3.2235117]
Model epoch 205: train total loss -14.008076667785645, train mean loss 1.5198568105697632, test mean loss [3.3330438 3.1934052 3.2807827 3.274479  3.1489875 3.1259031 3.2595453]
Model epoch 206: train total loss -13.794976234436035, train mean loss 1.5540968179702759, test mean loss [3.332941  3.2284763 3.2582054 3.306361  3.103837  3.12337   3.2400513]
Model epoch 207: train total loss -13.977509498596191, train mean loss 1.540106177330017, test mean loss [3.238731  3.2408016 3.284203  3.2580307 3.0994358 3.1843648 3.2017603]
Model epoch 208: train total loss -14.508003234863281, train mean loss 1.4283298254013062, test mean loss [3.3157196 3.2112465 3.2421832 3.2943287 3.0866354 3.1753654 3.2148232]
Model epoch 209: train total loss -14.71929931640625, train mean loss 1.4169808626174927, test mean loss [3.3226585 3.2394674 3.2577682 3.2495403 3.0996158 3.172163  3.205904 ]
Model epoch 210: train total loss -14.388766288757324, train mean loss 1.5015779733657837, test mean loss [3.318296  3.2065954 3.2605772 3.2575223 3.091707  3.1646333 3.2461288]
Model epoch 211: train total loss -14.623298645019531, train mean loss 1.4306875467300415, test mean loss [3.3023975 3.2312326 3.2656016 3.302759  3.1326284 3.1944778 3.2409167]
Model epoch 212: train total loss -14.814977645874023, train mean loss 1.4094984531402588, test mean loss [3.310949  3.1779802 3.271861  3.245216  3.1096911 3.18149   3.2494056]
Model epoch 213: train total loss -14.9038667678833, train mean loss 1.435735821723938, test mean loss [3.2820206 3.1904275 3.2728333 3.2591958 3.07411   3.1846225 3.2372742]
Model epoch 214: train total loss -15.421399116516113, train mean loss 1.3354191780090332, test mean loss [3.306069  3.2007072 3.2668886 3.313032  3.0910945 3.216294  3.2072058]
Model epoch 215: train total loss -14.85582447052002, train mean loss 1.423094630241394, test mean loss [3.268814  3.2686298 3.3414888 3.2882843 3.036284  3.1758776 3.2117393]
Model epoch 216: train total loss -14.939881324768066, train mean loss 1.4026323556900024, test mean loss [3.27991   3.1964252 3.2859797 3.295206  3.0829084 3.174626  3.2110438]
Model epoch 217: train total loss -15.217267036437988, train mean loss 1.3638936281204224, test mean loss [3.325035  3.18717   3.306688  3.239209  3.1121738 3.1741283 3.209867 ]
Model epoch 218: train total loss -15.293866157531738, train mean loss 1.354174256324768, test mean loss [3.3029068 3.19496   3.3015966 3.2558413 3.1223075 3.1463408 3.2735028]
Model epoch 219: train total loss -15.419047355651855, train mean loss 1.3457229137420654, test mean loss [3.3019218 3.232367  3.2907214 3.2715204 3.1145248 3.1307487 3.2401657]
Model epoch 220: train total loss -15.257560729980469, train mean loss 1.3587297201156616, test mean loss [3.227368  3.2190266 3.279768  3.3144176 3.086635  3.132677  3.2250628]
Model epoch 221: train total loss -15.590259552001953, train mean loss 1.312731146812439, test mean loss [3.292052  3.2436318 3.2810056 3.291088  3.1439512 3.1696098 3.2966871]
Model trained in 222 epochs with 4000 transitions.
[2025-01-29 16:40:38,332][absl][INFO] - {'eval/walltime': 23.719568490982056, 'training/sps': 17.164491205994548, 'training/walltime': 179.6233789920807, 'training/model_train_time': 35.181999921798706, 'training/other_time': 23.073818922042847, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 400, 'training/env_buffer_size': Array(5000, dtype=int32), 'model/train_total_loss': Array(-15.59026, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3127311, dtype=float32), 'model/test_total_loss': Array(1.4854969, dtype=float32), 'model/test_mean_loss': Array(3.2454324, dtype=float32), 'model/train_epochs': 222, 'model/sec_per_epoch': 0.1490693103085767, 'sac/actor_loss': Array(-38733.73, dtype=float32), 'sac/alpha': Array(0.20000003, dtype=float32), 'sac/alpha_loss': Array(-11.493571, dtype=float32), 'sac/buffer_current_size': Array(81195.5, dtype=float32), 'sac/critic_loss': Array(21792378., dtype=float32), 'eval/episode_distance_from_origin': Array(18.591118, dtype=float32), 'eval/episode_forward_reward': Array(5.006936, dtype=float32), 'eval/episode_reward': Array(-65.84973, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-94.171776, dtype=float32), 'eval/episode_reward_forward': Array(5.006936, dtype=float32), 'eval/episode_reward_survive': Array(26., dtype=float32), 'eval/episode_x_position': Array(0.11376083, dtype=float32), 'eval/episode_x_velocity': Array(5.006936, dtype=float32), 'eval/episode_y_position': Array(-0.43233496, dtype=float32), 'eval/episode_y_velocity': Array(-2.4461193, dtype=float32), 'eval/avg_episode_length': Array(27., dtype=float32), 'eval/epoch_eval_time': 3.283339262008667, 'eval/sps': 304.56797796406346}
5000.0 {'eval/walltime': 23.719568490982056, 'training/sps': 17.164491205994548, 'training/walltime': 179.6233789920807, 'training/model_train_time': 35.181999921798706, 'training/other_time': 23.073818922042847, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 400, 'training/env_buffer_size': Array(5000, dtype=int32), 'model/train_total_loss': Array(-15.59026, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3127311, dtype=float32), 'model/test_total_loss': Array(1.4854969, dtype=float32), 'model/test_mean_loss': Array(3.2454324, dtype=float32), 'model/train_epochs': 222, 'model/sec_per_epoch': 0.1490693103085767, 'sac/actor_loss': Array(-38733.73, dtype=float32), 'sac/alpha': Array(0.20000003, dtype=float32), 'sac/alpha_loss': Array(-11.493571, dtype=float32), 'sac/buffer_current_size': Array(81195.5, dtype=float32), 'sac/critic_loss': Array(21792378., dtype=float32), 'eval/episode_distance_from_origin': Array(18.591118, dtype=float32), 'eval/episode_forward_reward': Array(5.006936, dtype=float32), 'eval/episode_reward': Array(-65.84973, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-94.171776, dtype=float32), 'eval/episode_reward_forward': Array(5.006936, dtype=float32), 'eval/episode_reward_survive': Array(26., dtype=float32), 'eval/episode_x_position': Array(0.11376083, dtype=float32), 'eval/episode_x_velocity': Array(5.006936, dtype=float32), 'eval/episode_y_position': Array(-0.43233496, dtype=float32), 'eval/episode_y_velocity': Array(-2.4461193, dtype=float32), 'eval/avg_episode_length': Array(27., dtype=float32), 'eval/epoch_eval_time': 3.283339262008667, 'eval/sps': 304.56797796406346, 'steps': Array(5000., dtype=float32)}
Model horizon updated to 1.
Hallucination updates per training step updated to 500.
SAC buffer resized to 200000 samples.
Model epoch 0: train total loss -2.2903964519500732, train mean loss 2.5541720390319824, test mean loss [2.3405192 2.4539077 2.722528  2.5708692 2.4343467 2.4532475 2.4683902]
Model epoch 1: train total loss -6.768087863922119, train mean loss 2.1282615661621094, test mean loss [2.2057998 2.145764  2.260378  2.2421465 2.2074296 2.094126  2.2148619]
Model epoch 2: train total loss -9.5751371383667, train mean loss 1.8420768976211548, test mean loss [2.0993137 2.0664518 2.0710044 2.0672514 2.09371   2.0384057 2.0686822]
Model epoch 3: train total loss -10.878087997436523, train mean loss 1.801983118057251, test mean loss [1.9982133 1.9697467 2.072188  2.0015035 2.046504  1.9699998 2.0336742]
Model epoch 4: train total loss -11.745681762695312, train mean loss 1.69455087184906, test mean loss [2.0313442 1.9559501 2.003116  2.0050528 2.0144022 1.9520899 2.0528967]
Model epoch 5: train total loss -11.907033920288086, train mean loss 1.7644128799438477, test mean loss [1.9935702 1.9241357 2.0381563 1.9645531 2.007815  1.9255211 2.0215812]
Model epoch 6: train total loss -12.670143127441406, train mean loss 1.636197566986084, test mean loss [1.9949114 1.9013913 1.9661072 1.952389  1.9498644 1.9427133 2.003932 ]
Model epoch 7: train total loss -13.221073150634766, train mean loss 1.5874215364456177, test mean loss [1.9802909 1.9276087 1.9491087 1.9606011 1.9876404 1.950397  1.9686096]
Model epoch 8: train total loss -13.580842971801758, train mean loss 1.575635313987732, test mean loss [1.9831295 1.934939  1.9558787 1.9146686 1.9774413 1.9044845 1.9580857]
Model epoch 9: train total loss -13.58960247039795, train mean loss 1.595548391342163, test mean loss [1.987959  1.8964317 1.9465837 1.938399  1.9721036 1.9521226 1.9630053]
Model epoch 10: train total loss -13.90168285369873, train mean loss 1.5449312925338745, test mean loss [1.9606339 1.8562559 1.9352609 1.9399108 1.937761  1.9231699 1.9753711]
Model epoch 11: train total loss -13.715142250061035, train mean loss 1.547218918800354, test mean loss [2.0332031 1.8569578 1.9517783 1.912212  1.963465  1.9023257 1.957191 ]
Model epoch 12: train total loss -14.165870666503906, train mean loss 1.515663743019104, test mean loss [1.9908907 1.8828523 1.9612705 1.9244024 1.9211631 1.8884888 1.9603695]
Model epoch 13: train total loss -14.038196563720703, train mean loss 1.590815782546997, test mean loss [1.9874384 1.9044924 1.934083  1.9077619 1.9178691 1.8988572 1.9347417]
Model epoch 14: train total loss -14.555031776428223, train mean loss 1.478739857673645, test mean loss [1.9643263 1.8729312 1.9412796 1.9321034 1.9524763 1.902998  1.9611906]
Model epoch 15: train total loss -14.381326675415039, train mean loss 1.5453009605407715, test mean loss [2.0167215 1.8817874 1.9311794 1.8934311 1.9336636 1.8904797 1.9398191]
Model epoch 16: train total loss -14.275229454040527, train mean loss 1.5775665044784546, test mean loss [1.979928  1.8643358 1.9410372 1.9276541 1.9381485 1.8860524 1.9579809]
Model epoch 17: train total loss -14.342637062072754, train mean loss 1.5663037300109863, test mean loss [1.9717975 1.871854  1.9230468 1.9100403 1.932217  1.8926585 1.9876039]
Model epoch 18: train total loss -14.832210540771484, train mean loss 1.4568618535995483, test mean loss [1.9942338 1.8691943 1.9181867 1.9026992 1.9283342 1.8913656 1.9350492]
Model epoch 19: train total loss -15.330621719360352, train mean loss 1.3591519594192505, test mean loss [1.9941318 1.85269   1.9277332 1.929604  1.9165286 1.8911426 1.9515225]
Model epoch 20: train total loss -15.128704071044922, train mean loss 1.4290395975112915, test mean loss [1.9857432 1.8725367 1.8956528 1.9221585 1.9150767 1.886955  1.9495857]
Model epoch 21: train total loss -15.365371704101562, train mean loss 1.417362093925476, test mean loss [1.971876  1.8831866 1.9431028 1.9240425 1.9465833 1.859833  1.9508379]
Model epoch 22: train total loss -15.573688507080078, train mean loss 1.396968126296997, test mean loss [1.9741178 1.8641489 1.9208548 1.9277012 1.9523641 1.8570259 1.9552991]
Model epoch 23: train total loss -15.508275032043457, train mean loss 1.3915852308273315, test mean loss [2.0045738 1.8803542 1.9091372 1.8900819 1.9107809 1.8888841 1.9445772]
Model epoch 24: train total loss -16.287517547607422, train mean loss 1.2503926753997803, test mean loss [1.9723095 1.8793097 1.9260744 1.8929803 1.9218093 1.8890543 1.9696791]
Model epoch 25: train total loss -15.440084457397461, train mean loss 1.4393372535705566, test mean loss [1.9691572 1.88914   1.9223197 1.9250406 1.9133768 1.8962171 1.9535776]
Model epoch 26: train total loss -15.706883430480957, train mean loss 1.3458149433135986, test mean loss [1.955329  1.8638418 1.9289118 1.9244246 1.9073359 1.8873049 1.9529643]
Model epoch 27: train total loss -15.704922676086426, train mean loss 1.3600757122039795, test mean loss [1.977779  1.883374  1.9356737 1.8967881 1.9241421 1.8723302 1.9833862]
Model trained in 28 epochs with 5000 transitions.
[2025-01-29 16:41:12,567][absl][INFO] - {'eval/walltime': 26.99076223373413, 'training/sps': 32.343288653323846, 'training/walltime': 210.5416944026947, 'training/model_train_time': 8.054803609848022, 'training/other_time': 22.859325647354126, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 500, 'training/env_buffer_size': Array(6000, dtype=int32), 'model/train_total_loss': Array(-15.704923, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3600757, dtype=float32), 'model/test_total_loss': Array(-9.713953, dtype=float32), 'model/test_mean_loss': Array(1.924782, dtype=float32), 'model/train_epochs': 28, 'model/sec_per_epoch': 0.22176629304885864, 'sac/actor_loss': Array(-25780.936, dtype=float32), 'sac/alpha': Array(0.20000002, dtype=float32), 'sac/alpha_loss': Array(-8.899195, dtype=float32), 'sac/buffer_current_size': Array(101196.41, dtype=float32), 'sac/critic_loss': Array(8702954., dtype=float32), 'eval/episode_distance_from_origin': Array(28.146091, dtype=float32), 'eval/episode_forward_reward': Array(-8.393208, dtype=float32), 'eval/episode_reward': Array(-75.57816, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-97.06573, dtype=float32), 'eval/episode_reward_forward': Array(-8.393208, dtype=float32), 'eval/episode_reward_survive': Array(32., dtype=float32), 'eval/episode_x_position': Array(-3.8268573, dtype=float32), 'eval/episode_x_velocity': Array(-8.393208, dtype=float32), 'eval/episode_y_position': Array(0.04886889, dtype=float32), 'eval/episode_y_velocity': Array(-12.575388, dtype=float32), 'eval/avg_episode_length': Array(33., dtype=float32), 'eval/epoch_eval_time': 3.271193742752075, 'eval/sps': 305.69879947211376}
6000.0 {'eval/walltime': 26.99076223373413, 'training/sps': 32.343288653323846, 'training/walltime': 210.5416944026947, 'training/model_train_time': 8.054803609848022, 'training/other_time': 22.859325647354126, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 500, 'training/env_buffer_size': Array(6000, dtype=int32), 'model/train_total_loss': Array(-15.704923, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3600757, dtype=float32), 'model/test_total_loss': Array(-9.713953, dtype=float32), 'model/test_mean_loss': Array(1.924782, dtype=float32), 'model/train_epochs': 28, 'model/sec_per_epoch': 0.22176629304885864, 'sac/actor_loss': Array(-25780.936, dtype=float32), 'sac/alpha': Array(0.20000002, dtype=float32), 'sac/alpha_loss': Array(-8.899195, dtype=float32), 'sac/buffer_current_size': Array(101196.41, dtype=float32), 'sac/critic_loss': Array(8702954., dtype=float32), 'eval/episode_distance_from_origin': Array(28.146091, dtype=float32), 'eval/episode_forward_reward': Array(-8.393208, dtype=float32), 'eval/episode_reward': Array(-75.57816, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-97.06573, dtype=float32), 'eval/episode_reward_forward': Array(-8.393208, dtype=float32), 'eval/episode_reward_survive': Array(32., dtype=float32), 'eval/episode_x_position': Array(-3.8268573, dtype=float32), 'eval/episode_x_velocity': Array(-8.393208, dtype=float32), 'eval/episode_y_position': Array(0.04886889, dtype=float32), 'eval/episode_y_velocity': Array(-12.575388, dtype=float32), 'eval/avg_episode_length': Array(33., dtype=float32), 'eval/epoch_eval_time': 3.271193742752075, 'eval/sps': 305.69879947211376, 'steps': Array(6000., dtype=float32)}
Model horizon updated to 1.
Hallucination updates per training step updated to 600.
SAC buffer resized to 240000 samples.
Model epoch 0: train total loss -8.225120544433594, train mean loss 2.075612783432007, test mean loss [1.8248484 1.6836103 1.8646983 1.7153924 1.9656171 1.7766626 2.239317 ]
Model epoch 1: train total loss -10.465712547302246, train mean loss 1.8320640325546265, test mean loss [1.7710044 1.8684916 1.767909  1.6613934 1.7814262 1.6853404 1.852904 ]
Model epoch 2: train total loss -11.51937198638916, train mean loss 1.772057056427002, test mean loss [1.7629697 1.7189103 1.7181128 1.6741643 1.725476  1.6566335 1.7363784]
Model epoch 3: train total loss -11.126602172851562, train mean loss 1.7816756963729858, test mean loss [1.7358915 1.6866455 1.723541  1.7853522 1.735941  1.6520243 1.7529736]
Model epoch 4: train total loss -11.311322212219238, train mean loss 1.7652002573013306, test mean loss [1.7206503 1.6640633 1.7229323 1.7272717 1.7201133 1.6657739 1.7424326]
Model epoch 5: train total loss -11.434967994689941, train mean loss 1.7419960498809814, test mean loss [1.7450106 1.6470387 1.7480168 1.722152  1.69226   1.6633286 1.7410865]
Model epoch 6: train total loss -12.452031135559082, train mean loss 1.6196790933609009, test mean loss [1.7522118 1.6693714 1.7526495 1.6781857 1.6909014 1.6539301 1.6958239]
Model epoch 7: train total loss -12.931121826171875, train mean loss 1.5694869756698608, test mean loss [1.762641  1.6977346 1.7175943 1.6993487 1.6948037 1.6548225 1.6945534]
Model epoch 8: train total loss -13.372891426086426, train mean loss 1.563788652420044, test mean loss [1.7840682 1.6626021 1.717722  1.6916815 1.70443   1.6490822 1.7526782]
Model epoch 9: train total loss -13.15821647644043, train mean loss 1.6040655374526978, test mean loss [1.7476294 1.6796207 1.6901774 1.687264  1.731338  1.7121143 1.7150699]
Model epoch 10: train total loss -13.218994140625, train mean loss 1.646938681602478, test mean loss [1.7663896 1.65039   1.7124789 1.703058  1.6845249 1.6868052 1.7244952]
Model epoch 11: train total loss -13.544797897338867, train mean loss 1.5726441144943237, test mean loss [1.7477872 1.6705749 1.7157769 1.736356  1.7298632 1.6734151 1.7067016]
Model epoch 12: train total loss -13.672615051269531, train mean loss 1.5705431699752808, test mean loss [1.7461109 1.6573212 1.7313457 1.709069  1.6928535 1.6778276 1.7116649]
Model epoch 13: train total loss -13.650257110595703, train mean loss 1.5740445852279663, test mean loss [1.7937108 1.6664007 1.7322025 1.7082174 1.7379849 1.6783321 1.6914685]
Model epoch 14: train total loss -14.231223106384277, train mean loss 1.481138825416565, test mean loss [1.7867721 1.6669616 1.691118  1.7186468 1.7164395 1.7310994 1.7296559]
Model epoch 15: train total loss -14.8399076461792, train mean loss 1.4038923978805542, test mean loss [1.7940803 1.654881  1.7134016 1.7235031 1.7270286 1.7020828 1.7039912]
Model trained in 16 epochs with 6000 transitions.
[2025-01-29 16:41:47,101][absl][INFO] - {'eval/walltime': 30.268945693969727, 'training/sps': 32.03896976313368, 'training/walltime': 241.75368428230286, 'training/model_train_time': 6.391817569732666, 'training/other_time': 24.815496921539307, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 600, 'training/env_buffer_size': Array(7000, dtype=int32), 'model/train_total_loss': Array(-14.839908, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4038924, dtype=float32), 'model/test_total_loss': Array(-12.088297, dtype=float32), 'model/test_mean_loss': Array(1.7169956, dtype=float32), 'model/train_epochs': 16, 'model/sec_per_epoch': 0.2815990597009659, 'sac/actor_loss': Array(-2210.4534, dtype=float32), 'sac/alpha': Array(0.2, dtype=float32), 'sac/alpha_loss': Array(-5.794001, dtype=float32), 'sac/buffer_current_size': Array(121197., dtype=float32), 'sac/critic_loss': Array(613735.56, dtype=float32), 'eval/episode_distance_from_origin': Array(13.9869375, dtype=float32), 'eval/episode_forward_reward': Array(2.6523323, dtype=float32), 'eval/episode_reward': Array(-26.58078, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-44.155685, dtype=float32), 'eval/episode_reward_forward': Array(2.6523323, dtype=float32), 'eval/episode_reward_survive': Array(18., dtype=float32), 'eval/episode_x_position': Array(3.6860735, dtype=float32), 'eval/episode_x_velocity': Array(2.6523323, dtype=float32), 'eval/episode_y_position': Array(4.356414, dtype=float32), 'eval/episode_y_velocity': Array(3.4419358, dtype=float32), 'eval/avg_episode_length': Array(19., dtype=float32), 'eval/epoch_eval_time': 3.2781834602355957, 'eval/sps': 305.04699085027175}
7000.0 {'eval/walltime': 30.268945693969727, 'training/sps': 32.03896976313368, 'training/walltime': 241.75368428230286, 'training/model_train_time': 6.391817569732666, 'training/other_time': 24.815496921539307, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 600, 'training/env_buffer_size': Array(7000, dtype=int32), 'model/train_total_loss': Array(-14.839908, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4038924, dtype=float32), 'model/test_total_loss': Array(-12.088297, dtype=float32), 'model/test_mean_loss': Array(1.7169956, dtype=float32), 'model/train_epochs': 16, 'model/sec_per_epoch': 0.2815990597009659, 'sac/actor_loss': Array(-2210.4534, dtype=float32), 'sac/alpha': Array(0.2, dtype=float32), 'sac/alpha_loss': Array(-5.794001, dtype=float32), 'sac/buffer_current_size': Array(121197., dtype=float32), 'sac/critic_loss': Array(613735.56, dtype=float32), 'eval/episode_distance_from_origin': Array(13.9869375, dtype=float32), 'eval/episode_forward_reward': Array(2.6523323, dtype=float32), 'eval/episode_reward': Array(-26.58078, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-44.155685, dtype=float32), 'eval/episode_reward_forward': Array(2.6523323, dtype=float32), 'eval/episode_reward_survive': Array(18., dtype=float32), 'eval/episode_x_position': Array(3.6860735, dtype=float32), 'eval/episode_x_velocity': Array(2.6523323, dtype=float32), 'eval/episode_y_position': Array(4.356414, dtype=float32), 'eval/episode_y_velocity': Array(3.4419358, dtype=float32), 'eval/avg_episode_length': Array(19., dtype=float32), 'eval/epoch_eval_time': 3.2781834602355957, 'eval/sps': 305.04699085027175, 'steps': Array(7000., dtype=float32)}
Model horizon updated to 1.
Hallucination updates per training step updated to 700.
SAC buffer resized to 280000 samples.
Model epoch 0: train total loss -11.488530158996582, train mean loss 1.7614705562591553, test mean loss [1.7726481 1.836734  1.7974176 1.7594512 1.7836965 1.8072795 1.7651446]
Model epoch 1: train total loss -12.682640075683594, train mean loss 1.6390669345855713, test mean loss [1.7808579 1.771718  1.7612536 1.7133602 1.7589543 1.7599481 1.7679434]
Model epoch 2: train total loss -13.444707870483398, train mean loss 1.4971996545791626, test mean loss [1.7902724 1.7478431 1.7564863 1.7170552 1.7983679 1.7506516 1.761059 ]
Model epoch 3: train total loss -13.098197937011719, train mean loss 1.6182061433792114, test mean loss [1.7953894 1.7246557 1.7825806 1.7052469 1.7574675 1.7500905 1.7839078]
Model epoch 4: train total loss -13.62915325164795, train mean loss 1.5448353290557861, test mean loss [1.8038476 1.7446355 1.7730391 1.7152874 1.7683445 1.7511147 1.7848647]
Model epoch 5: train total loss -13.957307815551758, train mean loss 1.5847944021224976, test mean loss [1.7884848 1.7563418 1.7767216 1.7386694 1.7544603 1.7624811 1.7691567]
Model epoch 6: train total loss -14.621515274047852, train mean loss 1.4466912746429443, test mean loss [1.7527382 1.7561944 1.7765114 1.7318497 1.7645931 1.7627242 1.797758 ]
Model epoch 7: train total loss -14.346854209899902, train mean loss 1.4499739408493042, test mean loss [1.7838181 1.754445  1.7728208 1.7614186 1.7569484 1.7686236 1.9526873]
Model epoch 8: train total loss -14.460164070129395, train mean loss 1.4793540239334106, test mean loss [1.8290411 1.7571682 1.7893809 1.7627836 1.7612454 1.7619991 1.8417963]
Model epoch 9: train total loss -13.974167823791504, train mean loss 1.5547611713409424, test mean loss [1.7936085 1.7749188 1.7806656 1.7741396 1.8067162 1.7754602 1.802515 ]
Model epoch 10: train total loss -15.192222595214844, train mean loss 1.3517292737960815, test mean loss [1.8109906 1.787569  1.7767178 1.7561369 1.7890882 1.7716023 1.7940018]
Model epoch 11: train total loss -15.030754089355469, train mean loss 1.4213148355484009, test mean loss [1.7950516 1.7650899 1.8164101 1.7505192 1.7730061 1.7761395 1.8013706]
Model epoch 12: train total loss -15.33082103729248, train mean loss 1.397968053817749, test mean loss [1.7903382 1.7877821 1.7599185 1.7654133 1.7681202 1.7951416 1.8516432]
Model trained in 13 epochs with 7000 transitions.
[2025-01-29 16:42:22,220][absl][INFO] - {'eval/walltime': 33.54872155189514, 'training/sps': 31.451130360480615, 'training/walltime': 273.5490438938141, 'training/model_train_time': 6.983920335769653, 'training/other_time': 24.808045148849487, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 700, 'training/env_buffer_size': Array(8000, dtype=int32), 'model/train_total_loss': Array(-15.330821, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.397968, dtype=float32), 'model/test_total_loss': Array(-11.612968, dtype=float32), 'model/test_mean_loss': Array(1.7883368, dtype=float32), 'model/train_epochs': 13, 'model/sec_per_epoch': 0.3798967691568228, 'sac/actor_loss': Array(2491.972, dtype=float32), 'sac/alpha': Array(0.20000002, dtype=float32), 'sac/alpha_loss': Array(-4.8582406, dtype=float32), 'sac/buffer_current_size': Array(141197.42, dtype=float32), 'sac/critic_loss': Array(71773.766, dtype=float32), 'eval/episode_distance_from_origin': Array(8.910715, dtype=float32), 'eval/episode_forward_reward': Array(-2.3755128, dtype=float32), 'eval/episode_reward': Array(-27.861135, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-33.520576, dtype=float32), 'eval/episode_reward_forward': Array(-2.3755128, dtype=float32), 'eval/episode_reward_survive': Array(13., dtype=float32), 'eval/episode_x_position': Array(0.95347524, dtype=float32), 'eval/episode_x_velocity': Array(-2.3755128, dtype=float32), 'eval/episode_y_position': Array(-3.2128315, dtype=float32), 'eval/episode_y_velocity': Array(-6.43261, dtype=float32), 'eval/avg_episode_length': Array(14., dtype=float32), 'eval/epoch_eval_time': 3.279775857925415, 'eval/sps': 304.8988843501454}
8000.0 {'eval/walltime': 33.54872155189514, 'training/sps': 31.451130360480615, 'training/walltime': 273.5490438938141, 'training/model_train_time': 6.983920335769653, 'training/other_time': 24.808045148849487, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 700, 'training/env_buffer_size': Array(8000, dtype=int32), 'model/train_total_loss': Array(-15.330821, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.397968, dtype=float32), 'model/test_total_loss': Array(-11.612968, dtype=float32), 'model/test_mean_loss': Array(1.7883368, dtype=float32), 'model/train_epochs': 13, 'model/sec_per_epoch': 0.3798967691568228, 'sac/actor_loss': Array(2491.972, dtype=float32), 'sac/alpha': Array(0.20000002, dtype=float32), 'sac/alpha_loss': Array(-4.8582406, dtype=float32), 'sac/buffer_current_size': Array(141197.42, dtype=float32), 'sac/critic_loss': Array(71773.766, dtype=float32), 'eval/episode_distance_from_origin': Array(8.910715, dtype=float32), 'eval/episode_forward_reward': Array(-2.3755128, dtype=float32), 'eval/episode_reward': Array(-27.861135, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-33.520576, dtype=float32), 'eval/episode_reward_forward': Array(-2.3755128, dtype=float32), 'eval/episode_reward_survive': Array(13., dtype=float32), 'eval/episode_x_position': Array(0.95347524, dtype=float32), 'eval/episode_x_velocity': Array(-2.3755128, dtype=float32), 'eval/episode_y_position': Array(-3.2128315, dtype=float32), 'eval/episode_y_velocity': Array(-6.43261, dtype=float32), 'eval/avg_episode_length': Array(14., dtype=float32), 'eval/epoch_eval_time': 3.279775857925415, 'eval/sps': 304.8988843501454, 'steps': Array(8000., dtype=float32)}
Model horizon updated to 1.
Hallucination updates per training step updated to 800.
SAC buffer resized to 320000 samples.
Model epoch 0: train total loss -10.75799560546875, train mean loss 1.8472001552581787, test mean loss [1.9557006 1.9039572 1.9530348 1.9061201 1.9479342 1.9626882 1.9374933]
Model epoch 1: train total loss -11.960025787353516, train mean loss 1.675361156463623, test mean loss [1.9536638 1.9145114 1.9079366 1.893363  1.9590114 1.9730619 1.9651062]
Model epoch 2: train total loss -12.639521598815918, train mean loss 1.7076611518859863, test mean loss [1.9538307 1.9191847 1.9514179 1.9118333 1.9405587 1.9527242 1.918876 ]
Model epoch 3: train total loss -13.987897872924805, train mean loss 1.509114384651184, test mean loss [1.9668459 1.9124286 1.9528797 1.9524729 1.9587886 1.9767474 1.9220303]
Model epoch 4: train total loss -14.47233772277832, train mean loss 1.4241769313812256, test mean loss [1.9641991 1.9612749 1.9595613 1.9132216 1.9515656 1.9725822 1.9341125]
Model epoch 5: train total loss -14.547075271606445, train mean loss 1.4887219667434692, test mean loss [1.9594808 1.9529705 1.9412553 1.9235709 1.9574027 1.9468029 1.9804071]
Model epoch 6: train total loss -14.145055770874023, train mean loss 1.5367034673690796, test mean loss [1.9841707 1.9615598 1.9413793 1.937538  1.9520538 2.0013454 1.9435523]
Model epoch 7: train total loss -15.163155555725098, train mean loss 1.3952268362045288, test mean loss [2.0052745 1.948076  1.9266361 1.9363022 1.9578242 1.9677534 1.9658327]
Model trained in 8 epochs with 8000 transitions.
[2025-01-29 16:42:58,775][absl][INFO] - {'eval/walltime': 36.81963300704956, 'training/sps': 30.084987012605456, 'training/walltime': 306.788213968277, 'training/model_train_time': 6.187543153762817, 'training/other_time': 27.046292543411255, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 800, 'training/env_buffer_size': Array(9000, dtype=int32), 'model/train_total_loss': Array(-15.163156, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3952268, dtype=float32), 'model/test_total_loss': Array(-11.106115, dtype=float32), 'model/test_mean_loss': Array(1.9582428, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 0.5045547485351562, 'sac/actor_loss': Array(2717.663, dtype=float32), 'sac/alpha': Array(0.20000003, dtype=float32), 'sac/alpha_loss': Array(-5.036258, dtype=float32), 'sac/buffer_current_size': Array(161197.75, dtype=float32), 'sac/critic_loss': Array(64704.5, dtype=float32), 'eval/episode_distance_from_origin': Array(221.71774, dtype=float32), 'eval/episode_forward_reward': Array(-94.45134, dtype=float32), 'eval/episode_reward': Array(-258.07767, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-254.746, dtype=float32), 'eval/episode_reward_forward': Array(-94.45134, dtype=float32), 'eval/episode_reward_survive': Array(96., dtype=float32), 'eval/episode_x_position': Array(-189.89946, dtype=float32), 'eval/episode_x_velocity': Array(-94.45134, dtype=float32), 'eval/episode_y_position': Array(-77.09086, dtype=float32), 'eval/episode_y_velocity': Array(-42.360184, dtype=float32), 'eval/avg_episode_length': Array(97., dtype=float32), 'eval/epoch_eval_time': 3.270911455154419, 'eval/sps': 305.72518202049287}
9000.0 {'eval/walltime': 36.81963300704956, 'training/sps': 30.084987012605456, 'training/walltime': 306.788213968277, 'training/model_train_time': 6.187543153762817, 'training/other_time': 27.046292543411255, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 800, 'training/env_buffer_size': Array(9000, dtype=int32), 'model/train_total_loss': Array(-15.163156, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3952268, dtype=float32), 'model/test_total_loss': Array(-11.106115, dtype=float32), 'model/test_mean_loss': Array(1.9582428, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 0.5045547485351562, 'sac/actor_loss': Array(2717.663, dtype=float32), 'sac/alpha': Array(0.20000003, dtype=float32), 'sac/alpha_loss': Array(-5.036258, dtype=float32), 'sac/buffer_current_size': Array(161197.75, dtype=float32), 'sac/critic_loss': Array(64704.5, dtype=float32), 'eval/episode_distance_from_origin': Array(221.71774, dtype=float32), 'eval/episode_forward_reward': Array(-94.45134, dtype=float32), 'eval/episode_reward': Array(-258.07767, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-254.746, dtype=float32), 'eval/episode_reward_forward': Array(-94.45134, dtype=float32), 'eval/episode_reward_survive': Array(96., dtype=float32), 'eval/episode_x_position': Array(-189.89946, dtype=float32), 'eval/episode_x_velocity': Array(-94.45134, dtype=float32), 'eval/episode_y_position': Array(-77.09086, dtype=float32), 'eval/episode_y_velocity': Array(-42.360184, dtype=float32), 'eval/avg_episode_length': Array(97., dtype=float32), 'eval/epoch_eval_time': 3.270911455154419, 'eval/sps': 305.72518202049287, 'steps': Array(9000., dtype=float32)}
Model horizon updated to 1.
Hallucination updates per training step updated to 900.
SAC buffer resized to 360000 samples.
Model epoch 0: train total loss -12.852670669555664, train mean loss 1.7020790576934814, test mean loss [2.1949692 2.4846082 1.893152  1.8670467 1.8722402 1.38714   1.2287428]
Model epoch 1: train total loss -11.788134574890137, train mean loss 1.7100087404251099, test mean loss [2.6060643 2.46918   1.8945955 1.9357588 1.9492141 1.403659  1.2331498]
Model epoch 2: train total loss -12.842215538024902, train mean loss 1.610044240951538, test mean loss [2.391798  2.5661557 1.9032633 1.9309845 1.9451679 1.418687  1.2282397]
Model epoch 3: train total loss -12.609950065612793, train mean loss 1.5930533409118652, test mean loss [2.2806406 2.5093114 1.9008105 2.4233313 1.8880885 1.3984436 1.2551354]
Model epoch 4: train total loss -12.955674171447754, train mean loss 1.5926414728164673, test mean loss [2.2694428 2.5376713 1.858246  2.041724  1.9098942 1.4033544 1.2542285]
Model epoch 5: train total loss -12.992708206176758, train mean loss 1.5869991779327393, test mean loss [2.263813  2.6677928 1.9021909 2.0640984 1.982995  1.4313426 1.2753081]
Model epoch 6: train total loss -14.16907787322998, train mean loss 1.4445089101791382, test mean loss [2.2492476 2.6691298 1.8393643 2.0583444 1.9305494 1.419763  1.2932684]
Model epoch 7: train total loss -13.587167739868164, train mean loss 1.541878581047058, test mean loss [2.2867754 2.5982506 1.8934232 1.9994638 1.9136934 1.4414297 1.2780814]
Model epoch 8: train total loss -13.926139831542969, train mean loss 1.538223385810852, test mean loss [2.2364342 2.5921617 1.8288221 2.0208948 1.921572  1.4553822 1.2754703]
Model epoch 9: train total loss -14.44701099395752, train mean loss 1.4681899547576904, test mean loss [2.2741394 2.5811021 1.8389909 2.0567045 1.9473727 1.4495976 1.2440495]
Model epoch 10: train total loss -14.466745376586914, train mean loss 1.4757933616638184, test mean loss [2.2244425 2.5655072 1.8271147 2.0709324 1.9558403 1.4659762 1.2735367]
Model epoch 11: train total loss -14.64590835571289, train mean loss 1.4798649549484253, test mean loss [2.250575  2.5720766 1.8400902 2.0738983 1.9184822 1.419493  1.2831025]
Model epoch 12: train total loss -14.8306245803833, train mean loss 1.4168809652328491, test mean loss [2.2857144 2.6935718 1.872011  2.1118336 1.860635  1.4481509 1.2991742]
Model trained in 13 epochs with 9000 transitions.
[2025-01-29 16:43:34,653][absl][INFO] - {'eval/walltime': 40.10497426986694, 'training/sps': 30.72218768078146, 'training/walltime': 339.3379793167114, 'training/model_train_time': 7.218359470367432, 'training/other_time': 25.32632875442505, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 900, 'training/env_buffer_size': Array(10000, dtype=int32), 'model/train_total_loss': Array(-14.830625, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.416881, dtype=float32), 'model/test_total_loss': Array(-10.747347, dtype=float32), 'model/test_mean_loss': Array(1.9387274, dtype=float32), 'model/train_epochs': 13, 'model/sec_per_epoch': 0.39512903873737043, 'sac/actor_loss': Array(3188.3303, dtype=float32), 'sac/alpha': Array(0.2, dtype=float32), 'sac/alpha_loss': Array(-5.307451, dtype=float32), 'sac/buffer_current_size': Array(181198.02, dtype=float32), 'sac/critic_loss': Array(80043.14, dtype=float32), 'eval/episode_distance_from_origin': Array(7.6475835, dtype=float32), 'eval/episode_forward_reward': Array(-0.87574863, dtype=float32), 'eval/episode_reward': Array(-20.880344, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-26.36132, dtype=float32), 'eval/episode_reward_forward': Array(-0.87574863, dtype=float32), 'eval/episode_reward_survive': Array(9., dtype=float32), 'eval/episode_x_position': Array(-0.21191448, dtype=float32), 'eval/episode_x_velocity': Array(-0.87574863, dtype=float32), 'eval/episode_y_position': Array(0.52189773, dtype=float32), 'eval/episode_y_velocity': Array(-0.6972982, dtype=float32), 'eval/avg_episode_length': Array(10., dtype=float32), 'eval/epoch_eval_time': 3.285341262817383, 'eval/sps': 304.38238222547335}
10000.0 {'eval/walltime': 40.10497426986694, 'training/sps': 30.72218768078146, 'training/walltime': 339.3379793167114, 'training/model_train_time': 7.218359470367432, 'training/other_time': 25.32632875442505, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 900, 'training/env_buffer_size': Array(10000, dtype=int32), 'model/train_total_loss': Array(-14.830625, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.416881, dtype=float32), 'model/test_total_loss': Array(-10.747347, dtype=float32), 'model/test_mean_loss': Array(1.9387274, dtype=float32), 'model/train_epochs': 13, 'model/sec_per_epoch': 0.39512903873737043, 'sac/actor_loss': Array(3188.3303, dtype=float32), 'sac/alpha': Array(0.2, dtype=float32), 'sac/alpha_loss': Array(-5.307451, dtype=float32), 'sac/buffer_current_size': Array(181198.02, dtype=float32), 'sac/critic_loss': Array(80043.14, dtype=float32), 'eval/episode_distance_from_origin': Array(7.6475835, dtype=float32), 'eval/episode_forward_reward': Array(-0.87574863, dtype=float32), 'eval/episode_reward': Array(-20.880344, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-26.36132, dtype=float32), 'eval/episode_reward_forward': Array(-0.87574863, dtype=float32), 'eval/episode_reward_survive': Array(9., dtype=float32), 'eval/episode_x_position': Array(-0.21191448, dtype=float32), 'eval/episode_x_velocity': Array(-0.87574863, dtype=float32), 'eval/episode_y_position': Array(0.52189773, dtype=float32), 'eval/episode_y_velocity': Array(-0.6972982, dtype=float32), 'eval/avg_episode_length': Array(10., dtype=float32), 'eval/epoch_eval_time': 3.285341262817383, 'eval/sps': 304.38238222547335, 'steps': Array(10000., dtype=float32)}
Model horizon updated to 1.
Hallucination updates per training step updated to 1000.
SAC buffer resized to 400000 samples.
Model epoch 0: train total loss -12.631628036499023, train mean loss 1.5895978212356567, test mean loss [1.7763878 1.6658348 1.7522391 1.6931384 1.6396161 1.7344655 1.7807881]
Model epoch 1: train total loss -13.014131546020508, train mean loss 1.6630724668502808, test mean loss [1.7802553 1.6365012 1.7006131 1.7039015 1.6769058 1.7669561 1.7956165]
Model epoch 2: train total loss -13.749253273010254, train mean loss 1.6108548641204834, test mean loss [1.7947068 1.6699841 1.6990755 1.7284638 1.7011623 1.7509652 1.7584287]
Model epoch 3: train total loss -13.212960243225098, train mean loss 1.6518019437789917, test mean loss [1.786234  1.6770844 1.7710309 1.7386672 1.6998243 1.750169  1.760062 ]
Model epoch 4: train total loss -14.131599426269531, train mean loss 1.5649352073669434, test mean loss [1.8140332 1.6993777 1.7307535 1.7209918 1.6946813 1.7341173 1.8018352]
Model epoch 5: train total loss -14.0482816696167, train mean loss 1.5338943004608154, test mean loss [1.7750076 1.7579721 1.763986  1.7362126 1.6905005 1.76216   1.8165438]
Model epoch 6: train total loss -14.326031684875488, train mean loss 1.523892879486084, test mean loss [1.8156638 1.750831  1.7630463 1.7532206 1.6978452 1.7568346 1.8069141]
Model epoch 7: train total loss -14.507213592529297, train mean loss 1.4860483407974243, test mean loss [1.8000141 1.7267929 1.7357742 1.7473218 1.716275  1.8143092 1.8404045]
Model epoch 8: train total loss -14.379219055175781, train mean loss 1.5737764835357666, test mean loss [1.7931275 1.7550149 1.7985182 1.7468822 1.7257469 1.7820818 1.8298587]
Model trained in 9 epochs with 10000 transitions.
[2025-01-29 16:44:10,450][absl][INFO] - {'eval/walltime': 43.37683701515198, 'training/sps': 30.786979117517994, 'training/walltime': 371.8192434310913, 'training/model_train_time': 6.5822837352752686, 'training/other_time': 25.89344573020935, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(11000, dtype=int32), 'model/train_total_loss': Array(-14.379219, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.5737765, dtype=float32), 'model/test_total_loss': Array(-12.054277, dtype=float32), 'model/test_mean_loss': Array(1.7758901, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 0.5036327573988173, 'sac/actor_loss': Array(3538.146, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-6.2692633, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(110376.84, dtype=float32), 'eval/episode_distance_from_origin': Array(10.33184, dtype=float32), 'eval/episode_forward_reward': Array(-5.830557, dtype=float32), 'eval/episode_reward': Array(-28.744469, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-31.2646, dtype=float32), 'eval/episode_reward_forward': Array(-5.830557, dtype=float32), 'eval/episode_reward_survive': Array(11., dtype=float32), 'eval/episode_x_position': Array(-1.396122, dtype=float32), 'eval/episode_x_velocity': Array(-5.830557, dtype=float32), 'eval/episode_y_position': Array(-2.8473167, dtype=float32), 'eval/episode_y_velocity': Array(-9.087333, dtype=float32), 'eval/avg_episode_length': Array(12., dtype=float32), 'eval/epoch_eval_time': 3.271862745285034, 'eval/sps': 305.6362927940864}
11000.0 {'eval/walltime': 43.37683701515198, 'training/sps': 30.786979117517994, 'training/walltime': 371.8192434310913, 'training/model_train_time': 6.5822837352752686, 'training/other_time': 25.89344573020935, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(11000, dtype=int32), 'model/train_total_loss': Array(-14.379219, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.5737765, dtype=float32), 'model/test_total_loss': Array(-12.054277, dtype=float32), 'model/test_mean_loss': Array(1.7758901, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 0.5036327573988173, 'sac/actor_loss': Array(3538.146, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-6.2692633, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(110376.84, dtype=float32), 'eval/episode_distance_from_origin': Array(10.33184, dtype=float32), 'eval/episode_forward_reward': Array(-5.830557, dtype=float32), 'eval/episode_reward': Array(-28.744469, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-31.2646, dtype=float32), 'eval/episode_reward_forward': Array(-5.830557, dtype=float32), 'eval/episode_reward_survive': Array(11., dtype=float32), 'eval/episode_x_position': Array(-1.396122, dtype=float32), 'eval/episode_x_velocity': Array(-5.830557, dtype=float32), 'eval/episode_y_position': Array(-2.8473167, dtype=float32), 'eval/episode_y_velocity': Array(-9.087333, dtype=float32), 'eval/avg_episode_length': Array(12., dtype=float32), 'eval/epoch_eval_time': 3.271862745285034, 'eval/sps': 305.6362927940864, 'steps': Array(11000., dtype=float32)}
Model epoch 0: train total loss -13.445231437683105, train mean loss 1.6435173749923706, test mean loss [1.7509547 1.6940697 1.6781874 1.6990236 1.7374586 1.7277163 1.8562727]
Model epoch 1: train total loss -14.501041412353516, train mean loss 1.5243141651153564, test mean loss [1.8045012 1.7461407 1.6979198 1.7306349 1.7859846 1.7374457 1.8699498]
Model epoch 2: train total loss -14.493350982666016, train mean loss 1.5711405277252197, test mean loss [1.7944298 1.7752963 1.7039496 1.7241197 1.8078331 1.7425628 1.8539014]
Model epoch 3: train total loss -15.011478424072266, train mean loss 1.5410621166229248, test mean loss [1.8154423 1.7648513 1.7353638 1.7535886 1.7749363 1.769846  1.8718218]
Model epoch 4: train total loss -14.808708190917969, train mean loss 1.4828879833221436, test mean loss [1.8165917 1.791616  1.7495909 1.7421237 1.7901495 1.7693424 1.860276 ]
Model epoch 5: train total loss -15.258753776550293, train mean loss 1.492867112159729, test mean loss [1.8395617 1.779005  1.7663643 1.782869  1.79591   1.7701564 1.8764641]
Model epoch 6: train total loss -15.664612770080566, train mean loss 1.4264447689056396, test mean loss [1.8377073 1.7949339 1.7716185 1.7840302 1.8537165 1.7968674 1.873662 ]
Model trained in 7 epochs with 11000 transitions.
[2025-01-29 16:44:27,888][absl][INFO] - {'eval/walltime': 46.65075492858887, 'training/sps': 70.64952425356793, 'training/walltime': 385.9736204147339, 'training/model_train_time': 5.617029428482056, 'training/other_time': 8.530169010162354, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(12000, dtype=int32), 'model/train_total_loss': Array(-15.664613, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4264448, dtype=float32), 'model/test_total_loss': Array(-12.282657, dtype=float32), 'model/test_mean_loss': Array(1.8160766, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.5376785142081124, 'sac/actor_loss': Array(3755.9583, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-6.8524218, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(115870.7, dtype=float32), 'eval/episode_distance_from_origin': Array(3.0338166, dtype=float32), 'eval/episode_forward_reward': Array(5.2632656, dtype=float32), 'eval/episode_reward': Array(-2.181411, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-11.831751, dtype=float32), 'eval/episode_reward_forward': Array(5.2632656, dtype=float32), 'eval/episode_reward_survive': Array(4., dtype=float32), 'eval/episode_x_position': Array(0.5908003, dtype=float32), 'eval/episode_x_velocity': Array(5.2632656, dtype=float32), 'eval/episode_y_position': Array(0.4735046, dtype=float32), 'eval/episode_y_velocity': Array(-0.43368208, dtype=float32), 'eval/avg_episode_length': Array(5., dtype=float32), 'eval/epoch_eval_time': 3.2739179134368896, 'eval/sps': 305.4444327683895}
12000.0 {'eval/walltime': 46.65075492858887, 'training/sps': 70.64952425356793, 'training/walltime': 385.9736204147339, 'training/model_train_time': 5.617029428482056, 'training/other_time': 8.530169010162354, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(12000, dtype=int32), 'model/train_total_loss': Array(-15.664613, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4264448, dtype=float32), 'model/test_total_loss': Array(-12.282657, dtype=float32), 'model/test_mean_loss': Array(1.8160766, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.5376785142081124, 'sac/actor_loss': Array(3755.9583, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-6.8524218, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(115870.7, dtype=float32), 'eval/episode_distance_from_origin': Array(3.0338166, dtype=float32), 'eval/episode_forward_reward': Array(5.2632656, dtype=float32), 'eval/episode_reward': Array(-2.181411, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-11.831751, dtype=float32), 'eval/episode_reward_forward': Array(5.2632656, dtype=float32), 'eval/episode_reward_survive': Array(4., dtype=float32), 'eval/episode_x_position': Array(0.5908003, dtype=float32), 'eval/episode_x_velocity': Array(5.2632656, dtype=float32), 'eval/episode_y_position': Array(0.4735046, dtype=float32), 'eval/episode_y_velocity': Array(-0.43368208, dtype=float32), 'eval/avg_episode_length': Array(5., dtype=float32), 'eval/epoch_eval_time': 3.2739179134368896, 'eval/sps': 305.4444327683895, 'steps': Array(12000., dtype=float32)}
Model epoch 0: train total loss -14.117727279663086, train mean loss 1.5484709739685059, test mean loss [1.8961065 1.8299986 1.8938628 1.8799036 1.8762479 1.9316343 1.9321709]
Model epoch 1: train total loss -13.909930229187012, train mean loss 1.6197782754898071, test mean loss [1.9284456 1.8565812 1.9222423 1.8773134 1.8653919 1.9298055 1.9697819]
Model epoch 2: train total loss -14.21446704864502, train mean loss 1.5870097875595093, test mean loss [1.9522307 1.8865972 1.9154407 1.909107  1.8859974 1.9292675 1.9335766]
Model epoch 3: train total loss -14.205926895141602, train mean loss 1.6183298826217651, test mean loss [1.9483155 1.9236681 1.9553552 1.9164611 1.8973067 1.9304608 1.9658072]
Model epoch 4: train total loss -15.196931838989258, train mean loss 1.5088838338851929, test mean loss [1.9824123 1.9190339 1.9245864 1.9368488 1.9341618 1.9316188 1.996105 ]
Model epoch 5: train total loss -15.334376335144043, train mean loss 1.4811155796051025, test mean loss [1.9647077 1.9461343 1.9255676 1.9531084 1.909466  1.9715502 2.0131145]
Model epoch 6: train total loss -16.050701141357422, train mean loss 1.3757189512252808, test mean loss [1.973924  1.955207  1.9472656 1.9619683 1.9495612 1.9543631 2.0029578]
Model trained in 7 epochs with 12000 transitions.
[2025-01-29 16:44:46,273][absl][INFO] - {'eval/walltime': 49.93855023384094, 'training/sps': 66.28331433449462, 'training/walltime': 401.060373544693, 'training/model_train_time': 6.531374931335449, 'training/other_time': 8.547792673110962, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(13000, dtype=int32), 'model/train_total_loss': Array(-16.050701, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.375719, dtype=float32), 'model/test_total_loss': Array(-12.063439, dtype=float32), 'model/test_mean_loss': Array(1.9636068, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.6404459135872977, 'sac/actor_loss': Array(3806.5864, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.209506, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(127700.195, dtype=float32), 'eval/episode_distance_from_origin': Array(14.872665, dtype=float32), 'eval/episode_forward_reward': Array(-3.0728593, dtype=float32), 'eval/episode_reward': Array(-46.936134, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-60.013256, dtype=float32), 'eval/episode_reward_forward': Array(-3.0728593, dtype=float32), 'eval/episode_reward_survive': Array(19., dtype=float32), 'eval/episode_x_position': Array(1.7529552, dtype=float32), 'eval/episode_x_velocity': Array(-3.0728593, dtype=float32), 'eval/episode_y_position': Array(5.073524, dtype=float32), 'eval/episode_y_velocity': Array(-1.3473657, dtype=float32), 'eval/avg_episode_length': Array(20., dtype=float32), 'eval/epoch_eval_time': 3.287795305252075, 'eval/sps': 304.15518825109154}
13000.0 {'eval/walltime': 49.93855023384094, 'training/sps': 66.28331433449462, 'training/walltime': 401.060373544693, 'training/model_train_time': 6.531374931335449, 'training/other_time': 8.547792673110962, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(13000, dtype=int32), 'model/train_total_loss': Array(-16.050701, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.375719, dtype=float32), 'model/test_total_loss': Array(-12.063439, dtype=float32), 'model/test_mean_loss': Array(1.9636068, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.6404459135872977, 'sac/actor_loss': Array(3806.5864, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.209506, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(127700.195, dtype=float32), 'eval/episode_distance_from_origin': Array(14.872665, dtype=float32), 'eval/episode_forward_reward': Array(-3.0728593, dtype=float32), 'eval/episode_reward': Array(-46.936134, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-60.013256, dtype=float32), 'eval/episode_reward_forward': Array(-3.0728593, dtype=float32), 'eval/episode_reward_survive': Array(19., dtype=float32), 'eval/episode_x_position': Array(1.7529552, dtype=float32), 'eval/episode_x_velocity': Array(-3.0728593, dtype=float32), 'eval/episode_y_position': Array(5.073524, dtype=float32), 'eval/episode_y_velocity': Array(-1.3473657, dtype=float32), 'eval/avg_episode_length': Array(20., dtype=float32), 'eval/epoch_eval_time': 3.287795305252075, 'eval/sps': 304.15518825109154, 'steps': Array(13000., dtype=float32)}
Model epoch 0: train total loss -14.139873504638672, train mean loss 1.595407247543335, test mean loss [1.7581853 1.7284418 1.76812   1.7906475 1.7279013 1.7287301 1.7470592]
Model epoch 1: train total loss -14.474510192871094, train mean loss 1.6113194227218628, test mean loss [1.7879776 1.7517357 1.772911  1.8106631 1.7481598 1.7588379 1.7784541]
Model epoch 2: train total loss -14.988210678100586, train mean loss 1.5253344774246216, test mean loss [1.794459  1.7668259 1.7782145 1.8300095 1.7784884 1.7779846 1.7826207]
Model epoch 3: train total loss -15.246630668640137, train mean loss 1.4932271242141724, test mean loss [1.8406851 1.8035687 1.8070666 1.8212185 1.7865925 1.8006495 1.7828726]
Model epoch 4: train total loss -15.342817306518555, train mean loss 1.485355019569397, test mean loss [1.8267847 1.807901  1.7895066 1.8598505 1.8004684 1.8383532 1.8048954]
Model epoch 5: train total loss -15.310382843017578, train mean loss 1.4720431566238403, test mean loss [1.8568544 1.8252461 1.8085423 1.8995806 1.7921908 1.8593892 1.7953762]
Model epoch 6: train total loss -15.486043930053711, train mean loss 1.4546897411346436, test mean loss [1.8536711 1.814186  1.8477272 1.867458  1.7954152 1.8511137 1.8029419]
Model trained in 7 epochs with 13000 transitions.
[2025-01-29 16:45:04,480][absl][INFO] - {'eval/walltime': 53.21919393539429, 'training/sps': 67.04412726342048, 'training/walltime': 415.97592306137085, 'training/model_train_time': 6.3603105545043945, 'training/other_time': 8.548458337783813, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(14000, dtype=int32), 'model/train_total_loss': Array(-15.486044, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4546897, dtype=float32), 'model/test_total_loss': Array(-12.701877, dtype=float32), 'model/test_mean_loss': Array(1.8332162, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.6270309175763812, 'sac/actor_loss': Array(4035.239, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.3953943, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(147010.92, dtype=float32), 'eval/episode_distance_from_origin': Array(2.9953363, dtype=float32), 'eval/episode_forward_reward': Array(0.9930753, dtype=float32), 'eval/episode_reward': Array(-8.157174, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-11.865514, dtype=float32), 'eval/episode_reward_forward': Array(0.9930753, dtype=float32), 'eval/episode_reward_survive': Array(4., dtype=float32), 'eval/episode_x_position': Array(0.34947667, dtype=float32), 'eval/episode_x_velocity': Array(0.9930753, dtype=float32), 'eval/episode_y_position': Array(-0.18182707, dtype=float32), 'eval/episode_y_velocity': Array(1.1811136, dtype=float32), 'eval/avg_episode_length': Array(5., dtype=float32), 'eval/epoch_eval_time': 3.2806437015533447, 'eval/sps': 304.8182280588752}
14000.0 {'eval/walltime': 53.21919393539429, 'training/sps': 67.04412726342048, 'training/walltime': 415.97592306137085, 'training/model_train_time': 6.3603105545043945, 'training/other_time': 8.548458337783813, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(14000, dtype=int32), 'model/train_total_loss': Array(-15.486044, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4546897, dtype=float32), 'model/test_total_loss': Array(-12.701877, dtype=float32), 'model/test_mean_loss': Array(1.8332162, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.6270309175763812, 'sac/actor_loss': Array(4035.239, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.3953943, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(147010.92, dtype=float32), 'eval/episode_distance_from_origin': Array(2.9953363, dtype=float32), 'eval/episode_forward_reward': Array(0.9930753, dtype=float32), 'eval/episode_reward': Array(-8.157174, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-11.865514, dtype=float32), 'eval/episode_reward_forward': Array(0.9930753, dtype=float32), 'eval/episode_reward_survive': Array(4., dtype=float32), 'eval/episode_x_position': Array(0.34947667, dtype=float32), 'eval/episode_x_velocity': Array(0.9930753, dtype=float32), 'eval/episode_y_position': Array(-0.18182707, dtype=float32), 'eval/episode_y_velocity': Array(1.1811136, dtype=float32), 'eval/avg_episode_length': Array(5., dtype=float32), 'eval/epoch_eval_time': 3.2806437015533447, 'eval/sps': 304.8182280588752, 'steps': Array(14000., dtype=float32)}
Model epoch 0: train total loss -14.557292938232422, train mean loss 1.5512491464614868, test mean loss [1.6708189 1.6561091 1.6402628 1.6455711 1.6557018 1.6304061 1.7016039]
Model epoch 1: train total loss -14.233808517456055, train mean loss 1.6278570890426636, test mean loss [1.6909587 1.6763548 1.6493301 1.6700774 1.6939901 1.6471102 1.7226248]
Model epoch 2: train total loss -14.326763153076172, train mean loss 1.620578408241272, test mean loss [1.7270709 1.7157688 1.6880175 1.6851842 1.6976123 1.6763991 1.7640994]
Model epoch 3: train total loss -16.023326873779297, train mean loss 1.405908226966858, test mean loss [1.7552946 1.7142714 1.6990535 1.6923835 1.6984926 1.6878383 1.746734 ]
Model epoch 4: train total loss -16.160612106323242, train mean loss 1.3818199634552002, test mean loss [1.7380172 1.7445943 1.696522  1.6936187 1.709601  1.7151483 1.7698653]
Model epoch 5: train total loss -15.838278770446777, train mean loss 1.4726089239120483, test mean loss [1.7930316 1.7378963 1.709514  1.6979258 1.7002065 1.7117975 1.7783282]
Model epoch 6: train total loss -16.298553466796875, train mean loss 1.4444278478622437, test mean loss [1.7511684 1.7465057 1.6957927 1.7037776 1.7504644 1.7199881 1.8261043]
Model trained in 7 epochs with 14000 transitions.
[2025-01-29 16:45:23,176][absl][INFO] - {'eval/walltime': 56.5073344707489, 'training/sps': 64.94217986921308, 'training/walltime': 431.374235868454, 'training/model_train_time': 6.840149641036987, 'training/other_time': 8.551581382751465, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(15000, dtype=int32), 'model/train_total_loss': Array(-16.298553, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4444278, dtype=float32), 'model/test_total_loss': Array(-13.687483, dtype=float32), 'model/test_mean_loss': Array(1.7419716, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.6787097113473075, 'sac/actor_loss': Array(4208.331, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.5760827, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(172196.3, dtype=float32), 'eval/episode_distance_from_origin': Array(3.9568436, dtype=float32), 'eval/episode_forward_reward': Array(-0.59760594, dtype=float32), 'eval/episode_reward': Array(-13.631995, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-17.107452, dtype=float32), 'eval/episode_reward_forward': Array(-0.59760594, dtype=float32), 'eval/episode_reward_survive': Array(5., dtype=float32), 'eval/episode_x_position': Array(-0.3372917, dtype=float32), 'eval/episode_x_velocity': Array(-0.59760594, dtype=float32), 'eval/episode_y_position': Array(-0.14239377, dtype=float32), 'eval/episode_y_velocity': Array(0.01332104, dtype=float32), 'eval/avg_episode_length': Array(6., dtype=float32), 'eval/epoch_eval_time': 3.2881405353546143, 'eval/sps': 304.12325423680636}
15000.0 {'eval/walltime': 56.5073344707489, 'training/sps': 64.94217986921308, 'training/walltime': 431.374235868454, 'training/model_train_time': 6.840149641036987, 'training/other_time': 8.551581382751465, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(15000, dtype=int32), 'model/train_total_loss': Array(-16.298553, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4444278, dtype=float32), 'model/test_total_loss': Array(-13.687483, dtype=float32), 'model/test_mean_loss': Array(1.7419716, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.6787097113473075, 'sac/actor_loss': Array(4208.331, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.5760827, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(172196.3, dtype=float32), 'eval/episode_distance_from_origin': Array(3.9568436, dtype=float32), 'eval/episode_forward_reward': Array(-0.59760594, dtype=float32), 'eval/episode_reward': Array(-13.631995, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-17.107452, dtype=float32), 'eval/episode_reward_forward': Array(-0.59760594, dtype=float32), 'eval/episode_reward_survive': Array(5., dtype=float32), 'eval/episode_x_position': Array(-0.3372917, dtype=float32), 'eval/episode_x_velocity': Array(-0.59760594, dtype=float32), 'eval/episode_y_position': Array(-0.14239377, dtype=float32), 'eval/episode_y_velocity': Array(0.01332104, dtype=float32), 'eval/avg_episode_length': Array(6., dtype=float32), 'eval/epoch_eval_time': 3.2881405353546143, 'eval/sps': 304.12325423680636, 'steps': Array(15000., dtype=float32)}
Model epoch 0: train total loss -14.712640762329102, train mean loss 1.5600199699401855, test mean loss [1.7721857 1.7758987 1.7503426 1.7653248 1.7305231 1.7248752 1.7760334]
Model epoch 1: train total loss -15.689698219299316, train mean loss 1.5329252481460571, test mean loss [1.8071456 1.7750437 1.7949761 1.7616622 1.7540736 1.770921  1.8015126]
Model epoch 2: train total loss -15.455911636352539, train mean loss 1.5586127042770386, test mean loss [1.8065374 1.8185337 1.8171186 1.8038543 1.8281711 1.7948889 1.8358703]
Model epoch 3: train total loss -15.875283241271973, train mean loss 1.4860050678253174, test mean loss [1.8425591 1.8109416 1.828504  1.8184294 1.8048792 1.8216057 1.831536 ]
Model epoch 4: train total loss -16.00832748413086, train mean loss 1.5318589210510254, test mean loss [1.8686705 1.8586893 1.8354874 1.8511789 1.8250892 1.8117447 1.8669505]
Model epoch 5: train total loss -16.519018173217773, train mean loss 1.4333224296569824, test mean loss [1.882587  1.8604517 1.8580297 1.8561954 1.8428563 1.8278722 1.8770561]
Model epoch 6: train total loss -16.696107864379883, train mean loss 1.4643386602401733, test mean loss [1.8672274 1.8599794 1.8688161 1.8957655 1.8568934 1.8375826 1.8762348]
Model trained in 7 epochs with 15000 transitions.
[2025-01-29 16:45:41,964][absl][INFO] - {'eval/walltime': 59.782713413238525, 'training/sps': 64.51127072024428, 'training/walltime': 446.87540316581726, 'training/model_train_time': 6.944274187088013, 'training/other_time': 8.54955768585205, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(16000, dtype=int32), 'model/train_total_loss': Array(-16.696108, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4643387, dtype=float32), 'model/test_total_loss': Array(-12.241401, dtype=float32), 'model/test_mean_loss': Array(1.8660713, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.7007875442504883, 'sac/actor_loss': Array(4043.5298, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.669648, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(155118.48, dtype=float32), 'eval/episode_distance_from_origin': Array(8.654596, dtype=float32), 'eval/episode_forward_reward': Array(8.380749, dtype=float32), 'eval/episode_reward': Array(-16.326859, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-34.671883, dtype=float32), 'eval/episode_reward_forward': Array(8.380749, dtype=float32), 'eval/episode_reward_survive': Array(11., dtype=float32), 'eval/episode_x_position': Array(0.86020404, dtype=float32), 'eval/episode_x_velocity': Array(8.380749, dtype=float32), 'eval/episode_y_position': Array(0.6904291, dtype=float32), 'eval/episode_y_velocity': Array(2.8259, dtype=float32), 'eval/avg_episode_length': Array(12., dtype=float32), 'eval/epoch_eval_time': 3.275378942489624, 'eval/sps': 305.30818496375184}
16000.0 {'eval/walltime': 59.782713413238525, 'training/sps': 64.51127072024428, 'training/walltime': 446.87540316581726, 'training/model_train_time': 6.944274187088013, 'training/other_time': 8.54955768585205, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(16000, dtype=int32), 'model/train_total_loss': Array(-16.696108, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4643387, dtype=float32), 'model/test_total_loss': Array(-12.241401, dtype=float32), 'model/test_mean_loss': Array(1.8660713, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.7007875442504883, 'sac/actor_loss': Array(4043.5298, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.669648, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(155118.48, dtype=float32), 'eval/episode_distance_from_origin': Array(8.654596, dtype=float32), 'eval/episode_forward_reward': Array(8.380749, dtype=float32), 'eval/episode_reward': Array(-16.326859, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-34.671883, dtype=float32), 'eval/episode_reward_forward': Array(8.380749, dtype=float32), 'eval/episode_reward_survive': Array(11., dtype=float32), 'eval/episode_x_position': Array(0.86020404, dtype=float32), 'eval/episode_x_velocity': Array(8.380749, dtype=float32), 'eval/episode_y_position': Array(0.6904291, dtype=float32), 'eval/episode_y_velocity': Array(2.8259, dtype=float32), 'eval/avg_episode_length': Array(12., dtype=float32), 'eval/epoch_eval_time': 3.275378942489624, 'eval/sps': 305.30818496375184, 'steps': Array(16000., dtype=float32)}
Model epoch 0: train total loss -15.471741676330566, train mean loss 1.578970193862915, test mean loss [1.6308435 1.5280523 1.5496573 1.5547271 1.5433531 1.556857  1.5680579]
Model epoch 1: train total loss -15.688565254211426, train mean loss 1.5334511995315552, test mean loss [1.6361976 1.5636877 1.5676792 1.5676961 1.5676858 1.5979356 1.6161646]
Model epoch 2: train total loss -16.09221839904785, train mean loss 1.605616807937622, test mean loss [1.6587067 1.5760748 1.6087105 1.5617955 1.5753769 1.6141872 1.6226556]
Model epoch 3: train total loss -16.634824752807617, train mean loss 1.4764240980148315, test mean loss [1.6516407 1.5773343 1.6045158 1.600085  1.6120495 1.6185882 1.6378505]
Model epoch 4: train total loss -16.42826271057129, train mean loss 1.559071660041809, test mean loss [1.6749303 1.5941634 1.6096029 1.5931672 1.6240367 1.6243685 1.6619163]
Model epoch 5: train total loss -17.579700469970703, train mean loss 1.4176065921783447, test mean loss [1.6932964 1.5943221 1.6429622 1.6252116 1.6162529 1.6598177 1.6721858]
Model epoch 6: train total loss -17.945804595947266, train mean loss 1.3815540075302124, test mean loss [1.6897085 1.6201794 1.6451628 1.6242855 1.6183723 1.654495  1.6786532]
Model trained in 7 epochs with 16000 transitions.
[2025-01-29 16:46:00,319][absl][INFO] - {'eval/walltime': 63.07785987854004, 'training/sps': 66.44472189521083, 'training/walltime': 461.9255075454712, 'training/model_train_time': 6.441378831863403, 'training/other_time': 8.602113723754883, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(17000, dtype=int32), 'model/train_total_loss': Array(-17.945805, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.381554, dtype=float32), 'model/test_total_loss': Array(-14.4406, dtype=float32), 'model/test_mean_loss': Array(1.6472653, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.6616931983402797, 'sac/actor_loss': Array(4170.009, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.7970185, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(161879.95, dtype=float32), 'eval/episode_distance_from_origin': Array(12.859524, dtype=float32), 'eval/episode_forward_reward': Array(5.875473, dtype=float32), 'eval/episode_reward': Array(-32.867992, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-51.801876, dtype=float32), 'eval/episode_reward_forward': Array(5.875473, dtype=float32), 'eval/episode_reward_survive': Array(16., dtype=float32), 'eval/episode_x_position': Array(4.139454, dtype=float32), 'eval/episode_x_velocity': Array(5.875473, dtype=float32), 'eval/episode_y_position': Array(-0.6198507, dtype=float32), 'eval/episode_y_velocity': Array(4.886013, dtype=float32), 'eval/avg_episode_length': Array(17., dtype=float32), 'eval/epoch_eval_time': 3.2951464653015137, 'eval/sps': 303.47664679861134}
17000.0 {'eval/walltime': 63.07785987854004, 'training/sps': 66.44472189521083, 'training/walltime': 461.9255075454712, 'training/model_train_time': 6.441378831863403, 'training/other_time': 8.602113723754883, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(17000, dtype=int32), 'model/train_total_loss': Array(-17.945805, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.381554, dtype=float32), 'model/test_total_loss': Array(-14.4406, dtype=float32), 'model/test_mean_loss': Array(1.6472653, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.6616931983402797, 'sac/actor_loss': Array(4170.009, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.7970185, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(161879.95, dtype=float32), 'eval/episode_distance_from_origin': Array(12.859524, dtype=float32), 'eval/episode_forward_reward': Array(5.875473, dtype=float32), 'eval/episode_reward': Array(-32.867992, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-51.801876, dtype=float32), 'eval/episode_reward_forward': Array(5.875473, dtype=float32), 'eval/episode_reward_survive': Array(16., dtype=float32), 'eval/episode_x_position': Array(4.139454, dtype=float32), 'eval/episode_x_velocity': Array(5.875473, dtype=float32), 'eval/episode_y_position': Array(-0.6198507, dtype=float32), 'eval/episode_y_velocity': Array(4.886013, dtype=float32), 'eval/avg_episode_length': Array(17., dtype=float32), 'eval/epoch_eval_time': 3.2951464653015137, 'eval/sps': 303.47664679861134, 'steps': Array(17000., dtype=float32)}
Model epoch 0: train total loss -13.973697662353516, train mean loss 1.715339183807373, test mean loss [1.6758038 1.8499439 1.6793905 1.755314  1.6704061 2.081943  1.6503193]
Model epoch 1: train total loss -16.527666091918945, train mean loss 1.4883522987365723, test mean loss [1.709405  1.7266002 1.6975541 1.7093972 1.6634957 1.8274195 1.6720936]
Model epoch 2: train total loss -17.642887115478516, train mean loss 1.427611231803894, test mean loss [1.7035451 1.7242836 1.7261292 1.6683221 1.7073641 1.8140036 1.6973162]
Model epoch 3: train total loss -17.0870361328125, train mean loss 1.4835501909255981, test mean loss [1.7495505 1.7485336 1.6972054 1.6995169 1.7170656 1.7940952 1.701059 ]
Model epoch 4: train total loss -17.45711326599121, train mean loss 1.464471697807312, test mean loss [1.7372496 1.7172424 1.7494935 1.7304207 1.7282895 1.7944727 1.7334152]
Model epoch 5: train total loss -17.48737144470215, train mean loss 1.4790436029434204, test mean loss [1.7558774 1.7164209 1.7497942 1.7861025 1.7339543 1.7986672 1.755881 ]
Model epoch 6: train total loss -17.97211456298828, train mean loss 1.3926198482513428, test mean loss [1.7551997 1.7548016 1.7771908 1.7661047 1.7536203 1.8048779 1.772394 ]
Model epoch 7: train total loss -18.466777801513672, train mean loss 1.3844667673110962, test mean loss [1.7993836 1.7479652 1.7547848 1.7845814 1.7362509 1.7952075 1.784173 ]
Model epoch 8: train total loss -18.513151168823242, train mean loss 1.3793370723724365, test mean loss [1.8012061 1.7579724 1.8033788 1.7668678 1.7619478 1.827264  1.7523228]
Model epoch 9: train total loss -18.51535415649414, train mean loss 1.412595272064209, test mean loss [1.7959939 1.7871877 1.7832005 1.786719  1.7556869 1.8487424 1.7990465]
Model trained in 10 epochs with 17000 transitions.
[2025-01-29 16:46:20,774][absl][INFO] - {'eval/walltime': 66.36574673652649, 'training/sps': 58.282501377578136, 'training/walltime': 479.0833160877228, 'training/model_train_time': 8.60389232635498, 'training/other_time': 8.547138452529907, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(18000, dtype=int32), 'model/train_total_loss': Array(-18.515354, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4125953, dtype=float32), 'model/test_total_loss': Array(-14.545498, dtype=float32), 'model/test_mean_loss': Array(1.7937968, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 0.6502691030502319, 'sac/actor_loss': Array(4050.4424, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.013992, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(154285.67, dtype=float32), 'eval/episode_distance_from_origin': Array(3.0273392, dtype=float32), 'eval/episode_forward_reward': Array(0.99191284, dtype=float32), 'eval/episode_reward': Array(-10.3633585, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-13.751109, dtype=float32), 'eval/episode_reward_forward': Array(0.99191284, dtype=float32), 'eval/episode_reward_survive': Array(4., dtype=float32), 'eval/episode_x_position': Array(0.1735716, dtype=float32), 'eval/episode_x_velocity': Array(0.99191284, dtype=float32), 'eval/episode_y_position': Array(0.07530754, dtype=float32), 'eval/episode_y_velocity': Array(1.6124119, dtype=float32), 'eval/avg_episode_length': Array(5., dtype=float32), 'eval/epoch_eval_time': 3.28788685798645, 'eval/sps': 304.1467189088175}
18000.0 {'eval/walltime': 66.36574673652649, 'training/sps': 58.282501377578136, 'training/walltime': 479.0833160877228, 'training/model_train_time': 8.60389232635498, 'training/other_time': 8.547138452529907, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(18000, dtype=int32), 'model/train_total_loss': Array(-18.515354, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4125953, dtype=float32), 'model/test_total_loss': Array(-14.545498, dtype=float32), 'model/test_mean_loss': Array(1.7937968, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 0.6502691030502319, 'sac/actor_loss': Array(4050.4424, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.013992, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(154285.67, dtype=float32), 'eval/episode_distance_from_origin': Array(3.0273392, dtype=float32), 'eval/episode_forward_reward': Array(0.99191284, dtype=float32), 'eval/episode_reward': Array(-10.3633585, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-13.751109, dtype=float32), 'eval/episode_reward_forward': Array(0.99191284, dtype=float32), 'eval/episode_reward_survive': Array(4., dtype=float32), 'eval/episode_x_position': Array(0.1735716, dtype=float32), 'eval/episode_x_velocity': Array(0.99191284, dtype=float32), 'eval/episode_y_position': Array(0.07530754, dtype=float32), 'eval/episode_y_velocity': Array(1.6124119, dtype=float32), 'eval/avg_episode_length': Array(5., dtype=float32), 'eval/epoch_eval_time': 3.28788685798645, 'eval/sps': 304.1467189088175, 'steps': Array(18000., dtype=float32)}
Model epoch 0: train total loss -16.344783782958984, train mean loss 1.6327096223831177, test mean loss [1.7870013 1.6548673 1.4557507 1.5979686 1.5491345 1.5928385 1.920314 ]
Model epoch 1: train total loss -16.886301040649414, train mean loss 1.5746285915374756, test mean loss [1.9156963 1.6627194 1.6782672 1.6177312 1.5751756 1.611623  1.7446492]
Model epoch 2: train total loss -17.769399642944336, train mean loss 1.4242619276046753, test mean loss [1.8289604 1.6691457 1.4713379 1.7394577 1.5698161 1.5647473 1.8402717]
Model epoch 3: train total loss -18.19651222229004, train mean loss 1.4247567653656006, test mean loss [1.8715156 1.6650069 1.5526572 1.6966003 1.5455056 1.5499014 1.7315389]
Model epoch 4: train total loss -18.390830993652344, train mean loss 1.412042498588562, test mean loss [1.8388715 1.6762573 1.5425804 1.7055806 1.6115308 1.593529  1.7218313]
Model epoch 5: train total loss -18.41888999938965, train mean loss 1.446282148361206, test mean loss [1.8886752 1.6759967 1.5595084 1.6915882 1.5723141 1.5781158 1.7032977]
Model epoch 6: train total loss -19.311735153198242, train mean loss 1.3152539730072021, test mean loss [1.923163  1.713031  1.5678147 1.7077153 1.6093217 1.5898187 1.7091032]
Model epoch 7: train total loss -19.152158737182617, train mean loss 1.3471399545669556, test mean loss [1.8851866 1.7613313 1.5409687 1.6690105 1.6864306 1.5974908 1.7084838]
Model epoch 8: train total loss -18.63522720336914, train mean loss 1.4357469081878662, test mean loss [1.9516586 1.7101781 1.6189657 1.7013348 1.6609384 1.6190904 1.698049 ]
Model epoch 9: train total loss -19.710968017578125, train mean loss 1.3112881183624268, test mean loss [2.0237846 1.7210523 1.6238521 1.6633855 1.6736838 1.612924  1.7056848]
Model epoch 10: train total loss -18.973291397094727, train mean loss 1.397979497909546, test mean loss [1.9879295 1.7543254 1.5992951 1.7184278 1.6195514 1.6554086 1.7170177]
Model epoch 11: train total loss -19.865915298461914, train mean loss 1.3091330528259277, test mean loss [1.9514796 1.7864408 1.6566793 1.7555497 1.6430631 1.6766621 1.7315859]
Model trained in 12 epochs with 18000 transitions.
[2025-01-29 16:46:42,708][absl][INFO] - {'eval/walltime': 69.65370011329651, 'training/sps': 53.65732881041062, 'training/walltime': 497.72009921073914, 'training/model_train_time': 10.08533263206482, 'training/other_time': 8.544795751571655, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(19000, dtype=int32), 'model/train_total_loss': Array(-19.865915, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.309133, dtype=float32), 'model/test_total_loss': Array(-15.7620945, dtype=float32), 'model/test_mean_loss': Array(1.743066, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 0.6509079734484354, 'sac/actor_loss': Array(3764.6655, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.137434, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(151467.08, dtype=float32), 'eval/episode_distance_from_origin': Array(4.2968974, dtype=float32), 'eval/episode_forward_reward': Array(2.6447222, dtype=float32), 'eval/episode_reward': Array(-11.38688, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-17.479092, dtype=float32), 'eval/episode_reward_forward': Array(2.6447222, dtype=float32), 'eval/episode_reward_survive': Array(5., dtype=float32), 'eval/episode_x_position': Array(0.26706246, dtype=float32), 'eval/episode_x_velocity': Array(2.6447222, dtype=float32), 'eval/episode_y_position': Array(1.1464078, dtype=float32), 'eval/episode_y_velocity': Array(6.34345, dtype=float32), 'eval/avg_episode_length': Array(6., dtype=float32), 'eval/epoch_eval_time': 3.2879533767700195, 'eval/sps': 304.1405656981572}
19000.0 {'eval/walltime': 69.65370011329651, 'training/sps': 53.65732881041062, 'training/walltime': 497.72009921073914, 'training/model_train_time': 10.08533263206482, 'training/other_time': 8.544795751571655, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(19000, dtype=int32), 'model/train_total_loss': Array(-19.865915, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.309133, dtype=float32), 'model/test_total_loss': Array(-15.7620945, dtype=float32), 'model/test_mean_loss': Array(1.743066, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 0.6509079734484354, 'sac/actor_loss': Array(3764.6655, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.137434, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(151467.08, dtype=float32), 'eval/episode_distance_from_origin': Array(4.2968974, dtype=float32), 'eval/episode_forward_reward': Array(2.6447222, dtype=float32), 'eval/episode_reward': Array(-11.38688, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-17.479092, dtype=float32), 'eval/episode_reward_forward': Array(2.6447222, dtype=float32), 'eval/episode_reward_survive': Array(5., dtype=float32), 'eval/episode_x_position': Array(0.26706246, dtype=float32), 'eval/episode_x_velocity': Array(2.6447222, dtype=float32), 'eval/episode_y_position': Array(1.1464078, dtype=float32), 'eval/episode_y_velocity': Array(6.34345, dtype=float32), 'eval/avg_episode_length': Array(6., dtype=float32), 'eval/epoch_eval_time': 3.2879533767700195, 'eval/sps': 304.1405656981572, 'steps': Array(19000., dtype=float32)}
Model epoch 0: train total loss -17.988862991333008, train mean loss 1.457995057106018, test mean loss [1.5213317 1.5631262 1.5646939 1.5608927 1.577807  1.5864241 1.5610335]
Model epoch 1: train total loss -19.026870727539062, train mean loss 1.3759592771530151, test mean loss [1.5675478 1.551089  1.6054251 1.6070122 1.5891486 1.6011252 1.5671983]
Model epoch 2: train total loss -18.755111694335938, train mean loss 1.4471107721328735, test mean loss [1.5751119 1.5850264 1.6155332 1.6288465 1.6164864 1.6271552 1.5777949]
Model epoch 3: train total loss -18.906007766723633, train mean loss 1.4381943941116333, test mean loss [1.5726982 1.5935619 1.6364529 1.6338133 1.6195691 1.6354319 1.5889546]
Model epoch 4: train total loss -19.00309944152832, train mean loss 1.4509607553482056, test mean loss [1.5908738 1.6096232 1.6307216 1.6366544 1.6304951 1.6676801 1.5832886]
Model epoch 5: train total loss -19.221267700195312, train mean loss 1.4397412538528442, test mean loss [1.5941327 1.6192849 1.6472347 1.6660438 1.6453637 1.6729922 1.642251 ]
Model epoch 6: train total loss -19.537729263305664, train mean loss 1.4198135137557983, test mean loss [1.6341162 1.621462  1.6673517 1.6481379 1.6337409 1.7083026 1.6335609]
Model trained in 7 epochs with 19000 transitions.
[2025-01-29 16:47:02,319][absl][INFO] - {'eval/walltime': 72.9268696308136, 'training/sps': 61.244500258466935, 'training/walltime': 514.0480959415436, 'training/model_train_time': 7.771981239318848, 'training/other_time': 8.549221992492676, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(20000, dtype=int32), 'model/train_total_loss': Array(-19.53773, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4198135, dtype=float32), 'model/test_total_loss': Array(-16.738178, dtype=float32), 'model/test_mean_loss': Array(1.6495248, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.7932063170841762, 'sac/actor_loss': Array(3820.9104, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.98952, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(150840.86, dtype=float32), 'eval/episode_distance_from_origin': Array(10.384459, dtype=float32), 'eval/episode_forward_reward': Array(-5.0896215, dtype=float32), 'eval/episode_reward': Array(-41.774036, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-47.967846, dtype=float32), 'eval/episode_reward_forward': Array(-5.0896215, dtype=float32), 'eval/episode_reward_survive': Array(15., dtype=float32), 'eval/episode_x_position': Array(-1.4309367, dtype=float32), 'eval/episode_x_velocity': Array(-5.0896215, dtype=float32), 'eval/episode_y_position': Array(0.07157654, dtype=float32), 'eval/episode_y_velocity': Array(-1.1806313, dtype=float32), 'eval/avg_episode_length': Array(16., dtype=float32), 'eval/epoch_eval_time': 3.27316951751709, 'eval/sps': 305.5142713043975}
20000.0 {'eval/walltime': 72.9268696308136, 'training/sps': 61.244500258466935, 'training/walltime': 514.0480959415436, 'training/model_train_time': 7.771981239318848, 'training/other_time': 8.549221992492676, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(20000, dtype=int32), 'model/train_total_loss': Array(-19.53773, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4198135, dtype=float32), 'model/test_total_loss': Array(-16.738178, dtype=float32), 'model/test_mean_loss': Array(1.6495248, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.7932063170841762, 'sac/actor_loss': Array(3820.9104, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.98952, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(150840.86, dtype=float32), 'eval/episode_distance_from_origin': Array(10.384459, dtype=float32), 'eval/episode_forward_reward': Array(-5.0896215, dtype=float32), 'eval/episode_reward': Array(-41.774036, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-47.967846, dtype=float32), 'eval/episode_reward_forward': Array(-5.0896215, dtype=float32), 'eval/episode_reward_survive': Array(15., dtype=float32), 'eval/episode_x_position': Array(-1.4309367, dtype=float32), 'eval/episode_x_velocity': Array(-5.0896215, dtype=float32), 'eval/episode_y_position': Array(0.07157654, dtype=float32), 'eval/episode_y_velocity': Array(-1.1806313, dtype=float32), 'eval/avg_episode_length': Array(16., dtype=float32), 'eval/epoch_eval_time': 3.27316951751709, 'eval/sps': 305.5142713043975, 'steps': Array(20000., dtype=float32)}
Model epoch 0: train total loss -18.219337463378906, train mean loss 1.4897836446762085, test mean loss [1.5519927 1.4963664 1.5626227 1.5862691 1.527288  1.6607966 1.5250641]
Model epoch 1: train total loss -17.40021514892578, train mean loss 1.6294798851013184, test mean loss [1.6726146 1.5333233 1.5900116 1.6382294 1.5356083 1.6013814 1.5608346]
Model epoch 2: train total loss -18.768178939819336, train mean loss 1.4810715913772583, test mean loss [1.5867324 1.5594014 1.5723375 1.5914054 1.5765997 1.6262599 1.5955929]
Model epoch 3: train total loss -18.875856399536133, train mean loss 1.4550141096115112, test mean loss [1.6065693 1.5639008 1.5856209 1.6150323 1.5913206 1.8106523 1.603245 ]
Model epoch 4: train total loss -19.427574157714844, train mean loss 1.4272047281265259, test mean loss [1.5697571 1.5786027 1.6090245 1.6136776 1.6038768 1.6924063 1.6079359]
Model epoch 5: train total loss -20.119604110717773, train mean loss 1.3417881727218628, test mean loss [1.5967181 1.5845919 1.6387556 1.6161816 1.6053959 1.6694872 1.6157165]
Model epoch 6: train total loss -20.444080352783203, train mean loss 1.3083618879318237, test mean loss [1.6044753 1.6060042 1.6390822 1.6501317 1.6111939 1.6727232 1.6476753]
Model epoch 7: train total loss -20.267017364501953, train mean loss 1.3321757316589355, test mean loss [1.6030635 1.6303499 1.66673   1.6556988 1.6168336 1.6751778 1.662185 ]
Model trained in 8 epochs with 20000 transitions.
[2025-01-29 16:47:22,244][absl][INFO] - {'eval/walltime': 76.21446561813354, 'training/sps': 60.135984934461185, 'training/walltime': 530.677074432373, 'training/model_train_time': 8.07377314567566, 'training/other_time': 8.548313856124878, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(21000, dtype=int32), 'model/train_total_loss': Array(-20.267017, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3321757, dtype=float32), 'model/test_total_loss': Array(-16.672478, dtype=float32), 'model/test_mean_loss': Array(1.6442912, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 0.7576989233493805, 'sac/actor_loss': Array(3791.7544, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.066328, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(152988.92, dtype=float32), 'eval/episode_distance_from_origin': Array(4.5823407, dtype=float32), 'eval/episode_forward_reward': Array(-5.7754745, dtype=float32), 'eval/episode_reward': Array(-25.99059, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-22.876314, dtype=float32), 'eval/episode_reward_forward': Array(-5.7754745, dtype=float32), 'eval/episode_reward_survive': Array(7., dtype=float32), 'eval/episode_x_position': Array(-0.8933455, dtype=float32), 'eval/episode_x_velocity': Array(-5.7754745, dtype=float32), 'eval/episode_y_position': Array(0.99676764, dtype=float32), 'eval/episode_y_velocity': Array(0.6976999, dtype=float32), 'eval/avg_episode_length': Array(8., dtype=float32), 'eval/epoch_eval_time': 3.2875959873199463, 'eval/sps': 304.1736283463473}
21000.0 {'eval/walltime': 76.21446561813354, 'training/sps': 60.135984934461185, 'training/walltime': 530.677074432373, 'training/model_train_time': 8.07377314567566, 'training/other_time': 8.548313856124878, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(21000, dtype=int32), 'model/train_total_loss': Array(-20.267017, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3321757, dtype=float32), 'model/test_total_loss': Array(-16.672478, dtype=float32), 'model/test_mean_loss': Array(1.6442912, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 0.7576989233493805, 'sac/actor_loss': Array(3791.7544, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.066328, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(152988.92, dtype=float32), 'eval/episode_distance_from_origin': Array(4.5823407, dtype=float32), 'eval/episode_forward_reward': Array(-5.7754745, dtype=float32), 'eval/episode_reward': Array(-25.99059, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-22.876314, dtype=float32), 'eval/episode_reward_forward': Array(-5.7754745, dtype=float32), 'eval/episode_reward_survive': Array(7., dtype=float32), 'eval/episode_x_position': Array(-0.8933455, dtype=float32), 'eval/episode_x_velocity': Array(-5.7754745, dtype=float32), 'eval/episode_y_position': Array(0.99676764, dtype=float32), 'eval/episode_y_velocity': Array(0.6976999, dtype=float32), 'eval/avg_episode_length': Array(8., dtype=float32), 'eval/epoch_eval_time': 3.2875959873199463, 'eval/sps': 304.1736283463473, 'steps': Array(21000., dtype=float32)}
Model epoch 0: train total loss -18.59425163269043, train mean loss 1.4359911680221558, test mean loss [1.5529623 1.6512736 1.6076523 1.6060671 1.5399114 1.5914168 1.60258  ]
Model epoch 1: train total loss -19.30423927307129, train mean loss 1.4938570261001587, test mean loss [1.554266  1.5557923 1.5877029 1.6475579 1.539971  1.6078929 1.5652157]
Model epoch 2: train total loss -19.985883712768555, train mean loss 1.4238388538360596, test mean loss [1.5904615 1.5790684 1.6052532 1.6448416 1.5629436 1.7114028 1.5757715]
Model epoch 3: train total loss -18.917814254760742, train mean loss 1.5413888692855835, test mean loss [1.5753002 1.6032008 1.5991498 1.6362156 1.7999517 1.6425195 1.6007411]
Model epoch 4: train total loss -20.25802993774414, train mean loss 1.3537925481796265, test mean loss [1.5968227 1.6037309 1.6275473 1.6583239 1.621013  1.6848879 1.6136591]
Model epoch 5: train total loss -20.55147933959961, train mean loss 1.37919020652771, test mean loss [1.6096566 1.628173  1.6587605 1.6663696 1.5916581 1.6805443 1.6414624]
Model epoch 6: train total loss -20.398279190063477, train mean loss 1.362785816192627, test mean loss [1.6359384 1.6268066 1.6818763 1.6638932 1.6519624 1.6727905 1.6358491]
Model epoch 7: train total loss -21.009601593017578, train mean loss 1.3351205587387085, test mean loss [1.6453114 1.624061  1.699528  1.6862031 1.6203221 1.6820822 1.6615641]
Model trained in 8 epochs with 21000 transitions.
[2025-01-29 16:47:42,189][absl][INFO] - {'eval/walltime': 79.49874901771545, 'training/sps': 60.05674879822183, 'training/walltime': 547.32799243927, 'training/model_train_time': 8.097330808639526, 'training/other_time': 8.546391010284424, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(22000, dtype=int32), 'model/train_total_loss': Array(-21.009602, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3351206, dtype=float32), 'model/test_total_loss': Array(-17.49641, dtype=float32), 'model/test_mean_loss': Array(1.6598675, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 0.7658686339855194, 'sac/actor_loss': Array(3523.9507, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.100229, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(136289.9, dtype=float32), 'eval/episode_distance_from_origin': Array(5.7919874, dtype=float32), 'eval/episode_forward_reward': Array(-6.2677402, dtype=float32), 'eval/episode_reward': Array(-32.045258, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-30.543884, dtype=float32), 'eval/episode_reward_forward': Array(-6.2677402, dtype=float32), 'eval/episode_reward_survive': Array(9., dtype=float32), 'eval/episode_x_position': Array(-1.4162114, dtype=float32), 'eval/episode_x_velocity': Array(-6.2677402, dtype=float32), 'eval/episode_y_position': Array(0.2343707, dtype=float32), 'eval/episode_y_velocity': Array(1.5215449, dtype=float32), 'eval/avg_episode_length': Array(10., dtype=float32), 'eval/epoch_eval_time': 3.284283399581909, 'eval/sps': 304.4804233785977}
22000.0 {'eval/walltime': 79.49874901771545, 'training/sps': 60.05674879822183, 'training/walltime': 547.32799243927, 'training/model_train_time': 8.097330808639526, 'training/other_time': 8.546391010284424, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(22000, dtype=int32), 'model/train_total_loss': Array(-21.009602, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3351206, dtype=float32), 'model/test_total_loss': Array(-17.49641, dtype=float32), 'model/test_mean_loss': Array(1.6598675, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 0.7658686339855194, 'sac/actor_loss': Array(3523.9507, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.100229, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(136289.9, dtype=float32), 'eval/episode_distance_from_origin': Array(5.7919874, dtype=float32), 'eval/episode_forward_reward': Array(-6.2677402, dtype=float32), 'eval/episode_reward': Array(-32.045258, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-30.543884, dtype=float32), 'eval/episode_reward_forward': Array(-6.2677402, dtype=float32), 'eval/episode_reward_survive': Array(9., dtype=float32), 'eval/episode_x_position': Array(-1.4162114, dtype=float32), 'eval/episode_x_velocity': Array(-6.2677402, dtype=float32), 'eval/episode_y_position': Array(0.2343707, dtype=float32), 'eval/episode_y_velocity': Array(1.5215449, dtype=float32), 'eval/avg_episode_length': Array(10., dtype=float32), 'eval/epoch_eval_time': 3.284283399581909, 'eval/sps': 304.4804233785977, 'steps': Array(22000., dtype=float32)}
Model epoch 0: train total loss -19.065500259399414, train mean loss 1.5452874898910522, test mean loss [1.4796065 1.5023005 1.4851373 1.5116712 1.4758987 1.5576078 1.4934384]
Model epoch 1: train total loss -19.38517951965332, train mean loss 1.4847337007522583, test mean loss [1.4869992 1.4947882 1.5182438 1.6291171 1.5059347 1.5945665 1.6053802]
Model epoch 2: train total loss -19.45053482055664, train mean loss 1.4741753339767456, test mean loss [1.5024395 1.5169302 1.5159667 1.5708663 1.5088311 1.584489  1.5392505]
Model epoch 3: train total loss -19.588645935058594, train mean loss 1.4869295358657837, test mean loss [1.5058105 1.5580722 1.5269432 1.5743059 1.5480899 1.6353505 1.5400002]
Model epoch 4: train total loss -20.720924377441406, train mean loss 1.426253080368042, test mean loss [1.4986278 1.5527755 1.5519363 1.5590613 1.5446116 1.6043679 1.5581759]
Model epoch 5: train total loss -20.219707489013672, train mean loss 1.410893201828003, test mean loss [1.5176111 1.5604491 1.5538146 1.5743138 1.5565789 1.6324359 1.5749503]
Model epoch 6: train total loss -20.452089309692383, train mean loss 1.4152004718780518, test mean loss [1.5268534 1.5825944 1.5734615 1.5908223 1.550658  1.6091737 1.589761 ]
Model trained in 7 epochs with 22000 transitions.
[2025-01-29 16:48:02,364][absl][INFO] - {'eval/walltime': 82.78165197372437, 'training/sps': 59.23330431761415, 'training/walltime': 564.2103867530823, 'training/model_train_time': 8.328430652618408, 'training/other_time': 8.547162294387817, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(23000, dtype=int32), 'model/train_total_loss': Array(-20.45209, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4152005, dtype=float32), 'model/test_total_loss': Array(-18.629145, dtype=float32), 'model/test_mean_loss': Array(1.5747607, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.8798649651663644, 'sac/actor_loss': Array(3514.3223, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.061401, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(131738.55, dtype=float32), 'eval/episode_distance_from_origin': Array(8.194859, dtype=float32), 'eval/episode_forward_reward': Array(-0.5247302, dtype=float32), 'eval/episode_reward': Array(-23.450817, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-30.815897, dtype=float32), 'eval/episode_reward_forward': Array(-0.5247302, dtype=float32), 'eval/episode_reward_survive': Array(10., dtype=float32), 'eval/episode_x_position': Array(-1.4291931, dtype=float32), 'eval/episode_x_velocity': Array(-0.5247302, dtype=float32), 'eval/episode_y_position': Array(1.7606994, dtype=float32), 'eval/episode_y_velocity': Array(1.4112962, dtype=float32), 'eval/avg_episode_length': Array(11., dtype=float32), 'eval/epoch_eval_time': 3.282902956008911, 'eval/sps': 304.60845580879413}
23000.0 {'eval/walltime': 82.78165197372437, 'training/sps': 59.23330431761415, 'training/walltime': 564.2103867530823, 'training/model_train_time': 8.328430652618408, 'training/other_time': 8.547162294387817, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(23000, dtype=int32), 'model/train_total_loss': Array(-20.45209, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4152005, dtype=float32), 'model/test_total_loss': Array(-18.629145, dtype=float32), 'model/test_mean_loss': Array(1.5747607, dtype=float32), 'model/train_epochs': 7, 'model/sec_per_epoch': 0.8798649651663644, 'sac/actor_loss': Array(3514.3223, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.061401, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(131738.55, dtype=float32), 'eval/episode_distance_from_origin': Array(8.194859, dtype=float32), 'eval/episode_forward_reward': Array(-0.5247302, dtype=float32), 'eval/episode_reward': Array(-23.450817, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-30.815897, dtype=float32), 'eval/episode_reward_forward': Array(-0.5247302, dtype=float32), 'eval/episode_reward_survive': Array(10., dtype=float32), 'eval/episode_x_position': Array(-1.4291931, dtype=float32), 'eval/episode_x_velocity': Array(-0.5247302, dtype=float32), 'eval/episode_y_position': Array(1.7606994, dtype=float32), 'eval/episode_y_velocity': Array(1.4112962, dtype=float32), 'eval/avg_episode_length': Array(11., dtype=float32), 'eval/epoch_eval_time': 3.282902956008911, 'eval/sps': 304.60845580879413, 'steps': Array(23000., dtype=float32)}
Model epoch 0: train total loss -18.630769729614258, train mean loss 1.539011836051941, test mean loss [1.7746688 1.5534087 1.586522  1.6095821 1.5620285 1.6267809 1.5530612]
Model epoch 1: train total loss -19.942638397216797, train mean loss 1.463638186454773, test mean loss [1.6882443 1.5771877 1.6443648 1.6393007 1.5838608 1.6543387 1.5639256]
Model epoch 2: train total loss -19.601511001586914, train mean loss 1.47455632686615, test mean loss [1.6770457 1.6016879 1.6057564 1.6541746 1.5992696 1.6778556 1.6067182]
Model epoch 3: train total loss -19.320493698120117, train mean loss 1.574467420578003, test mean loss [1.6576256 1.6116114 1.634399  1.6544929 1.6189542 1.6893327 1.5981127]
Model epoch 4: train total loss -20.61446762084961, train mean loss 1.4152017831802368, test mean loss [1.6515002 1.607191  1.6555202 1.6675469 1.6408354 1.6929666 1.6325499]
Model epoch 5: train total loss -21.337696075439453, train mean loss 1.3440120220184326, test mean loss [1.6544914 1.639821  1.6791397 1.6900128 1.6505516 1.7075402 1.6394339]
Model epoch 6: train total loss -20.576534271240234, train mean loss 1.3932896852493286, test mean loss [1.6822134 1.6616503 1.6994662 1.7028759 1.6523091 1.7015265 1.6575212]
Model epoch 7: train total loss -20.992870330810547, train mean loss 1.395470142364502, test mean loss [1.6731766 1.6525865 1.6907545 1.6885709 1.6715902 1.735521  1.6514946]
Model epoch 8: train total loss -20.73458480834961, train mean loss 1.392300009727478, test mean loss [1.6674635 1.6444001 1.6778303 1.7581815 1.6738888 1.7445283 1.6596227]
Model epoch 9: train total loss -22.23909568786621, train mean loss 1.1933791637420654, test mean loss [1.6571671 1.6759284 1.7049598 1.733083  1.6980829 1.7669083 1.6795475]
Model trained in 10 epochs with 23000 transitions.
[2025-01-29 16:48:24,542][absl][INFO] - {'eval/walltime': 86.05869746208191, 'training/sps': 52.938053486933605, 'training/walltime': 583.100389957428, 'training/model_train_time': 10.330357789993286, 'training/other_time': 8.552665948867798, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(24000, dtype=int32), 'model/train_total_loss': Array(-22.239096, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.1933792, dtype=float32), 'model/test_total_loss': Array(-17.923275, dtype=float32), 'model/test_mean_loss': Array(1.7022396, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 0.8133739233016968, 'sac/actor_loss': Array(3217.698, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.188386, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(124085.84, dtype=float32), 'eval/episode_distance_from_origin': Array(13.947666, dtype=float32), 'eval/episode_forward_reward': Array(6.3477836, dtype=float32), 'eval/episode_reward': Array(-33.993927, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-54.514156, dtype=float32), 'eval/episode_reward_forward': Array(6.3477836, dtype=float32), 'eval/episode_reward_survive': Array(17., dtype=float32), 'eval/episode_x_position': Array(5.1422453, dtype=float32), 'eval/episode_x_velocity': Array(6.3477836, dtype=float32), 'eval/episode_y_position': Array(3.1406727, dtype=float32), 'eval/episode_y_velocity': Array(3.8091629, dtype=float32), 'eval/avg_episode_length': Array(18., dtype=float32), 'eval/epoch_eval_time': 3.277045488357544, 'eval/sps': 305.1529200777742}
24000.0 {'eval/walltime': 86.05869746208191, 'training/sps': 52.938053486933605, 'training/walltime': 583.100389957428, 'training/model_train_time': 10.330357789993286, 'training/other_time': 8.552665948867798, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(24000, dtype=int32), 'model/train_total_loss': Array(-22.239096, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.1933792, dtype=float32), 'model/test_total_loss': Array(-17.923275, dtype=float32), 'model/test_mean_loss': Array(1.7022396, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 0.8133739233016968, 'sac/actor_loss': Array(3217.698, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.188386, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(124085.84, dtype=float32), 'eval/episode_distance_from_origin': Array(13.947666, dtype=float32), 'eval/episode_forward_reward': Array(6.3477836, dtype=float32), 'eval/episode_reward': Array(-33.993927, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-54.514156, dtype=float32), 'eval/episode_reward_forward': Array(6.3477836, dtype=float32), 'eval/episode_reward_survive': Array(17., dtype=float32), 'eval/episode_x_position': Array(5.1422453, dtype=float32), 'eval/episode_x_velocity': Array(6.3477836, dtype=float32), 'eval/episode_y_position': Array(3.1406727, dtype=float32), 'eval/episode_y_velocity': Array(3.8091629, dtype=float32), 'eval/avg_episode_length': Array(18., dtype=float32), 'eval/epoch_eval_time': 3.277045488357544, 'eval/sps': 305.1529200777742, 'steps': Array(24000., dtype=float32)}
Model epoch 0: train total loss -20.106687545776367, train mean loss 1.4098973274230957, test mean loss [1.5860504 1.522446  1.5746559 1.598054  1.5623935 1.6365315 1.6404628]
Model epoch 1: train total loss -20.191221237182617, train mean loss 1.3706529140472412, test mean loss [1.587277  1.5630848 1.6132172 1.5990179 1.5774072 1.7251205 1.6040796]
Model epoch 2: train total loss -20.768245697021484, train mean loss 1.337133526802063, test mean loss [1.613183  1.5700467 1.6150889 1.6054921 1.5701246 1.692633  1.614773 ]
Model epoch 3: train total loss -20.751598358154297, train mean loss 1.408642053604126, test mean loss [1.6169987 1.5981909 1.6302048 1.6475744 1.6218647 1.6693671 1.6249305]
Model epoch 4: train total loss -20.45863914489746, train mean loss 1.3967366218566895, test mean loss [1.6403327 1.5921113 1.6494392 1.6430923 1.6009063 1.7110301 1.6207832]
Model epoch 5: train total loss -21.508298873901367, train mean loss 1.359544038772583, test mean loss [1.624271  1.6222993 1.6842395 1.6543418 1.611108  1.7069033 1.6582749]
Model epoch 6: train total loss -21.2694149017334, train mean loss 1.3947378396987915, test mean loss [1.6693487 1.629158  1.6618861 1.6869023 1.623028  1.74849   1.6458975]
Model epoch 7: train total loss -21.037919998168945, train mean loss 1.3363224267959595, test mean loss [1.655073  1.6444924 1.6623845 1.6917454 1.6226635 1.7446371 1.6788062]
Model trained in 8 epochs with 24000 transitions.
[2025-01-29 16:48:45,922][absl][INFO] - {'eval/walltime': 89.34019064903259, 'training/sps': 55.28086052116953, 'training/walltime': 601.1898334026337, 'training/model_train_time': 9.533942222595215, 'training/other_time': 8.548727035522461, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(25000, dtype=int32), 'model/train_total_loss': Array(-21.03792, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3363224, dtype=float32), 'model/test_total_loss': Array(-18.49842, dtype=float32), 'model/test_mean_loss': Array(1.6714004, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 0.8914667367935181, 'sac/actor_loss': Array(2864.8994, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.072344, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(101303.305, dtype=float32), 'eval/episode_distance_from_origin': Array(5.2415333, dtype=float32), 'eval/episode_forward_reward': Array(-3.0927417, dtype=float32), 'eval/episode_reward': Array(-24.649977, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-25.21671, dtype=float32), 'eval/episode_reward_forward': Array(-3.0927417, dtype=float32), 'eval/episode_reward_survive': Array(8., dtype=float32), 'eval/episode_x_position': Array(-0.77312946, dtype=float32), 'eval/episode_x_velocity': Array(-3.0927417, dtype=float32), 'eval/episode_y_position': Array(0.6695412, dtype=float32), 'eval/episode_y_velocity': Array(2.622969, dtype=float32), 'eval/avg_episode_length': Array(9., dtype=float32), 'eval/epoch_eval_time': 3.2814931869506836, 'eval/sps': 304.7393192759442}
25000.0 {'eval/walltime': 89.34019064903259, 'training/sps': 55.28086052116953, 'training/walltime': 601.1898334026337, 'training/model_train_time': 9.533942222595215, 'training/other_time': 8.548727035522461, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(25000, dtype=int32), 'model/train_total_loss': Array(-21.03792, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3363224, dtype=float32), 'model/test_total_loss': Array(-18.49842, dtype=float32), 'model/test_mean_loss': Array(1.6714004, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 0.8914667367935181, 'sac/actor_loss': Array(2864.8994, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.072344, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(101303.305, dtype=float32), 'eval/episode_distance_from_origin': Array(5.2415333, dtype=float32), 'eval/episode_forward_reward': Array(-3.0927417, dtype=float32), 'eval/episode_reward': Array(-24.649977, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-25.21671, dtype=float32), 'eval/episode_reward_forward': Array(-3.0927417, dtype=float32), 'eval/episode_reward_survive': Array(8., dtype=float32), 'eval/episode_x_position': Array(-0.77312946, dtype=float32), 'eval/episode_x_velocity': Array(-3.0927417, dtype=float32), 'eval/episode_y_position': Array(0.6695412, dtype=float32), 'eval/episode_y_velocity': Array(2.622969, dtype=float32), 'eval/avg_episode_length': Array(9., dtype=float32), 'eval/epoch_eval_time': 3.2814931869506836, 'eval/sps': 304.7393192759442, 'steps': Array(25000., dtype=float32)}
Model epoch 0: train total loss -19.547269821166992, train mean loss 1.5150288343429565, test mean loss [1.542085  1.5440446 1.5433502 1.5904992 1.5396212 1.6325362 1.6300999]
Model epoch 1: train total loss -19.519500732421875, train mean loss 1.5386797189712524, test mean loss [1.5235932 1.6387485 1.6066494 1.5960709 1.5781788 1.6087668 1.5603496]
Model epoch 2: train total loss -20.772916793823242, train mean loss 1.4158179759979248, test mean loss [1.5524842 1.5508409 1.5935348 1.5952094 1.5528852 1.6072613 1.5766443]
Model epoch 3: train total loss -21.35319709777832, train mean loss 1.361238956451416, test mean loss [1.5623627 1.570542  1.6085596 1.5898608 1.6332483 1.6445305 1.5773045]
Model epoch 4: train total loss -20.86840057373047, train mean loss 1.420393705368042, test mean loss [1.5704571 1.5584794 1.6438582 1.6153036 1.5755056 1.6664675 1.5826759]
Model epoch 5: train total loss -21.53855323791504, train mean loss 1.3398839235305786, test mean loss [1.6093706 1.563941  1.6630375 1.6341652 1.5914885 1.6668959 1.5929192]
Model epoch 6: train total loss -21.579957962036133, train mean loss 1.3253380060195923, test mean loss [1.6086514 1.5708836 1.6486809 1.6699313 1.5773976 1.6769041 1.5880198]
Model epoch 7: train total loss -21.499542236328125, train mean loss 1.3111016750335693, test mean loss [1.60609   1.5708779 1.6624781 1.6821965 1.5917494 1.6813844 1.6232346]
Model trained in 8 epochs with 25000 transitions.
[2025-01-29 16:49:06,866][absl][INFO] - {'eval/walltime': 92.62897729873657, 'training/sps': 56.670361187512825, 'training/walltime': 618.8357417583466, 'training/model_train_time': 9.089518547058105, 'training/other_time': 8.54959774017334, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(26000, dtype=int32), 'model/train_total_loss': Array(-21.499542, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3111017, dtype=float32), 'model/test_total_loss': Array(-18.933973, dtype=float32), 'model/test_mean_loss': Array(1.6311444, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 0.88984614610672, 'sac/actor_loss': Array(2841.1428, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.8340898, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(100859.336, dtype=float32), 'eval/episode_distance_from_origin': Array(6.090193, dtype=float32), 'eval/episode_forward_reward': Array(-2.306536, dtype=float32), 'eval/episode_reward': Array(-25.066967, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-31.416386, dtype=float32), 'eval/episode_reward_forward': Array(-2.306536, dtype=float32), 'eval/episode_reward_survive': Array(10., dtype=float32), 'eval/episode_x_position': Array(-2.0055375, dtype=float32), 'eval/episode_x_velocity': Array(-2.306536, dtype=float32), 'eval/episode_y_position': Array(1.2335143, dtype=float32), 'eval/episode_y_velocity': Array(4.5551157, dtype=float32), 'eval/avg_episode_length': Array(11., dtype=float32), 'eval/epoch_eval_time': 3.2887866497039795, 'eval/sps': 304.0635062447754}
26000.0 {'eval/walltime': 92.62897729873657, 'training/sps': 56.670361187512825, 'training/walltime': 618.8357417583466, 'training/model_train_time': 9.089518547058105, 'training/other_time': 8.54959774017334, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(26000, dtype=int32), 'model/train_total_loss': Array(-21.499542, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3111017, dtype=float32), 'model/test_total_loss': Array(-18.933973, dtype=float32), 'model/test_mean_loss': Array(1.6311444, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 0.88984614610672, 'sac/actor_loss': Array(2841.1428, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.8340898, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(100859.336, dtype=float32), 'eval/episode_distance_from_origin': Array(6.090193, dtype=float32), 'eval/episode_forward_reward': Array(-2.306536, dtype=float32), 'eval/episode_reward': Array(-25.066967, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-31.416386, dtype=float32), 'eval/episode_reward_forward': Array(-2.306536, dtype=float32), 'eval/episode_reward_survive': Array(10., dtype=float32), 'eval/episode_x_position': Array(-2.0055375, dtype=float32), 'eval/episode_x_velocity': Array(-2.306536, dtype=float32), 'eval/episode_y_position': Array(1.2335143, dtype=float32), 'eval/episode_y_velocity': Array(4.5551157, dtype=float32), 'eval/avg_episode_length': Array(11., dtype=float32), 'eval/epoch_eval_time': 3.2887866497039795, 'eval/sps': 304.0635062447754, 'steps': Array(26000., dtype=float32)}
Model epoch 0: train total loss -19.641311645507812, train mean loss 1.3990041017532349, test mean loss [1.5902785 1.4717702 1.534728  1.5140927 1.4915341 1.5639886 1.4877965]
Model epoch 1: train total loss -20.238811492919922, train mean loss 1.3738234043121338, test mean loss [1.5374199 1.6211742 1.5178553 1.5313426 1.4977282 1.57712   1.5603127]
Model epoch 2: train total loss -20.5200138092041, train mean loss 1.4654291868209839, test mean loss [1.5141954 1.4951067 1.6339382 1.5539657 1.5162153 1.6190428 1.52822  ]
Model epoch 3: train total loss -20.989532470703125, train mean loss 1.3885877132415771, test mean loss [1.5308794 1.5248563 1.5517768 1.5628793 1.5448756 1.5884417 1.5262967]
Model epoch 4: train total loss -21.670902252197266, train mean loss 1.317588448524475, test mean loss [1.5485696 1.5350989 1.5571898 1.5848302 1.5779576 1.6014814 1.5476798]
Model epoch 5: train total loss -21.484878540039062, train mean loss 1.3142684698104858, test mean loss [1.541236  1.5185177 1.5606612 1.5873356 1.5753411 1.6458439 1.5554613]
Model epoch 6: train total loss -21.291269302368164, train mean loss 1.3854327201843262, test mean loss [1.5516536 1.5199873 1.5697322 1.6320835 1.5815133 1.6230586 1.5654235]
Model epoch 7: train total loss -22.0714054107666, train mean loss 1.3191368579864502, test mean loss [1.5780636 1.5525184 1.5822899 1.6252291 1.5707878 1.6299667 1.5896407]
Model epoch 8: train total loss -22.10633659362793, train mean loss 1.3126299381256104, test mean loss [1.5663332 1.5468514 1.5931157 1.644118  1.5834881 1.6317757 1.5835184]
Model trained in 9 epochs with 26000 transitions.
[2025-01-29 16:49:28,546][absl][INFO] - {'eval/walltime': 95.92068719863892, 'training/sps': 54.41193337156595, 'training/walltime': 637.214063167572, 'training/model_train_time': 9.829049110412598, 'training/other_time': 8.542279720306396, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(27000, dtype=int32), 'model/train_total_loss': Array(-22.106337, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3126299, dtype=float32), 'model/test_total_loss': Array(-19.280916, dtype=float32), 'model/test_mean_loss': Array(1.5927428, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 0.8668011824289957, 'sac/actor_loss': Array(2627.1128, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.122439, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(87649.24, dtype=float32), 'eval/episode_distance_from_origin': Array(4.9347367, dtype=float32), 'eval/episode_forward_reward': Array(2.1258903, dtype=float32), 'eval/episode_reward': Array(-11.69914, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-18.155567, dtype=float32), 'eval/episode_reward_forward': Array(2.1258903, dtype=float32), 'eval/episode_reward_survive': Array(6., dtype=float32), 'eval/episode_x_position': Array(0.91334134, dtype=float32), 'eval/episode_x_velocity': Array(2.1258903, dtype=float32), 'eval/episode_y_position': Array(1.1026806, dtype=float32), 'eval/episode_y_velocity': Array(3.8600478, dtype=float32), 'eval/avg_episode_length': Array(7., dtype=float32), 'eval/epoch_eval_time': 3.2917098999023438, 'eval/sps': 303.7934782860626}
27000.0 {'eval/walltime': 95.92068719863892, 'training/sps': 54.41193337156595, 'training/walltime': 637.214063167572, 'training/model_train_time': 9.829049110412598, 'training/other_time': 8.542279720306396, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(27000, dtype=int32), 'model/train_total_loss': Array(-22.106337, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3126299, dtype=float32), 'model/test_total_loss': Array(-19.280916, dtype=float32), 'model/test_mean_loss': Array(1.5927428, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 0.8668011824289957, 'sac/actor_loss': Array(2627.1128, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.122439, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(87649.24, dtype=float32), 'eval/episode_distance_from_origin': Array(4.9347367, dtype=float32), 'eval/episode_forward_reward': Array(2.1258903, dtype=float32), 'eval/episode_reward': Array(-11.69914, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-18.155567, dtype=float32), 'eval/episode_reward_forward': Array(2.1258903, dtype=float32), 'eval/episode_reward_survive': Array(6., dtype=float32), 'eval/episode_x_position': Array(0.91334134, dtype=float32), 'eval/episode_x_velocity': Array(2.1258903, dtype=float32), 'eval/episode_y_position': Array(1.1026806, dtype=float32), 'eval/episode_y_velocity': Array(3.8600478, dtype=float32), 'eval/avg_episode_length': Array(7., dtype=float32), 'eval/epoch_eval_time': 3.2917098999023438, 'eval/sps': 303.7934782860626, 'steps': Array(27000., dtype=float32)}
Model epoch 0: train total loss -19.27422523498535, train mean loss 1.500011920928955, test mean loss [1.3692583 1.3826408 1.6693052 1.6359092 1.5268228 1.570792  1.4154721]
Model epoch 1: train total loss -19.463876724243164, train mean loss 1.5006407499313354, test mean loss [1.3980739 1.3694106 1.5672686 1.601322  1.464201  1.9160507 1.4916335]
Model epoch 2: train total loss -20.383581161499023, train mean loss 1.402967095375061, test mean loss [1.4172026 1.3747374 1.495671  1.6295068 1.5898441 1.7249682 1.4851786]
Model epoch 3: train total loss -21.862443923950195, train mean loss 1.2626526355743408, test mean loss [1.4638557 1.4216421 1.5478716 1.5983905 1.5874892 1.7315739 1.5625035]
Model epoch 4: train total loss -21.247737884521484, train mean loss 1.3961851596832275, test mean loss [1.4413431 1.4027119 1.5071734 1.5469522 1.661866  1.7215704 1.5507301]
Model epoch 5: train total loss -21.7796630859375, train mean loss 1.3267605304718018, test mean loss [1.44461   1.4354274 1.5119805 1.5902534 1.5675867 1.7276784 1.5077952]
Model epoch 6: train total loss -21.889474868774414, train mean loss 1.2847297191619873, test mean loss [1.4667399 1.451131  1.5762242 1.5821036 1.5480094 1.7075214 1.5565326]
Model epoch 7: train total loss -21.755325317382812, train mean loss 1.360950231552124, test mean loss [1.4553541 1.4431202 1.5063114 1.6163319 1.5667062 1.6791    1.5482267]
Model epoch 8: train total loss -21.794401168823242, train mean loss 1.3681358098983765, test mean loss [1.4667072 1.4245507 1.562372  1.6287467 1.5710909 1.6927109 1.5573795]
Model epoch 9: train total loss -21.869028091430664, train mean loss 1.301397681236267, test mean loss [1.4706664 1.440948  1.5975602 1.6834247 1.5753235 1.7285489 1.5480995]
Model epoch 10: train total loss -21.593494415283203, train mean loss 1.3650357723236084, test mean loss [1.5019268 1.4275539 1.5763917 1.5825413 1.6145216 1.7669113 1.5336353]
Model trained in 11 epochs with 27000 transitions.
[2025-01-29 16:49:52,327][absl][INFO] - {'eval/walltime': 99.2080774307251, 'training/sps': 48.81854473945968, 'training/walltime': 657.6980822086334, 'training/model_train_time': 11.926324129104614, 'training/other_time': 8.551082611083984, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(28000, dtype=int32), 'model/train_total_loss': Array(-21.593494, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3650358, dtype=float32), 'model/test_total_loss': Array(36.707485, dtype=float32), 'model/test_mean_loss': Array(1.571926, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 0.8859648704528809, 'sac/actor_loss': Array(2402.994, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.001889, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(74984.64, dtype=float32), 'eval/episode_distance_from_origin': Array(13.451198, dtype=float32), 'eval/episode_forward_reward': Array(0.7228191, dtype=float32), 'eval/episode_reward': Array(-42.117996, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-56.144615, dtype=float32), 'eval/episode_reward_forward': Array(0.7228191, dtype=float32), 'eval/episode_reward_survive': Array(17., dtype=float32), 'eval/episode_x_position': Array(4.1319175, dtype=float32), 'eval/episode_x_velocity': Array(0.7228191, dtype=float32), 'eval/episode_y_position': Array(1.4444292, dtype=float32), 'eval/episode_y_velocity': Array(8.522891, dtype=float32), 'eval/avg_episode_length': Array(18., dtype=float32), 'eval/epoch_eval_time': 3.2873902320861816, 'eval/sps': 304.1926663404967}
28000.0 {'eval/walltime': 99.2080774307251, 'training/sps': 48.81854473945968, 'training/walltime': 657.6980822086334, 'training/model_train_time': 11.926324129104614, 'training/other_time': 8.551082611083984, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(28000, dtype=int32), 'model/train_total_loss': Array(-21.593494, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3650358, dtype=float32), 'model/test_total_loss': Array(36.707485, dtype=float32), 'model/test_mean_loss': Array(1.571926, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 0.8859648704528809, 'sac/actor_loss': Array(2402.994, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-8.001889, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(74984.64, dtype=float32), 'eval/episode_distance_from_origin': Array(13.451198, dtype=float32), 'eval/episode_forward_reward': Array(0.7228191, dtype=float32), 'eval/episode_reward': Array(-42.117996, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-56.144615, dtype=float32), 'eval/episode_reward_forward': Array(0.7228191, dtype=float32), 'eval/episode_reward_survive': Array(17., dtype=float32), 'eval/episode_x_position': Array(4.1319175, dtype=float32), 'eval/episode_x_velocity': Array(0.7228191, dtype=float32), 'eval/episode_y_position': Array(1.4444292, dtype=float32), 'eval/episode_y_velocity': Array(8.522891, dtype=float32), 'eval/avg_episode_length': Array(18., dtype=float32), 'eval/epoch_eval_time': 3.2873902320861816, 'eval/sps': 304.1926663404967, 'steps': Array(28000., dtype=float32)}
Model epoch 0: train total loss -17.8464412689209, train mean loss 1.6940515041351318, test mean loss [1.4931834 1.5711806 1.5507998 1.5930729 1.4920294 2.149698  1.5466613]
Model epoch 1: train total loss -19.91218376159668, train mean loss 1.3937923908233643, test mean loss [1.4676085 1.5186499 1.4917773 1.6177778 1.4821215 1.7623451 1.5099138]
Model epoch 2: train total loss -20.0511417388916, train mean loss 1.4666028022766113, test mean loss [1.541     1.5190573 1.5120018 1.5584804 1.488872  1.7160864 1.5082248]
Model epoch 3: train total loss -21.13858413696289, train mean loss 1.3800015449523926, test mean loss [1.5051295 1.4869913 1.5113124 1.562846  1.5280641 1.670696  1.5304275]
Model epoch 4: train total loss -21.4198055267334, train mean loss 1.3603874444961548, test mean loss [1.4950614 1.5036743 1.4985307 1.7296522 1.4795184 1.6657276 1.5221958]
Model epoch 5: train total loss -22.03343963623047, train mean loss 1.315155029296875, test mean loss [1.5137236 1.5301235 1.5333458 1.5841358 1.5489527 1.6491833 1.5278853]
Model epoch 6: train total loss -21.15668296813965, train mean loss 1.4138990640640259, test mean loss [1.5222477 1.5172698 1.534819  1.5629348 1.5590531 1.6434854 1.5553017]
Model epoch 7: train total loss -21.742502212524414, train mean loss 1.337530255317688, test mean loss [1.5315559 1.5578047 1.5626917 1.5711756 1.5226965 1.6652683 1.5231968]
Model epoch 8: train total loss -22.593477249145508, train mean loss 1.24359929561615, test mean loss [1.6737181 1.5377665 1.5695025 1.5749065 1.5303824 1.6612892 1.550706 ]
Model epoch 9: train total loss -22.44289779663086, train mean loss 1.2675281763076782, test mean loss [1.5500433 1.5691324 1.5515442 1.5992191 1.5161471 1.6608512 1.5485183]
Model epoch 10: train total loss -21.803722381591797, train mean loss 1.4084676504135132, test mean loss [1.5897843 1.6071044 1.5783119 1.6121927 1.5296931 1.674586  1.5697325]
Model epoch 11: train total loss -22.503721237182617, train mean loss 1.258929967880249, test mean loss [1.5737343 1.5827651 1.5711875 1.6055145 1.5526    1.6666181 1.6006685]
Model trained in 12 epochs with 28000 transitions.
[2025-01-29 16:50:17,371][absl][INFO] - {'eval/walltime': 102.5007643699646, 'training/sps': 45.99361184727113, 'training/walltime': 679.4402320384979, 'training/model_train_time': 13.191572427749634, 'training/other_time': 8.543647527694702, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(29000, dtype=int32), 'model/train_total_loss': Array(-22.503721, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.25893, dtype=float32), 'model/test_total_loss': Array(-19.078295, dtype=float32), 'model/test_mean_loss': Array(1.5932984, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 0.9020755688349406, 'sac/actor_loss': Array(2007.5555, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.7621374, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(57824.125, dtype=float32), 'eval/episode_distance_from_origin': Array(4.748786, dtype=float32), 'eval/episode_forward_reward': Array(-5.960439, dtype=float32), 'eval/episode_reward': Array(-31.142262, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-27.982073, dtype=float32), 'eval/episode_reward_forward': Array(-5.960439, dtype=float32), 'eval/episode_reward_survive': Array(8., dtype=float32), 'eval/episode_x_position': Array(-0.29706994, dtype=float32), 'eval/episode_x_velocity': Array(-5.960439, dtype=float32), 'eval/episode_y_position': Array(-0.87535566, dtype=float32), 'eval/episode_y_velocity': Array(-1.3031988, dtype=float32), 'eval/avg_episode_length': Array(9., dtype=float32), 'eval/epoch_eval_time': 3.292686939239502, 'eval/sps': 303.70333361572654}
29000.0 {'eval/walltime': 102.5007643699646, 'training/sps': 45.99361184727113, 'training/walltime': 679.4402320384979, 'training/model_train_time': 13.191572427749634, 'training/other_time': 8.543647527694702, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(29000, dtype=int32), 'model/train_total_loss': Array(-22.503721, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.25893, dtype=float32), 'model/test_total_loss': Array(-19.078295, dtype=float32), 'model/test_mean_loss': Array(1.5932984, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 0.9020755688349406, 'sac/actor_loss': Array(2007.5555, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.7621374, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(57824.125, dtype=float32), 'eval/episode_distance_from_origin': Array(4.748786, dtype=float32), 'eval/episode_forward_reward': Array(-5.960439, dtype=float32), 'eval/episode_reward': Array(-31.142262, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-27.982073, dtype=float32), 'eval/episode_reward_forward': Array(-5.960439, dtype=float32), 'eval/episode_reward_survive': Array(8., dtype=float32), 'eval/episode_x_position': Array(-0.29706994, dtype=float32), 'eval/episode_x_velocity': Array(-5.960439, dtype=float32), 'eval/episode_y_position': Array(-0.87535566, dtype=float32), 'eval/episode_y_velocity': Array(-1.3031988, dtype=float32), 'eval/avg_episode_length': Array(9., dtype=float32), 'eval/epoch_eval_time': 3.292686939239502, 'eval/sps': 303.70333361572654, 'steps': Array(29000., dtype=float32)}
Model epoch 0: train total loss -20.504051208496094, train mean loss 1.4625548124313354, test mean loss [1.4062071 1.4666032 1.4690968 1.4189563 1.4353803 1.5204746 1.4002018]
Model epoch 1: train total loss -20.688302993774414, train mean loss 1.4279123544692993, test mean loss [1.4286146 1.4497442 1.626299  1.4469857 1.4361408 1.489126  1.3949089]
Model epoch 2: train total loss -21.51213264465332, train mean loss 1.3759771585464478, test mean loss [1.4204589 1.4454557 1.4770936 1.5624479 1.4517967 1.5165137 1.4503723]
Model epoch 3: train total loss -21.461307525634766, train mean loss 1.3436890840530396, test mean loss [1.5571425 1.4375712 1.4536124 1.5180485 1.4432436 1.5306073 1.4275576]
Model epoch 4: train total loss -21.95100212097168, train mean loss 1.315666675567627, test mean loss [1.4415443 1.506837  1.4805001 1.50811   1.4471388 1.5383842 1.4227108]
Model epoch 5: train total loss -22.33921241760254, train mean loss 1.2891355752944946, test mean loss [1.452818  1.471296  1.4831785 1.500755  1.4512372 1.5498033 1.4389327]
Model epoch 6: train total loss -22.250957489013672, train mean loss 1.3182073831558228, test mean loss [1.4792192 1.7994033 1.4980397 1.5118841 1.443183  1.5556232 1.4322704]
Model epoch 7: train total loss -21.66946029663086, train mean loss 1.419350266456604, test mean loss [1.4756504 1.5433804 1.497793  1.5165795 1.4717885 1.5468957 1.4393629]
Model epoch 8: train total loss -22.446619033813477, train mean loss 1.3022053241729736, test mean loss [1.4946764 1.518364  1.5279714 1.5281304 1.4892653 1.5452362 1.4786556]
Model epoch 9: train total loss -23.302837371826172, train mean loss 1.2496474981307983, test mean loss [1.4801978 1.504316  1.5092052 1.5416777 1.4962313 1.5660731 1.4595876]
Model trained in 10 epochs with 29000 transitions.
[2025-01-29 16:50:40,844][absl][INFO] - {'eval/walltime': 105.7812864780426, 'training/sps': 49.54860640055441, 'training/walltime': 699.6224343776703, 'training/model_train_time': 11.626214981079102, 'training/other_time': 8.549216032028198, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(30000, dtype=int32), 'model/train_total_loss': Array(-23.302837, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2496475, dtype=float32), 'model/test_total_loss': Array(-19.984636, dtype=float32), 'model/test_mean_loss': Array(1.5081842, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 0.9440724372863769, 'sac/actor_loss': Array(1850.2439, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.463292, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(50326.684, dtype=float32), 'eval/episode_distance_from_origin': Array(6.9868507, dtype=float32), 'eval/episode_forward_reward': Array(-6.3059416, dtype=float32), 'eval/episode_reward': Array(-35.64679, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-36.466545, dtype=float32), 'eval/episode_reward_forward': Array(-6.3059416, dtype=float32), 'eval/episode_reward_survive': Array(11., dtype=float32), 'eval/episode_x_position': Array(0.8484758, dtype=float32), 'eval/episode_x_velocity': Array(-6.3059416, dtype=float32), 'eval/episode_y_position': Array(-1.4478844, dtype=float32), 'eval/episode_y_velocity': Array(-2.305924, dtype=float32), 'eval/avg_episode_length': Array(12., dtype=float32), 'eval/epoch_eval_time': 3.280522108078003, 'eval/sps': 304.82952623229886}
30000.0 {'eval/walltime': 105.7812864780426, 'training/sps': 49.54860640055441, 'training/walltime': 699.6224343776703, 'training/model_train_time': 11.626214981079102, 'training/other_time': 8.549216032028198, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(30000, dtype=int32), 'model/train_total_loss': Array(-23.302837, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2496475, dtype=float32), 'model/test_total_loss': Array(-19.984636, dtype=float32), 'model/test_mean_loss': Array(1.5081842, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 0.9440724372863769, 'sac/actor_loss': Array(1850.2439, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.463292, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(50326.684, dtype=float32), 'eval/episode_distance_from_origin': Array(6.9868507, dtype=float32), 'eval/episode_forward_reward': Array(-6.3059416, dtype=float32), 'eval/episode_reward': Array(-35.64679, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-36.466545, dtype=float32), 'eval/episode_reward_forward': Array(-6.3059416, dtype=float32), 'eval/episode_reward_survive': Array(11., dtype=float32), 'eval/episode_x_position': Array(0.8484758, dtype=float32), 'eval/episode_x_velocity': Array(-6.3059416, dtype=float32), 'eval/episode_y_position': Array(-1.4478844, dtype=float32), 'eval/episode_y_velocity': Array(-2.305924, dtype=float32), 'eval/avg_episode_length': Array(12., dtype=float32), 'eval/epoch_eval_time': 3.280522108078003, 'eval/sps': 304.82952623229886, 'steps': Array(30000., dtype=float32)}
Model epoch 0: train total loss -19.217321395874023, train mean loss 1.5268479585647583, test mean loss [1.5438807 1.9795364 1.910059  1.4512019 1.4004596 1.5221705 1.4146146]
Model epoch 1: train total loss -21.436342239379883, train mean loss 1.3614486455917358, test mean loss [1.4629147 1.55243   1.563061  1.4802735 1.4286789 1.5340099 1.4171975]
Model epoch 2: train total loss -21.332624435424805, train mean loss 1.3621947765350342, test mean loss [1.4245601 1.4839659 1.5281293 1.4699059 1.4685469 1.5377243 1.4213765]
Model epoch 3: train total loss -21.149131774902344, train mean loss 1.5052905082702637, test mean loss [1.4257088 1.4555176 1.5553663 1.5058742 1.4795178 1.5488933 1.4315599]
Model epoch 4: train total loss -22.52256202697754, train mean loss 1.2923810482025146, test mean loss [1.4238839 1.4744682 1.5327988 1.5175416 1.452477  1.562836  1.4650671]
Model epoch 5: train total loss -22.512144088745117, train mean loss 1.3129926919937134, test mean loss [1.4471372 1.4982598 1.5215453 1.5056381 1.4514649 1.5671929 1.4550531]
Model epoch 6: train total loss -22.608814239501953, train mean loss 1.2086906433105469, test mean loss [1.4870008 1.4784812 1.5196769 1.515758  1.4726155 1.5712316 1.4753308]
Model epoch 7: train total loss -21.999250411987305, train mean loss 1.3272042274475098, test mean loss [1.4530119 1.4868945 1.5284407 1.5341878 1.507864  1.5908394 1.4926   ]
Model epoch 8: train total loss -22.274505615234375, train mean loss 1.3938714265823364, test mean loss [1.4639163 1.4836735 1.5354408 1.5443554 1.5043883 1.5838325 1.4920185]
Model epoch 9: train total loss -22.578413009643555, train mean loss 1.2941070795059204, test mean loss [1.4876628 1.4993868 1.5481377 1.5663484 1.5118208 1.6015413 1.4879211]
Model trained in 10 epochs with 30000 transitions.
[2025-01-29 16:51:04,824][absl][INFO] - {'eval/walltime': 109.07654523849487, 'training/sps': 48.36929226923529, 'training/walltime': 720.2967083454132, 'training/model_train_time': 12.10099458694458, 'training/other_time': 8.566024780273438, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(31000, dtype=int32), 'model/train_total_loss': Array(-22.578413, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2941071, dtype=float32), 'model/test_total_loss': Array(-20.087101, dtype=float32), 'model/test_mean_loss': Array(1.5289742, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 0.9824001550674438, 'sac/actor_loss': Array(1736.3944, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.114242, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(44237.51, dtype=float32), 'eval/episode_distance_from_origin': Array(9.843204, dtype=float32), 'eval/episode_forward_reward': Array(-12.8761635, dtype=float32), 'eval/episode_reward': Array(-47.83819, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-44.953777, dtype=float32), 'eval/episode_reward_forward': Array(-12.8761635, dtype=float32), 'eval/episode_reward_survive': Array(14., dtype=float32), 'eval/episode_x_position': Array(-1.2771323, dtype=float32), 'eval/episode_x_velocity': Array(-12.8761635, dtype=float32), 'eval/episode_y_position': Array(-1.2853146, dtype=float32), 'eval/episode_y_velocity': Array(-11.508696, dtype=float32), 'eval/avg_episode_length': Array(15., dtype=float32), 'eval/epoch_eval_time': 3.2952587604522705, 'eval/sps': 303.4663049838159}
31000.0 {'eval/walltime': 109.07654523849487, 'training/sps': 48.36929226923529, 'training/walltime': 720.2967083454132, 'training/model_train_time': 12.10099458694458, 'training/other_time': 8.566024780273438, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(31000, dtype=int32), 'model/train_total_loss': Array(-22.578413, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2941071, dtype=float32), 'model/test_total_loss': Array(-20.087101, dtype=float32), 'model/test_mean_loss': Array(1.5289742, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 0.9824001550674438, 'sac/actor_loss': Array(1736.3944, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-7.114242, dtype=float32), 'sac/buffer_current_size': Array(201198.22, dtype=float32), 'sac/critic_loss': Array(44237.51, dtype=float32), 'eval/episode_distance_from_origin': Array(9.843204, dtype=float32), 'eval/episode_forward_reward': Array(-12.8761635, dtype=float32), 'eval/episode_reward': Array(-47.83819, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-44.953777, dtype=float32), 'eval/episode_reward_forward': Array(-12.8761635, dtype=float32), 'eval/episode_reward_survive': Array(14., dtype=float32), 'eval/episode_x_position': Array(-1.2771323, dtype=float32), 'eval/episode_x_velocity': Array(-12.8761635, dtype=float32), 'eval/episode_y_position': Array(-1.2853146, dtype=float32), 'eval/episode_y_velocity': Array(-11.508696, dtype=float32), 'eval/avg_episode_length': Array(15., dtype=float32), 'eval/epoch_eval_time': 3.2952587604522705, 'eval/sps': 303.4663049838159, 'steps': Array(31000., dtype=float32)}
Model horizon updated to 2.
Hallucination updates per training step updated to 1000.
SAC buffer resized to 800000 samples.
Model epoch 0: train total loss -21.750072479248047, train mean loss 1.2601333856582642, test mean loss [1.3006651 1.339052  1.4154637 1.383161  1.3659029 1.4865844 1.2929351]
Model epoch 1: train total loss -21.625244140625, train mean loss 1.3364835977554321, test mean loss [1.3264096 1.3388714 1.3959577 1.4052972 1.4448383 1.4353943 1.3542864]
Model epoch 2: train total loss -21.165231704711914, train mean loss 1.4129434823989868, test mean loss [1.3558099 1.3695906 1.3833356 1.6989505 1.3800704 1.468143  1.3689855]
Model epoch 3: train total loss -22.145824432373047, train mean loss 1.297698736190796, test mean loss [1.3583577 1.3742855 1.389913  1.5396069 1.389313  1.4554191 1.3769538]
Model epoch 4: train total loss -21.909774780273438, train mean loss 1.3569198846817017, test mean loss [1.3914325 1.3820947 1.4090438 1.6323168 1.38696   1.4813638 1.3533577]
Model epoch 5: train total loss -22.29637336730957, train mean loss 1.27889883518219, test mean loss [1.3691471 1.3891509 1.4271401 1.4938848 1.4280599 1.4821889 1.3831657]
Model epoch 6: train total loss -22.937204360961914, train mean loss 1.2143580913543701, test mean loss [1.368993  1.4221714 1.4353714 1.4826546 1.4177411 1.5095843 1.3787334]
Model epoch 7: train total loss -22.913564682006836, train mean loss 1.2410417795181274, test mean loss [1.3792119 1.4075062 1.4325829 1.473423  1.4100256 1.4968504 1.4348285]
Model trained in 8 epochs with 31000 transitions.
[2025-01-29 16:51:46,081][absl][INFO] - {'eval/walltime': 112.35948276519775, 'training/sps': 26.362495602955136, 'training/walltime': 758.2293841838837, 'training/model_train_time': 10.105802536010742, 'training/other_time': 27.819827795028687, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(32000, dtype=int32), 'model/train_total_loss': Array(-22.913565, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2410418, dtype=float32), 'model/test_total_loss': Array(-20.773472, dtype=float32), 'model/test_mean_loss': Array(1.4334899, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 1.0089214444160461, 'sac/actor_loss': Array(987.22296, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-5.943366, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(25476.523, dtype=float32), 'eval/episode_distance_from_origin': Array(24.439405, dtype=float32), 'eval/episode_forward_reward': Array(-8.772045, dtype=float32), 'eval/episode_reward': Array(-62.876514, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-79.09241, dtype=float32), 'eval/episode_reward_forward': Array(-8.772045, dtype=float32), 'eval/episode_reward_survive': Array(27., dtype=float32), 'eval/episode_x_position': Array(-9.780548, dtype=float32), 'eval/episode_x_velocity': Array(-8.772045, dtype=float32), 'eval/episode_y_position': Array(-1.416141, dtype=float32), 'eval/episode_y_velocity': Array(-21.551449, dtype=float32), 'eval/avg_episode_length': Array(28., dtype=float32), 'eval/epoch_eval_time': 3.282937526702881, 'eval/sps': 304.60524815539816}
32000.0 {'eval/walltime': 112.35948276519775, 'training/sps': 26.362495602955136, 'training/walltime': 758.2293841838837, 'training/model_train_time': 10.105802536010742, 'training/other_time': 27.819827795028687, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(32000, dtype=int32), 'model/train_total_loss': Array(-22.913565, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2410418, dtype=float32), 'model/test_total_loss': Array(-20.773472, dtype=float32), 'model/test_mean_loss': Array(1.4334899, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 1.0089214444160461, 'sac/actor_loss': Array(987.22296, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-5.943366, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(25476.523, dtype=float32), 'eval/episode_distance_from_origin': Array(24.439405, dtype=float32), 'eval/episode_forward_reward': Array(-8.772045, dtype=float32), 'eval/episode_reward': Array(-62.876514, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-79.09241, dtype=float32), 'eval/episode_reward_forward': Array(-8.772045, dtype=float32), 'eval/episode_reward_survive': Array(27., dtype=float32), 'eval/episode_x_position': Array(-9.780548, dtype=float32), 'eval/episode_x_velocity': Array(-8.772045, dtype=float32), 'eval/episode_y_position': Array(-1.416141, dtype=float32), 'eval/episode_y_velocity': Array(-21.551449, dtype=float32), 'eval/avg_episode_length': Array(28., dtype=float32), 'eval/epoch_eval_time': 3.282937526702881, 'eval/sps': 304.60524815539816, 'steps': Array(32000., dtype=float32)}
Model epoch 0: train total loss -21.022619247436523, train mean loss 1.3052319288253784, test mean loss [1.3875624 1.4271399 1.5081222 1.482068  1.4257197 1.5068142 1.3654945]
Model epoch 1: train total loss -21.642648696899414, train mean loss 1.335965871810913, test mean loss [1.417674  1.4336041 1.4445918 1.4876943 1.4427013 1.5348878 1.4153993]
Model epoch 2: train total loss -21.564640045166016, train mean loss 1.367155909538269, test mean loss [1.403054  1.4976104 1.4447087 1.4897898 1.4553094 1.5414503 1.4209385]
Model epoch 3: train total loss -22.633567810058594, train mean loss 1.313183307647705, test mean loss [1.4471111 1.457372  1.4442892 1.5262849 1.5202776 1.6207373 1.428641 ]
Model epoch 4: train total loss -22.390897750854492, train mean loss 1.3755812644958496, test mean loss [1.4560542 1.4583627 1.4758552 1.5215069 1.4563764 1.5578079 1.4491916]
Model epoch 5: train total loss -22.699033737182617, train mean loss 1.2887201309204102, test mean loss [1.4644467 1.4642444 1.4993066 1.5544059 1.4632902 1.5696603 1.4609251]
Model epoch 6: train total loss -22.885334014892578, train mean loss 1.2215228080749512, test mean loss [1.4976279 1.4832011 1.4904318 1.5344725 1.4806182 1.5767212 1.4776654]
Model epoch 7: train total loss -23.02836036682129, train mean loss 1.3085886240005493, test mean loss [1.5039941 1.4846404 1.5060017 1.5500714 1.4953794 1.5987903 1.5247684]
Model trained in 8 epochs with 32000 transitions.
[2025-01-29 16:52:10,128][absl][INFO] - {'eval/walltime': 115.6365339756012, 'training/sps': 48.16893230286393, 'training/walltime': 778.9896533489227, 'training/model_train_time': 11.480134725570679, 'training/other_time': 9.271507740020752, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(33000, dtype=int32), 'model/train_total_loss': Array(-23.02836, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3085886, dtype=float32), 'model/test_total_loss': Array(-20.64791, dtype=float32), 'model/test_mean_loss': Array(1.5233781, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 1.1195329129695892, 'sac/actor_loss': Array(412.89325, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-3.4148543, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(4792.943, dtype=float32), 'eval/episode_distance_from_origin': Array(55.657616, dtype=float32), 'eval/episode_forward_reward': Array(-21.143848, dtype=float32), 'eval/episode_reward': Array(-102.32939, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-123.693436, dtype=float32), 'eval/episode_reward_forward': Array(-21.143848, dtype=float32), 'eval/episode_reward_survive': Array(45., dtype=float32), 'eval/episode_x_position': Array(-39.37164, dtype=float32), 'eval/episode_x_velocity': Array(-21.143848, dtype=float32), 'eval/episode_y_position': Array(-3.3934188, dtype=float32), 'eval/episode_y_velocity': Array(3.419982, dtype=float32), 'eval/avg_episode_length': Array(46., dtype=float32), 'eval/epoch_eval_time': 3.2770512104034424, 'eval/sps': 305.15238725149146}
33000.0 {'eval/walltime': 115.6365339756012, 'training/sps': 48.16893230286393, 'training/walltime': 778.9896533489227, 'training/model_train_time': 11.480134725570679, 'training/other_time': 9.271507740020752, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(33000, dtype=int32), 'model/train_total_loss': Array(-23.02836, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3085886, dtype=float32), 'model/test_total_loss': Array(-20.64791, dtype=float32), 'model/test_mean_loss': Array(1.5233781, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 1.1195329129695892, 'sac/actor_loss': Array(412.89325, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-3.4148543, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(4792.943, dtype=float32), 'eval/episode_distance_from_origin': Array(55.657616, dtype=float32), 'eval/episode_forward_reward': Array(-21.143848, dtype=float32), 'eval/episode_reward': Array(-102.32939, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-123.693436, dtype=float32), 'eval/episode_reward_forward': Array(-21.143848, dtype=float32), 'eval/episode_reward_survive': Array(45., dtype=float32), 'eval/episode_x_position': Array(-39.37164, dtype=float32), 'eval/episode_x_velocity': Array(-21.143848, dtype=float32), 'eval/episode_y_position': Array(-3.3934188, dtype=float32), 'eval/episode_y_velocity': Array(3.419982, dtype=float32), 'eval/avg_episode_length': Array(46., dtype=float32), 'eval/epoch_eval_time': 3.2770512104034424, 'eval/sps': 305.15238725149146, 'steps': Array(33000., dtype=float32)}
Model epoch 0: train total loss -21.684850692749023, train mean loss 1.3660390377044678, test mean loss [1.3810996 1.5202675 1.4425286 1.4562235 1.398591  1.5579481 1.3806431]
Model epoch 1: train total loss -22.407136917114258, train mean loss 1.22231924533844, test mean loss [1.4720522 1.4666687 1.4525297 1.4784405 1.4179734 1.5525733 1.4258288]
Model epoch 2: train total loss -22.002498626708984, train mean loss 1.3201669454574585, test mean loss [1.4144696 1.488522  1.4698279 1.4807571 1.4403927 1.559092  1.4197552]
Model epoch 3: train total loss -23.212766647338867, train mean loss 1.2508121728897095, test mean loss [1.4162333 1.4720967 1.5366876 1.4997982 1.4172807 1.5455742 1.4360147]
Model epoch 4: train total loss -23.329328536987305, train mean loss 1.1550520658493042, test mean loss [1.4479793 1.5684863 1.486626  1.5214213 1.4443802 1.5674952 1.4421729]
Model epoch 5: train total loss -22.87614631652832, train mean loss 1.266730785369873, test mean loss [1.4775052 1.5153809 1.5079932 1.5212522 1.4737196 1.5868475 1.448661 ]
Model epoch 6: train total loss -23.122089385986328, train mean loss 1.162975549697876, test mean loss [1.4522234 1.5043471 1.5252322 1.5277967 1.4792178 1.6543769 1.4683313]
Model epoch 7: train total loss -23.188213348388672, train mean loss 1.2117289304733276, test mean loss [1.4504464 1.5098149 1.5189217 1.5550921 1.4852219 1.690326  1.4921026]
Model trained in 8 epochs with 33000 transitions.
[2025-01-29 16:52:33,681][absl][INFO] - {'eval/walltime': 118.9242012500763, 'training/sps': 49.37080853058675, 'training/walltime': 799.2445373535156, 'training/model_train_time': 10.972108602523804, 'training/other_time': 9.274041414260864, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(34000, dtype=int32), 'model/train_total_loss': Array(-23.188213, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2117289, dtype=float32), 'model/test_total_loss': Array(-20.491297, dtype=float32), 'model/test_mean_loss': Array(1.5288465, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 1.0812232196331024, 'sac/actor_loss': Array(206.56548, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-1.7803253, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(1156.3416, dtype=float32), 'eval/episode_distance_from_origin': Array(4.068637, dtype=float32), 'eval/episode_forward_reward': Array(-5.8872156, dtype=float32), 'eval/episode_reward': Array(-21.2275, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-18.131626, dtype=float32), 'eval/episode_reward_forward': Array(-5.8872156, dtype=float32), 'eval/episode_reward_survive': Array(6., dtype=float32), 'eval/episode_x_position': Array(-0.13825037, dtype=float32), 'eval/episode_x_velocity': Array(-5.8872156, dtype=float32), 'eval/episode_y_position': Array(0.260553, dtype=float32), 'eval/episode_y_velocity': Array(-0.52257764, dtype=float32), 'eval/avg_episode_length': Array(7., dtype=float32), 'eval/epoch_eval_time': 3.2876672744750977, 'eval/sps': 304.1670328879792}
34000.0 {'eval/walltime': 118.9242012500763, 'training/sps': 49.37080853058675, 'training/walltime': 799.2445373535156, 'training/model_train_time': 10.972108602523804, 'training/other_time': 9.274041414260864, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(34000, dtype=int32), 'model/train_total_loss': Array(-23.188213, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2117289, dtype=float32), 'model/test_total_loss': Array(-20.491297, dtype=float32), 'model/test_mean_loss': Array(1.5288465, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 1.0812232196331024, 'sac/actor_loss': Array(206.56548, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-1.7803253, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(1156.3416, dtype=float32), 'eval/episode_distance_from_origin': Array(4.068637, dtype=float32), 'eval/episode_forward_reward': Array(-5.8872156, dtype=float32), 'eval/episode_reward': Array(-21.2275, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-18.131626, dtype=float32), 'eval/episode_reward_forward': Array(-5.8872156, dtype=float32), 'eval/episode_reward_survive': Array(6., dtype=float32), 'eval/episode_x_position': Array(-0.13825037, dtype=float32), 'eval/episode_x_velocity': Array(-5.8872156, dtype=float32), 'eval/episode_y_position': Array(0.260553, dtype=float32), 'eval/episode_y_velocity': Array(-0.52257764, dtype=float32), 'eval/avg_episode_length': Array(7., dtype=float32), 'eval/epoch_eval_time': 3.2876672744750977, 'eval/sps': 304.1670328879792, 'steps': Array(34000., dtype=float32)}
Model epoch 0: train total loss -21.299360275268555, train mean loss 1.3384402990341187, test mean loss [1.422331  1.5667806 1.5124013 1.4620003 1.4381337 1.5435194 1.4092401]
Model epoch 1: train total loss -21.679183959960938, train mean loss 1.415575623512268, test mean loss [1.4215001 1.4862744 1.5235974 1.4661247 1.4306738 1.7192777 1.4005536]
Model epoch 2: train total loss -21.74419403076172, train mean loss 1.4046818017959595, test mean loss [1.4188386 1.4580355 1.5128891 1.4670931 1.4622514 1.5332166 1.4183661]
Model epoch 3: train total loss -22.350473403930664, train mean loss 1.3035236597061157, test mean loss [1.4652418 1.4664353 1.503455  1.4899182 1.6152238 1.5441647 1.4079994]
Model epoch 4: train total loss -22.28185272216797, train mean loss 1.3114675283432007, test mean loss [1.4667662 1.4723501 1.5133069 1.5142295 1.5189126 1.5353962 1.4840482]
Model epoch 5: train total loss -23.494895935058594, train mean loss 1.1703628301620483, test mean loss [1.4557792 1.4856119 1.509826  1.5174531 1.4758147 1.5459378 1.4579538]
Model epoch 6: train total loss -22.58599281311035, train mean loss 1.3119056224822998, test mean loss [1.4902134 1.4816285 1.5534716 1.5325222 1.4926331 1.5749887 1.505268 ]
Model epoch 7: train total loss -23.38217544555664, train mean loss 1.181173324584961, test mean loss [1.4722451 1.5370568 1.5435758 1.5471181 1.5266316 1.6185197 1.4905717]
Model epoch 8: train total loss -21.74363899230957, train mean loss 1.4712308645248413, test mean loss [1.5535667 1.5115056 1.521932  1.5461488 1.490851  1.5840447 1.4921037]
Model trained in 9 epochs with 34000 transitions.
[2025-01-29 16:52:58,463][absl][INFO] - {'eval/walltime': 122.20947170257568, 'training/sps': 46.54004453384131, 'training/walltime': 820.7314097881317, 'training/model_train_time': 12.209853649139404, 'training/other_time': 9.267935276031494, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(35000, dtype=int32), 'model/train_total_loss': Array(-21.743639, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4712309, dtype=float32), 'model/test_total_loss': Array(-20.526234, dtype=float32), 'model/test_mean_loss': Array(1.5285933, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.0972852971818712, 'sac/actor_loss': Array(128.97401, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-0.8685244, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(427.33054, dtype=float32), 'eval/episode_distance_from_origin': Array(8.717533, dtype=float32), 'eval/episode_forward_reward': Array(-4.371718, dtype=float32), 'eval/episode_reward': Array(-26.990412, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-30.546906, dtype=float32), 'eval/episode_reward_forward': Array(-4.371718, dtype=float32), 'eval/episode_reward_survive': Array(12., dtype=float32), 'eval/episode_x_position': Array(-1.0266154, dtype=float32), 'eval/episode_x_velocity': Array(-4.371718, dtype=float32), 'eval/episode_y_position': Array(-4.246406, dtype=float32), 'eval/episode_y_velocity': Array(-12.082933, dtype=float32), 'eval/avg_episode_length': Array(13., dtype=float32), 'eval/epoch_eval_time': 3.2852704524993896, 'eval/sps': 304.3889428461555}
35000.0 {'eval/walltime': 122.20947170257568, 'training/sps': 46.54004453384131, 'training/walltime': 820.7314097881317, 'training/model_train_time': 12.209853649139404, 'training/other_time': 9.267935276031494, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(35000, dtype=int32), 'model/train_total_loss': Array(-21.743639, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.4712309, dtype=float32), 'model/test_total_loss': Array(-20.526234, dtype=float32), 'model/test_mean_loss': Array(1.5285933, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.0972852971818712, 'sac/actor_loss': Array(128.97401, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-0.8685244, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(427.33054, dtype=float32), 'eval/episode_distance_from_origin': Array(8.717533, dtype=float32), 'eval/episode_forward_reward': Array(-4.371718, dtype=float32), 'eval/episode_reward': Array(-26.990412, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-30.546906, dtype=float32), 'eval/episode_reward_forward': Array(-4.371718, dtype=float32), 'eval/episode_reward_survive': Array(12., dtype=float32), 'eval/episode_x_position': Array(-1.0266154, dtype=float32), 'eval/episode_x_velocity': Array(-4.371718, dtype=float32), 'eval/episode_y_position': Array(-4.246406, dtype=float32), 'eval/episode_y_velocity': Array(-12.082933, dtype=float32), 'eval/avg_episode_length': Array(13., dtype=float32), 'eval/epoch_eval_time': 3.2852704524993896, 'eval/sps': 304.3889428461555, 'steps': Array(35000., dtype=float32)}
Model epoch 0: train total loss -20.866376876831055, train mean loss 1.4572007656097412, test mean loss [1.8180057 1.4072421 1.5143064 1.6818768 1.3363477 1.5547372 1.3728576]
Model epoch 1: train total loss -19.529314041137695, train mean loss 1.545291543006897, test mean loss [1.6801584 1.8611917 1.4425226 1.6042664 1.3668939 1.622038  1.396208 ]
Model epoch 2: train total loss -21.95124626159668, train mean loss 1.3519068956375122, test mean loss [1.608499  1.6426216 1.5006791 1.6091081 1.3359708 1.5283868 1.29702  ]
Model epoch 3: train total loss -22.496971130371094, train mean loss 1.25807523727417, test mean loss [1.6059544 1.6209158 1.4760435 1.5721062 1.3455641 1.5269588 1.3181349]
Model epoch 4: train total loss -23.044166564941406, train mean loss 1.2265541553497314, test mean loss [1.5830083 1.5818933 1.4896033 1.594566  1.3349593 1.5362868 1.3063539]
Model epoch 5: train total loss -22.26532745361328, train mean loss 1.2871965169906616, test mean loss [1.5849919 1.6010067 1.5525848 1.5427248 1.3532885 1.540213  1.2965395]
Model epoch 6: train total loss -22.580602645874023, train mean loss 1.3005201816558838, test mean loss [1.6135682 1.5268904 1.536553  1.5656006 1.3920032 1.5494698 1.31788  ]
Model epoch 7: train total loss -22.57341194152832, train mean loss 1.2346420288085938, test mean loss [1.5971776 1.5816772 1.5110116 1.5699515 1.4106046 1.5780443 1.3558099]
Model epoch 8: train total loss -23.27202606201172, train mean loss 1.2266343832015991, test mean loss [1.6125373 1.4986838 1.5572518 1.5729392 1.387487  1.5566531 1.416798 ]
Model epoch 9: train total loss -22.82511329650879, train mean loss 1.2003380060195923, test mean loss [1.65406   1.4895707 1.514407  1.5775647 1.4203973 1.5902753 1.365617 ]
Model epoch 10: train total loss -23.43522834777832, train mean loss 1.2512325048446655, test mean loss [1.6647912 1.542267  1.4832889 1.5682695 1.4237622 1.6036304 1.3679225]
Model epoch 11: train total loss -23.226314544677734, train mean loss 1.264722228050232, test mean loss [1.6032891 1.529096  1.5082008 1.596268  1.4086765 1.6312814 1.424126 ]
Model trained in 12 epochs with 35000 transitions.
[2025-01-29 16:53:25,733][absl][INFO] - {'eval/walltime': 125.49307918548584, 'training/sps': 41.709067563324744, 'training/walltime': 844.7070116996765, 'training/model_train_time': 14.694733619689941, 'training/other_time': 9.272357940673828, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(36000, dtype=int32), 'model/train_total_loss': Array(-23.226315, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2647222, dtype=float32), 'model/test_total_loss': Array(-20.413736, dtype=float32), 'model/test_mean_loss': Array(1.5287054, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.0503642161687214, 'sac/actor_loss': Array(101.02241, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-0.42812172, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(243.39879, dtype=float32), 'eval/episode_distance_from_origin': Array(15.732128, dtype=float32), 'eval/episode_forward_reward': Array(-22.359682, dtype=float32), 'eval/episode_reward': Array(-52.218945, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-41.46091, dtype=float32), 'eval/episode_reward_forward': Array(-22.359682, dtype=float32), 'eval/episode_reward_survive': Array(16., dtype=float32), 'eval/episode_x_position': Array(-6.71538, dtype=float32), 'eval/episode_x_velocity': Array(-22.359682, dtype=float32), 'eval/episode_y_position': Array(5.807416, dtype=float32), 'eval/episode_y_velocity': Array(8.269146, dtype=float32), 'eval/avg_episode_length': Array(17., dtype=float32), 'eval/epoch_eval_time': 3.2836074829101562, 'eval/sps': 304.5430993821868}
36000.0 {'eval/walltime': 125.49307918548584, 'training/sps': 41.709067563324744, 'training/walltime': 844.7070116996765, 'training/model_train_time': 14.694733619689941, 'training/other_time': 9.272357940673828, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(36000, dtype=int32), 'model/train_total_loss': Array(-23.226315, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2647222, dtype=float32), 'model/test_total_loss': Array(-20.413736, dtype=float32), 'model/test_mean_loss': Array(1.5287054, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.0503642161687214, 'sac/actor_loss': Array(101.02241, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(-0.42812172, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(243.39879, dtype=float32), 'eval/episode_distance_from_origin': Array(15.732128, dtype=float32), 'eval/episode_forward_reward': Array(-22.359682, dtype=float32), 'eval/episode_reward': Array(-52.218945, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-41.46091, dtype=float32), 'eval/episode_reward_forward': Array(-22.359682, dtype=float32), 'eval/episode_reward_survive': Array(16., dtype=float32), 'eval/episode_x_position': Array(-6.71538, dtype=float32), 'eval/episode_x_velocity': Array(-22.359682, dtype=float32), 'eval/episode_y_position': Array(5.807416, dtype=float32), 'eval/episode_y_velocity': Array(8.269146, dtype=float32), 'eval/avg_episode_length': Array(17., dtype=float32), 'eval/epoch_eval_time': 3.2836074829101562, 'eval/sps': 304.5430993821868, 'steps': Array(36000., dtype=float32)}
Model epoch 0: train total loss -20.932222366333008, train mean loss 1.4043385982513428, test mean loss [1.386273  1.2875838 1.240511  1.6546122 1.654075  1.2621435 1.6296144]
Model epoch 1: train total loss -21.230506896972656, train mean loss 1.4853167533874512, test mean loss [1.327129  1.2979023 1.501531  1.5162812 1.450617  1.2884641 1.4653755]
Model epoch 2: train total loss -22.5247859954834, train mean loss 1.2520244121551514, test mean loss [1.3758336 1.3297393 1.3953807 1.5174146 1.4196026 1.3038447 1.4370581]
Model epoch 3: train total loss -22.588550567626953, train mean loss 1.2522369623184204, test mean loss [1.3734032 1.3140821 1.3690592 1.7610996 1.4195358 1.297031  1.4998628]
Model epoch 4: train total loss -22.7552433013916, train mean loss 1.239599585533142, test mean loss [1.3804613 1.3110291 1.373067  1.5175501 1.4121385 1.4280378 1.5101924]
Model epoch 5: train total loss -22.987560272216797, train mean loss 1.2691171169281006, test mean loss [1.3761734 1.3261061 1.3567195 1.5221038 1.4101932 1.3218842 1.4629456]
Model epoch 6: train total loss -23.399486541748047, train mean loss 1.1657050848007202, test mean loss [1.418247  1.3477724 1.3335626 1.5354785 1.4313956 1.334689  1.499226 ]
Model epoch 7: train total loss -23.51761245727539, train mean loss 1.2126957178115845, test mean loss [1.4034722 1.3499438 1.350042  1.5215129 1.4081177 1.3466061 1.4918725]
Model epoch 8: train total loss -23.42976951599121, train mean loss 1.2557353973388672, test mean loss [1.3769838 1.3352484 1.3749696 1.523364  1.412141  1.3641738 1.8145766]
Model trained in 9 epochs with 36000 transitions.
[2025-01-29 16:53:50,284][absl][INFO] - {'eval/walltime': 128.77940154075623, 'training/sps': 47.04732416986532, 'training/walltime': 865.9622056484222, 'training/model_train_time': 11.961548089981079, 'training/other_time': 9.285221815109253, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(37000, dtype=int32), 'model/train_total_loss': Array(-23.42977, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2557354, dtype=float32), 'model/test_total_loss': Array(-21.284101, dtype=float32), 'model/test_mean_loss': Array(1.457351, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.0845377975040011, 'sac/actor_loss': Array(72.45024, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.09428748, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(115.09597, dtype=float32), 'eval/episode_distance_from_origin': Array(11.771346, dtype=float32), 'eval/episode_forward_reward': Array(-2.2318988, dtype=float32), 'eval/episode_reward': Array(-20.618452, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-29.674095, dtype=float32), 'eval/episode_reward_forward': Array(-2.2318988, dtype=float32), 'eval/episode_reward_survive': Array(15., dtype=float32), 'eval/episode_x_position': Array(1.129786, dtype=float32), 'eval/episode_x_velocity': Array(-2.2318988, dtype=float32), 'eval/episode_y_position': Array(-2.7984786, dtype=float32), 'eval/episode_y_velocity': Array(-0.46756727, dtype=float32), 'eval/avg_episode_length': Array(16., dtype=float32), 'eval/epoch_eval_time': 3.2863223552703857, 'eval/sps': 304.2915124854585}
37000.0 {'eval/walltime': 128.77940154075623, 'training/sps': 47.04732416986532, 'training/walltime': 865.9622056484222, 'training/model_train_time': 11.961548089981079, 'training/other_time': 9.285221815109253, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(37000, dtype=int32), 'model/train_total_loss': Array(-23.42977, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.2557354, dtype=float32), 'model/test_total_loss': Array(-21.284101, dtype=float32), 'model/test_mean_loss': Array(1.457351, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.0845377975040011, 'sac/actor_loss': Array(72.45024, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.09428748, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(115.09597, dtype=float32), 'eval/episode_distance_from_origin': Array(11.771346, dtype=float32), 'eval/episode_forward_reward': Array(-2.2318988, dtype=float32), 'eval/episode_reward': Array(-20.618452, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-29.674095, dtype=float32), 'eval/episode_reward_forward': Array(-2.2318988, dtype=float32), 'eval/episode_reward_survive': Array(15., dtype=float32), 'eval/episode_x_position': Array(1.129786, dtype=float32), 'eval/episode_x_velocity': Array(-2.2318988, dtype=float32), 'eval/episode_y_position': Array(-2.7984786, dtype=float32), 'eval/episode_y_velocity': Array(-0.46756727, dtype=float32), 'eval/avg_episode_length': Array(16., dtype=float32), 'eval/epoch_eval_time': 3.2863223552703857, 'eval/sps': 304.2915124854585, 'steps': Array(37000., dtype=float32)}
Model epoch 0: train total loss -22.562246322631836, train mean loss 1.2485039234161377, test mean loss [1.5002275 1.4028921 1.4423051 1.4216509 1.4750314 1.6505331 1.3354875]
Model epoch 1: train total loss -22.216230392456055, train mean loss 1.2764463424682617, test mean loss [1.4108521 1.4100093 1.4707346 1.4306424 1.4203341 1.7660972 1.3709944]
Model epoch 2: train total loss -23.888656616210938, train mean loss 1.1331593990325928, test mean loss [1.4544066 1.4590014 1.4419866 1.4576063 1.4307275 1.6200181 1.3729455]
Model epoch 3: train total loss -22.66421890258789, train mean loss 1.2344701290130615, test mean loss [1.4304223 1.5164218 1.4541768 1.4782841 1.5022936 1.5578179 1.379952 ]
Model epoch 4: train total loss -21.5395450592041, train mean loss 1.3333487510681152, test mean loss [1.4261552 1.7577553 1.4720117 1.5021212 1.5290197 1.7327554 1.4114661]
Model epoch 5: train total loss -22.845006942749023, train mean loss 1.222240924835205, test mean loss [1.4579434 1.5583801 1.4608777 1.4935662 1.4853367 1.5680932 1.4337851]
Model epoch 6: train total loss -23.434661865234375, train mean loss 1.2345831394195557, test mean loss [1.459548  1.5378355 1.4691714 1.5193981 1.4703482 1.577592  1.4439185]
Model epoch 7: train total loss -23.250732421875, train mean loss 1.199548363685608, test mean loss [1.4521042 1.518056  1.5098574 1.5370939 1.4808321 1.558809  1.4135468]
Model epoch 8: train total loss -23.520784378051758, train mean loss 1.2187600135803223, test mean loss [1.4688739 1.5219529 1.4945555 1.5146828 1.475487  1.5635387 1.4327664]
Model epoch 9: train total loss -23.376720428466797, train mean loss 1.341934084892273, test mean loss [1.4620416 1.5346644 1.5335634 1.545177  1.4830296 1.5692251 1.4442339]
Model trained in 10 epochs with 37000 transitions.
[2025-01-29 16:54:16,299][absl][INFO] - {'eval/walltime': 132.06468796730042, 'training/sps': 44.01620409310954, 'training/walltime': 888.681111574173, 'training/model_train_time': 13.430138111114502, 'training/other_time': 9.279455661773682, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(38000, dtype=int32), 'model/train_total_loss': Array(-23.37672, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3419341, dtype=float32), 'model/test_total_loss': Array(-20.812786, dtype=float32), 'model/test_mean_loss': Array(1.5102766, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 1.1201212882995606, 'sac/actor_loss': Array(55.73665, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.30141407, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(75.4215, dtype=float32), 'eval/episode_distance_from_origin': Array(4.0385604, dtype=float32), 'eval/episode_forward_reward': Array(-1.4263937, dtype=float32), 'eval/episode_reward': Array(-12.129648, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-14.138342, dtype=float32), 'eval/episode_reward_forward': Array(-1.4263937, dtype=float32), 'eval/episode_reward_survive': Array(5., dtype=float32), 'eval/episode_x_position': Array(0.00503931, dtype=float32), 'eval/episode_x_velocity': Array(-1.4263937, dtype=float32), 'eval/episode_y_position': Array(0.6859556, dtype=float32), 'eval/episode_y_velocity': Array(4.19442, dtype=float32), 'eval/avg_episode_length': Array(6., dtype=float32), 'eval/epoch_eval_time': 3.2852864265441895, 'eval/sps': 304.38746281611293}
38000.0 {'eval/walltime': 132.06468796730042, 'training/sps': 44.01620409310954, 'training/walltime': 888.681111574173, 'training/model_train_time': 13.430138111114502, 'training/other_time': 9.279455661773682, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(38000, dtype=int32), 'model/train_total_loss': Array(-23.37672, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3419341, dtype=float32), 'model/test_total_loss': Array(-20.812786, dtype=float32), 'model/test_mean_loss': Array(1.5102766, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 1.1201212882995606, 'sac/actor_loss': Array(55.73665, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.30141407, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(75.4215, dtype=float32), 'eval/episode_distance_from_origin': Array(4.0385604, dtype=float32), 'eval/episode_forward_reward': Array(-1.4263937, dtype=float32), 'eval/episode_reward': Array(-12.129648, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-14.138342, dtype=float32), 'eval/episode_reward_forward': Array(-1.4263937, dtype=float32), 'eval/episode_reward_survive': Array(5., dtype=float32), 'eval/episode_x_position': Array(0.00503931, dtype=float32), 'eval/episode_x_velocity': Array(-1.4263937, dtype=float32), 'eval/episode_y_position': Array(0.6859556, dtype=float32), 'eval/episode_y_velocity': Array(4.19442, dtype=float32), 'eval/avg_episode_length': Array(6., dtype=float32), 'eval/epoch_eval_time': 3.2852864265441895, 'eval/sps': 304.38746281611293, 'steps': Array(38000., dtype=float32)}
Model epoch 0: train total loss -21.67965316772461, train mean loss 1.330754041671753, test mean loss [1.2792621 1.3437225 1.3617772 1.4485563 1.3382845 1.462548  1.4439464]
Model epoch 1: train total loss -21.998947143554688, train mean loss 1.3604979515075684, test mean loss [1.3605393 1.4924626 1.3560067 1.3853328 1.4126619 1.6066642 1.3042928]
Model epoch 2: train total loss -22.54811668395996, train mean loss 1.2568906545639038, test mean loss [1.3351604 1.5900955 1.3841274 1.5975021 1.3219978 1.5226468 1.3358016]
Model epoch 3: train total loss -22.197240829467773, train mean loss 1.3147053718566895, test mean loss [1.3525497 1.412242  1.3870031 1.4770591 1.3595151 1.4963734 1.3101972]
Model epoch 4: train total loss -23.33055877685547, train mean loss 1.1749171018600464, test mean loss [1.3821969 1.4871763 1.452906  1.4579463 1.385025  1.4849445 1.3486067]
Model epoch 5: train total loss -23.546859741210938, train mean loss 1.2151552438735962, test mean loss [1.3827193 1.4018627 1.4434658 1.4846164 1.4090066 1.5389513 1.3779399]
Model epoch 6: train total loss -23.275619506835938, train mean loss 1.3171887397766113, test mean loss [1.4227284 1.4354398 1.4272101 1.455115  1.4378629 1.4886993 1.3748032]
Model epoch 7: train total loss -23.62256622314453, train mean loss 1.2178306579589844, test mean loss [1.4256083 1.4445803 1.464695  1.4861112 1.3872689 1.499426  1.3834883]
Model epoch 8: train total loss -24.19765853881836, train mean loss 1.100527286529541, test mean loss [1.4146916 1.4629956 1.447612  1.538925  1.4562383 1.5141132 1.4317963]
Model trained in 9 epochs with 38000 transitions.
[2025-01-29 16:54:41,927][absl][INFO] - {'eval/walltime': 135.34231328964233, 'training/sps': 44.76237299623925, 'training/walltime': 911.0213034152985, 'training/model_train_time': 13.060631036758423, 'training/other_time': 9.270779848098755, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(39000, dtype=int32), 'model/train_total_loss': Array(-24.197659, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.1005273, dtype=float32), 'model/test_total_loss': Array(-20.965237, dtype=float32), 'model/test_mean_loss': Array(1.4666246, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.186802281273736, 'sac/actor_loss': Array(44.239372, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.58679783, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(50.887882, dtype=float32), 'eval/episode_distance_from_origin': Array(3.1312256, dtype=float32), 'eval/episode_forward_reward': Array(0.7967359, dtype=float32), 'eval/episode_reward': Array(-5.3653736, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-8.463304, dtype=float32), 'eval/episode_reward_forward': Array(0.7967359, dtype=float32), 'eval/episode_reward_survive': Array(4., dtype=float32), 'eval/episode_x_position': Array(-0.02592516, dtype=float32), 'eval/episode_x_velocity': Array(0.7967359, dtype=float32), 'eval/episode_y_position': Array(-0.20724155, dtype=float32), 'eval/episode_y_velocity': Array(-1.667074, dtype=float32), 'eval/avg_episode_length': Array(5., dtype=float32), 'eval/epoch_eval_time': 3.277625322341919, 'eval/sps': 305.0989364719342}
39000.0 {'eval/walltime': 135.34231328964233, 'training/sps': 44.76237299623925, 'training/walltime': 911.0213034152985, 'training/model_train_time': 13.060631036758423, 'training/other_time': 9.270779848098755, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(39000, dtype=int32), 'model/train_total_loss': Array(-24.197659, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.1005273, dtype=float32), 'model/test_total_loss': Array(-20.965237, dtype=float32), 'model/test_mean_loss': Array(1.4666246, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.186802281273736, 'sac/actor_loss': Array(44.239372, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.58679783, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(50.887882, dtype=float32), 'eval/episode_distance_from_origin': Array(3.1312256, dtype=float32), 'eval/episode_forward_reward': Array(0.7967359, dtype=float32), 'eval/episode_reward': Array(-5.3653736, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-8.463304, dtype=float32), 'eval/episode_reward_forward': Array(0.7967359, dtype=float32), 'eval/episode_reward_survive': Array(4., dtype=float32), 'eval/episode_x_position': Array(-0.02592516, dtype=float32), 'eval/episode_x_velocity': Array(0.7967359, dtype=float32), 'eval/episode_y_position': Array(-0.20724155, dtype=float32), 'eval/episode_y_velocity': Array(-1.667074, dtype=float32), 'eval/avg_episode_length': Array(5., dtype=float32), 'eval/epoch_eval_time': 3.277625322341919, 'eval/sps': 305.0989364719342, 'steps': Array(39000., dtype=float32)}
Model epoch 0: train total loss -21.672603607177734, train mean loss 1.4194046258926392, test mean loss [1.283001  1.3164113 1.3616111 1.3320643 1.2497791 1.4221085 1.2650391]
Model epoch 1: train total loss -18.641159057617188, train mean loss 1.4110634326934814, test mean loss [1.3001578 1.3216296 1.302732  1.4729058 1.2792455 1.4552519 1.2429913]
Model epoch 2: train total loss -22.2280216217041, train mean loss 1.2699813842773438, test mean loss [1.3219744 1.3129992 1.3221179 1.5751294 1.3220547 1.4386022 1.2240149]
Model epoch 3: train total loss -23.00985336303711, train mean loss 1.2543877363204956, test mean loss [1.4359435 1.3405517 1.3423477 1.4901896 1.313641  1.4297577 1.2702711]
Model epoch 4: train total loss -23.926776885986328, train mean loss 1.155071496963501, test mean loss [1.346111  1.3751148 1.3268642 1.4484156 1.3121909 1.4058242 1.3321604]
Model epoch 5: train total loss -23.35061264038086, train mean loss 1.2160606384277344, test mean loss [1.3278741 1.3704424 1.4738247 1.42898   1.2906984 1.4845473 1.3611866]
Model epoch 6: train total loss -23.481849670410156, train mean loss 1.2125837802886963, test mean loss [1.356071  1.4479377 1.3931084 1.4585564 1.345219  1.438663  1.3020288]
Model epoch 7: train total loss -22.823490142822266, train mean loss 1.274948000907898, test mean loss [1.3446835 1.4237903 1.3981216 1.4524955 1.3798048 1.4180323 1.3339367]
Model epoch 8: train total loss -23.07573699951172, train mean loss 1.2592053413391113, test mean loss [1.3758982 1.4061476 1.3752564 1.4537979 1.318471  1.4439231 1.3147802]
Model epoch 9: train total loss -24.096328735351562, train mean loss 1.1498523950576782, test mean loss [1.3840061 1.4663235 1.3879522 1.4512451 1.3643935 1.4433695 1.3449498]
Model epoch 10: train total loss -22.28973388671875, train mean loss 1.385525107383728, test mean loss [1.3678315 1.9394795 1.4004266 1.4601786 1.3439618 1.458032  1.3671778]
Model trained in 11 epochs with 39000 transitions.
[2025-01-29 16:55:09,634][absl][INFO] - {'eval/walltime': 138.6294503211975, 'training/sps': 40.96713244176926, 'training/walltime': 935.4311153888702, 'training/model_train_time': 15.145468950271606, 'training/other_time': 9.255237102508545, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(40000, dtype=int32), 'model/train_total_loss': Array(-22.289734, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3855251, dtype=float32), 'model/test_total_loss': Array(-20.588533, dtype=float32), 'model/test_mean_loss': Array(1.4767269, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.1626885370774702, 'sac/actor_loss': Array(31.176193, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.93657535, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(30.275677, dtype=float32), 'eval/episode_distance_from_origin': Array(30.738909, dtype=float32), 'eval/episode_forward_reward': Array(19.360462, dtype=float32), 'eval/episode_reward': Array(-0.5609262, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-52.451523, dtype=float32), 'eval/episode_reward_forward': Array(19.360462, dtype=float32), 'eval/episode_reward_survive': Array(33., dtype=float32), 'eval/episode_x_position': Array(10.751708, dtype=float32), 'eval/episode_x_velocity': Array(19.360462, dtype=float32), 'eval/episode_y_position': Array(-13.51685, dtype=float32), 'eval/episode_y_velocity': Array(-10.983906, dtype=float32), 'eval/avg_episode_length': Array(34., dtype=float32), 'eval/epoch_eval_time': 3.287137031555176, 'eval/sps': 304.2160975950828}
40000.0 {'eval/walltime': 138.6294503211975, 'training/sps': 40.96713244176926, 'training/walltime': 935.4311153888702, 'training/model_train_time': 15.145468950271606, 'training/other_time': 9.255237102508545, 'training/model_horizon': 2, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(40000, dtype=int32), 'model/train_total_loss': Array(-22.289734, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.3855251, dtype=float32), 'model/test_total_loss': Array(-20.588533, dtype=float32), 'model/test_mean_loss': Array(1.4767269, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.1626885370774702, 'sac/actor_loss': Array(31.176193, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.93657535, dtype=float32), 'sac/buffer_current_size': Array(401398.8, dtype=float32), 'sac/critic_loss': Array(30.275677, dtype=float32), 'eval/episode_distance_from_origin': Array(30.738909, dtype=float32), 'eval/episode_forward_reward': Array(19.360462, dtype=float32), 'eval/episode_reward': Array(-0.5609262, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-52.451523, dtype=float32), 'eval/episode_reward_forward': Array(19.360462, dtype=float32), 'eval/episode_reward_survive': Array(33., dtype=float32), 'eval/episode_x_position': Array(10.751708, dtype=float32), 'eval/episode_x_velocity': Array(19.360462, dtype=float32), 'eval/episode_y_position': Array(-13.51685, dtype=float32), 'eval/episode_y_velocity': Array(-10.983906, dtype=float32), 'eval/avg_episode_length': Array(34., dtype=float32), 'eval/epoch_eval_time': 3.287137031555176, 'eval/sps': 304.2160975950828, 'steps': Array(40000., dtype=float32)}
Model horizon updated to 3.
Hallucination updates per training step updated to 1000.
SAC buffer resized to 1200000 samples.
Model epoch 0: train total loss -21.74586296081543, train mean loss 1.3407790660858154, test mean loss [1.2928191 1.4807428 1.4154966 1.3889481 1.2686955 1.3582611 1.2791892]
Model epoch 1: train total loss -22.048986434936523, train mean loss 1.4230101108551025, test mean loss [1.3100142 1.4147742 1.3239428 1.3594717 1.728391  1.3940935 1.320319 ]
Model epoch 2: train total loss -22.74771499633789, train mean loss 1.2749980688095093, test mean loss [1.3049321 1.3828876 1.3564775 1.3547568 1.4881976 1.4037646 1.3486708]
Model epoch 3: train total loss -22.062185287475586, train mean loss 1.3807824850082397, test mean loss [1.3191    1.393963  1.3512161 1.3738519 1.4697373 1.4099869 1.3745126]
Model epoch 4: train total loss -23.699970245361328, train mean loss 1.1494604349136353, test mean loss [1.3134438 1.4243481 1.3448672 1.4790277 1.4742408 1.4733809 1.3744857]
Model epoch 5: train total loss -23.562314987182617, train mean loss 1.1803486347198486, test mean loss [1.330232  1.4449383 1.3876699 1.387473  1.4289008 1.5758684 1.4345474]
Model epoch 6: train total loss -23.408096313476562, train mean loss 1.204773187637329, test mean loss [1.3574226 1.3840883 1.3885068 1.3944877 1.4271107 1.502644  1.3839064]
Model epoch 7: train total loss -23.233089447021484, train mean loss 1.2381045818328857, test mean loss [1.3720587 1.388299  1.3572512 1.4482527 1.4527626 1.5175424 1.4777502]
Model epoch 8: train total loss -24.797012329101562, train mean loss 1.1299183368682861, test mean loss [1.3549138 1.4142015 1.3777139 1.4441024 1.4110985 1.5006796 1.4182808]
Model trained in 9 epochs with 40000 transitions.
[2025-01-29 16:55:52,034][absl][INFO] - {'eval/walltime': 141.89979815483093, 'training/sps': 25.582604484754437, 'training/walltime': 974.5201768875122, 'training/model_train_time': 12.83526611328125, 'training/other_time': 26.244460821151733, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(41000, dtype=int32), 'model/train_total_loss': Array(-24.797012, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.1299183, dtype=float32), 'model/test_total_loss': Array(-21.671925, dtype=float32), 'model/test_mean_loss': Array(1.4172845, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.202122449874878, 'sac/actor_loss': Array(19.982056, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.246456, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(21.197561, dtype=float32), 'eval/episode_distance_from_origin': Array(10.615776, dtype=float32), 'eval/episode_forward_reward': Array(1.8885583, dtype=float32), 'eval/episode_reward': Array(-5.888341, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-21.537516, dtype=float32), 'eval/episode_reward_forward': Array(1.8885583, dtype=float32), 'eval/episode_reward_survive': Array(15., dtype=float32), 'eval/episode_x_position': Array(1.992528, dtype=float32), 'eval/episode_x_velocity': Array(1.8885583, dtype=float32), 'eval/episode_y_position': Array(3.179192, dtype=float32), 'eval/episode_y_velocity': Array(7.72533, dtype=float32), 'eval/avg_episode_length': Array(16., dtype=float32), 'eval/epoch_eval_time': 3.270347833633423, 'eval/sps': 305.77787161220084}
41000.0 {'eval/walltime': 141.89979815483093, 'training/sps': 25.582604484754437, 'training/walltime': 974.5201768875122, 'training/model_train_time': 12.83526611328125, 'training/other_time': 26.244460821151733, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(41000, dtype=int32), 'model/train_total_loss': Array(-24.797012, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.1299183, dtype=float32), 'model/test_total_loss': Array(-21.671925, dtype=float32), 'model/test_mean_loss': Array(1.4172845, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.202122449874878, 'sac/actor_loss': Array(19.982056, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.246456, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(21.197561, dtype=float32), 'eval/episode_distance_from_origin': Array(10.615776, dtype=float32), 'eval/episode_forward_reward': Array(1.8885583, dtype=float32), 'eval/episode_reward': Array(-5.888341, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-21.537516, dtype=float32), 'eval/episode_reward_forward': Array(1.8885583, dtype=float32), 'eval/episode_reward_survive': Array(15., dtype=float32), 'eval/episode_x_position': Array(1.992528, dtype=float32), 'eval/episode_x_velocity': Array(1.8885583, dtype=float32), 'eval/episode_y_position': Array(3.179192, dtype=float32), 'eval/episode_y_velocity': Array(7.72533, dtype=float32), 'eval/avg_episode_length': Array(16., dtype=float32), 'eval/epoch_eval_time': 3.270347833633423, 'eval/sps': 305.77787161220084, 'steps': Array(41000., dtype=float32)}
Model epoch 0: train total loss -22.877696990966797, train mean loss 1.2478584051132202, test mean loss [1.2387935 1.2683042 1.2444414 1.3056172 1.2689917 1.483494  1.1855595]
Model epoch 1: train total loss -21.50699234008789, train mean loss 1.3475233316421509, test mean loss [1.3387611 1.4316729 1.2994328 1.4363145 1.3244445 1.3822508 1.2231784]
Model epoch 2: train total loss -23.165546417236328, train mean loss 1.154091715812683, test mean loss [1.2604356 1.3212649 1.2874465 1.3641158 1.3026414 1.3997478 1.2190328]
Model epoch 3: train total loss -23.017013549804688, train mean loss 1.2314428091049194, test mean loss [1.2696395 1.341638  1.3114462 1.3758482 1.2851009 1.3941375 1.4988047]
Model epoch 4: train total loss -23.666423797607422, train mean loss 1.2518513202667236, test mean loss [1.4028157 1.4546701 1.3172104 1.4002657 1.3043883 1.379049  1.2748953]
Model epoch 5: train total loss -23.443761825561523, train mean loss 1.2632843255996704, test mean loss [1.3204476 1.344342  1.321808  1.3992444 1.3045142 1.4149759 1.2594678]
Model epoch 6: train total loss -23.165157318115234, train mean loss 1.2148587703704834, test mean loss [1.3178643 1.3517689 1.3412793 1.3650782 1.3096943 1.4307342 1.2811604]
Model epoch 7: train total loss -23.713502883911133, train mean loss 1.1736834049224854, test mean loss [1.2894137 1.3377546 1.3473692 1.3730134 1.3052126 1.3940834 1.2867303]
Model trained in 8 epochs with 41000 transitions.
[2025-01-29 16:56:17,793][absl][INFO] - {'eval/walltime': 145.17680382728577, 'training/sps': 44.50056536766935, 'training/walltime': 996.9918015003204, 'training/model_train_time': 12.484788179397583, 'training/other_time': 9.976248979568481, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(42000, dtype=int32), 'model/train_total_loss': Array(-23.713503, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.1736834, dtype=float32), 'model/test_total_loss': Array(-22.103619, dtype=float32), 'model/test_mean_loss': Array(1.3333681, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 1.2907492518424988, 'sac/actor_loss': Array(11.655908, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.4417737, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(13.922128, dtype=float32), 'eval/episode_distance_from_origin': Array(65.76485, dtype=float32), 'eval/episode_forward_reward': Array(26.779907, dtype=float32), 'eval/episode_reward': Array(17.881674, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-59.49176, dtype=float32), 'eval/episode_reward_forward': Array(26.779907, dtype=float32), 'eval/episode_reward_survive': Array(51., dtype=float32), 'eval/episode_x_position': Array(29.101416, dtype=float32), 'eval/episode_x_velocity': Array(26.779907, dtype=float32), 'eval/episode_y_position': Array(36.80913, dtype=float32), 'eval/episode_y_velocity': Array(32.94295, dtype=float32), 'eval/avg_episode_length': Array(52., dtype=float32), 'eval/epoch_eval_time': 3.277005672454834, 'eval/sps': 305.15662771217944}
42000.0 {'eval/walltime': 145.17680382728577, 'training/sps': 44.50056536766935, 'training/walltime': 996.9918015003204, 'training/model_train_time': 12.484788179397583, 'training/other_time': 9.976248979568481, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(42000, dtype=int32), 'model/train_total_loss': Array(-23.713503, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.1736834, dtype=float32), 'model/test_total_loss': Array(-22.103619, dtype=float32), 'model/test_mean_loss': Array(1.3333681, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 1.2907492518424988, 'sac/actor_loss': Array(11.655908, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.4417737, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(13.922128, dtype=float32), 'eval/episode_distance_from_origin': Array(65.76485, dtype=float32), 'eval/episode_forward_reward': Array(26.779907, dtype=float32), 'eval/episode_reward': Array(17.881674, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-59.49176, dtype=float32), 'eval/episode_reward_forward': Array(26.779907, dtype=float32), 'eval/episode_reward_survive': Array(51., dtype=float32), 'eval/episode_x_position': Array(29.101416, dtype=float32), 'eval/episode_x_velocity': Array(26.779907, dtype=float32), 'eval/episode_y_position': Array(36.80913, dtype=float32), 'eval/episode_y_velocity': Array(32.94295, dtype=float32), 'eval/avg_episode_length': Array(52., dtype=float32), 'eval/epoch_eval_time': 3.277005672454834, 'eval/sps': 305.15662771217944, 'steps': Array(42000., dtype=float32)}
Model epoch 0: train total loss -22.438732147216797, train mean loss 1.22344172000885, test mean loss [1.5218661 1.2497486 1.2529718 1.2690716 1.3501807 1.3392854 1.4547617]
Model epoch 1: train total loss -22.177589416503906, train mean loss 1.3243541717529297, test mean loss [1.3499609 1.2657543 1.33208   1.4563323 1.2749618 1.3851069 1.3465106]
Model epoch 2: train total loss -22.661161422729492, train mean loss 1.236906886100769, test mean loss [1.320026  1.248849  1.3394432 1.3440593 1.2793856 1.4542317 1.2957615]
Model epoch 3: train total loss -22.944135665893555, train mean loss 1.2070024013519287, test mean loss [1.3001951 1.3203876 1.3006299 1.3495648 1.3319815 1.3751326 1.314353 ]
Model epoch 4: train total loss -24.417505264282227, train mean loss 1.106204867362976, test mean loss [1.3054193 1.2976936 1.3316991 1.374278  1.3131868 1.3715453 1.3304285]
Model epoch 5: train total loss -24.014545440673828, train mean loss 1.1706924438476562, test mean loss [1.3036133 1.3182309 1.3188685 1.3623077 1.3242712 1.3978657 1.2771565]
Model epoch 6: train total loss -23.676265716552734, train mean loss 1.2166191339492798, test mean loss [1.3079829 1.3040327 1.3276917 1.3774855 1.3314829 1.3705912 1.288237 ]
Model epoch 7: train total loss -24.10995101928711, train mean loss 1.1670958995819092, test mean loss [1.3444082 1.3248416 1.5491058 1.3998353 1.3617985 1.3976843 1.2937177]
Model epoch 8: train total loss -24.87261390686035, train mean loss 1.0407980680465698, test mean loss [1.3395224 1.3542371 1.3500265 1.3681225 1.3708103 1.4053258 1.391135 ]
Model epoch 9: train total loss -23.61501121520996, train mean loss 1.1661394834518433, test mean loss [1.3302293 1.3440771 1.3634973 1.3870062 1.3568916 1.494215  1.3010858]
Model epoch 10: train total loss -24.677547454833984, train mean loss 1.103980302810669, test mean loss [1.336763  1.3429912 1.3604592 1.3903875 1.3893651 1.4125208 1.2869841]
Model epoch 11: train total loss -24.389612197875977, train mean loss 1.0999956130981445, test mean loss [1.3256382 1.3971103 1.3776511 1.4912187 1.480317  1.4392288 1.4027997]
Model trained in 12 epochs with 42000 transitions.
[2025-01-29 16:56:48,246][absl][INFO] - {'eval/walltime': 148.47045516967773, 'training/sps': 36.83342059926336, 'training/walltime': 1024.1410584449768, 'training/model_train_time': 17.180332899093628, 'training/other_time': 9.95868706703186, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(43000, dtype=int32), 'model/train_total_loss': Array(-24.389612, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0999956, dtype=float32), 'model/test_total_loss': Array(-21.428694, dtype=float32), 'model/test_mean_loss': Array(1.4162805, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.2311004002888997, 'sac/actor_loss': Array(5.896205, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.4602381, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(12.196053, dtype=float32), 'eval/episode_distance_from_origin': Array(103.710205, dtype=float32), 'eval/episode_forward_reward': Array(51.0945, dtype=float32), 'eval/episode_reward': Array(56.40681, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-78.50325, dtype=float32), 'eval/episode_reward_forward': Array(51.0945, dtype=float32), 'eval/episode_reward_survive': Array(83., dtype=float32), 'eval/episode_x_position': Array(74.34538, dtype=float32), 'eval/episode_x_velocity': Array(51.0945, dtype=float32), 'eval/episode_y_position': Array(-16.49536, dtype=float32), 'eval/episode_y_velocity': Array(-3.886624, dtype=float32), 'eval/avg_episode_length': Array(84., dtype=float32), 'eval/epoch_eval_time': 3.2936513423919678, 'eval/sps': 303.61440724741806}
43000.0 {'eval/walltime': 148.47045516967773, 'training/sps': 36.83342059926336, 'training/walltime': 1024.1410584449768, 'training/model_train_time': 17.180332899093628, 'training/other_time': 9.95868706703186, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(43000, dtype=int32), 'model/train_total_loss': Array(-24.389612, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0999956, dtype=float32), 'model/test_total_loss': Array(-21.428694, dtype=float32), 'model/test_mean_loss': Array(1.4162805, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.2311004002888997, 'sac/actor_loss': Array(5.896205, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.4602381, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(12.196053, dtype=float32), 'eval/episode_distance_from_origin': Array(103.710205, dtype=float32), 'eval/episode_forward_reward': Array(51.0945, dtype=float32), 'eval/episode_reward': Array(56.40681, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-78.50325, dtype=float32), 'eval/episode_reward_forward': Array(51.0945, dtype=float32), 'eval/episode_reward_survive': Array(83., dtype=float32), 'eval/episode_x_position': Array(74.34538, dtype=float32), 'eval/episode_x_velocity': Array(51.0945, dtype=float32), 'eval/episode_y_position': Array(-16.49536, dtype=float32), 'eval/episode_y_velocity': Array(-3.886624, dtype=float32), 'eval/avg_episode_length': Array(84., dtype=float32), 'eval/epoch_eval_time': 3.2936513423919678, 'eval/sps': 303.61440724741806, 'steps': Array(43000., dtype=float32)}
Model epoch 0: train total loss -20.458724975585938, train mean loss 1.470007300376892, test mean loss [1.3019754 1.3394231 1.2775773 1.4337654 1.7756742 1.3013555 1.2727396]
Model epoch 1: train total loss -21.554887771606445, train mean loss 1.2602496147155762, test mean loss [1.2570026 1.3442466 1.5057195 1.3297955 1.3538127 1.3900765 1.4902651]
Model epoch 2: train total loss -22.378219604492188, train mean loss 1.3035407066345215, test mean loss [1.2856171 1.3436165 1.3142308 1.3330133 1.2947848 1.4875416 1.3722583]
Model epoch 3: train total loss -23.409053802490234, train mean loss 1.1792494058609009, test mean loss [1.2622907 1.3898395 1.2896656 1.3500509 1.3035613 1.3656857 1.3304458]
Model epoch 4: train total loss -24.06957244873047, train mean loss 1.103601336479187, test mean loss [1.2650901 1.4134985 1.3078868 1.3330859 1.3163844 1.3661726 1.3877922]
Model epoch 5: train total loss -24.210601806640625, train mean loss 1.1384214162826538, test mean loss [1.2612073 1.3406931 1.3311309 1.6413937 1.3219643 1.382583  1.3158333]
Model epoch 6: train total loss -24.169448852539062, train mean loss 1.1215863227844238, test mean loss [1.2780435 1.3551304 1.3035021 1.5199335 1.3343986 1.3752388 1.300056 ]
Model epoch 7: train total loss -23.698486328125, train mean loss 1.1456499099731445, test mean loss [1.2781188 1.3615806 1.4475609 1.418505  1.366182  1.364852  1.3003052]
Model epoch 8: train total loss -24.6081600189209, train mean loss 1.0700236558914185, test mean loss [1.4184898 1.4108803 1.3736994 1.4150013 1.3380226 1.4176568 1.3309197]
Model trained in 9 epochs with 43000 transitions.
[2025-01-29 16:57:16,518][absl][INFO] - {'eval/walltime': 151.76191902160645, 'training/sps': 40.04916504641151, 'training/walltime': 1049.110368013382, 'training/model_train_time': 14.988624095916748, 'training/other_time': 9.970012664794922, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(44000, dtype=int32), 'model/train_total_loss': Array(-24.60816, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0700237, dtype=float32), 'model/test_total_loss': Array(-21.656572, dtype=float32), 'model/test_mean_loss': Array(1.3863815, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.3700623247358534, 'sac/actor_loss': Array(0.7588774, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.3945307, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(11.079837, dtype=float32), 'eval/episode_distance_from_origin': Array(5.8514314, dtype=float32), 'eval/episode_forward_reward': Array(4.7271624, dtype=float32), 'eval/episode_reward': Array(5.176335, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-9.916438, dtype=float32), 'eval/episode_reward_forward': Array(4.7271624, dtype=float32), 'eval/episode_reward_survive': Array(10., dtype=float32), 'eval/episode_x_position': Array(1.5635455, dtype=float32), 'eval/episode_x_velocity': Array(4.7271624, dtype=float32), 'eval/episode_y_position': Array(-0.27523363, dtype=float32), 'eval/episode_y_velocity': Array(2.1156259, dtype=float32), 'eval/avg_episode_length': Array(11., dtype=float32), 'eval/epoch_eval_time': 3.291463851928711, 'eval/sps': 303.8161878685152}
44000.0 {'eval/walltime': 151.76191902160645, 'training/sps': 40.04916504641151, 'training/walltime': 1049.110368013382, 'training/model_train_time': 14.988624095916748, 'training/other_time': 9.970012664794922, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(44000, dtype=int32), 'model/train_total_loss': Array(-24.60816, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0700237, dtype=float32), 'model/test_total_loss': Array(-21.656572, dtype=float32), 'model/test_mean_loss': Array(1.3863815, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.3700623247358534, 'sac/actor_loss': Array(0.7588774, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.3945307, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(11.079837, dtype=float32), 'eval/episode_distance_from_origin': Array(5.8514314, dtype=float32), 'eval/episode_forward_reward': Array(4.7271624, dtype=float32), 'eval/episode_reward': Array(5.176335, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-9.916438, dtype=float32), 'eval/episode_reward_forward': Array(4.7271624, dtype=float32), 'eval/episode_reward_survive': Array(10., dtype=float32), 'eval/episode_x_position': Array(1.5635455, dtype=float32), 'eval/episode_x_velocity': Array(4.7271624, dtype=float32), 'eval/episode_y_position': Array(-0.27523363, dtype=float32), 'eval/episode_y_velocity': Array(2.1156259, dtype=float32), 'eval/avg_episode_length': Array(11., dtype=float32), 'eval/epoch_eval_time': 3.291463851928711, 'eval/sps': 303.8161878685152, 'steps': Array(44000., dtype=float32)}
Model epoch 0: train total loss -22.14056968688965, train mean loss 1.2320181131362915, test mean loss [1.1797451 1.4299693 1.2309376 1.3233179 1.2522563 1.5160238 1.2341783]
Model epoch 1: train total loss -24.279014587402344, train mean loss 1.0734260082244873, test mean loss [1.1354333 1.2161108 1.1256206 1.2543901 1.3125633 1.5039085 1.2802488]
Model epoch 2: train total loss -23.992345809936523, train mean loss 1.0853400230407715, test mean loss [1.1732719 1.2168654 1.1569916 1.2294395 1.1879323 1.52298   1.2236357]
Model epoch 3: train total loss -23.906343460083008, train mean loss 1.07944917678833, test mean loss [1.163553  1.211489  1.1298696 1.2134978 1.3145913 1.5190221 1.2045373]
Model epoch 4: train total loss -23.6275634765625, train mean loss 1.0978494882583618, test mean loss [1.1271145 1.3532314 1.1245824 1.2723677 1.269061  1.4773182 1.2799053]
Model epoch 5: train total loss -23.916330337524414, train mean loss 1.2071083784103394, test mean loss [1.1460438 1.2589964 1.1068735 1.2321588 1.2615217 1.5952044 1.2305886]
Model epoch 6: train total loss -23.640640258789062, train mean loss 1.1893131732940674, test mean loss [1.1617899 1.219787  1.1718897 1.2265244 1.2986866 1.4920248 1.2419841]
Model epoch 7: train total loss -24.57887840270996, train mean loss 1.090399980545044, test mean loss [1.1614373 1.3674966 1.1252025 1.214012  1.25297   1.5644488 1.2361002]
Model epoch 8: train total loss -23.94501304626465, train mean loss 1.2142956256866455, test mean loss [1.1608485 1.2686226 1.12177   1.2172185 1.3148807 1.4796581 1.330146 ]
Model epoch 9: train total loss -24.589523315429688, train mean loss 1.1016483306884766, test mean loss [1.214317  1.3288082 1.1672524 1.256638  1.3360164 1.5623877 1.3998492]
Model epoch 10: train total loss -25.000732421875, train mean loss 1.079521656036377, test mean loss [1.1681514 1.2775654 1.1358461 1.2400556 1.2955427 1.5157055 1.3506007]
Model epoch 11: train total loss -24.72608757019043, train mean loss 1.051409125328064, test mean loss [1.2391466 1.3140991 1.2199254 1.2511721 1.2952964 1.7199917 1.3048997]
Model trained in 12 epochs with 44000 transitions.
[2025-01-29 16:57:47,684][absl][INFO] - {'eval/walltime': 155.0565755367279, 'training/sps': 35.890650636899494, 'training/walltime': 1076.9727773666382, 'training/model_train_time': 17.880243062973022, 'training/other_time': 9.971814155578613, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(45000, dtype=int32), 'model/train_total_loss': Array(-24.726088, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0514091, dtype=float32), 'model/test_total_loss': Array(-22.21372, dtype=float32), 'model/test_mean_loss': Array(1.3349329, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.2790419459342957, 'sac/actor_loss': Array(-0.646759, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.3632346, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(11.697111, dtype=float32), 'eval/episode_distance_from_origin': Array(39.924862, dtype=float32), 'eval/episode_forward_reward': Array(23.211065, dtype=float32), 'eval/episode_reward': Array(23.76832, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-40.869144, dtype=float32), 'eval/episode_reward_forward': Array(23.211065, dtype=float32), 'eval/episode_reward_survive': Array(41., dtype=float32), 'eval/episode_x_position': Array(25.238735, dtype=float32), 'eval/episode_x_velocity': Array(23.211065, dtype=float32), 'eval/episode_y_position': Array(0.19367255, dtype=float32), 'eval/episode_y_velocity': Array(1.7368002, dtype=float32), 'eval/avg_episode_length': Array(42., dtype=float32), 'eval/epoch_eval_time': 3.29465651512146, 'eval/sps': 303.5217769774505}
45000.0 {'eval/walltime': 155.0565755367279, 'training/sps': 35.890650636899494, 'training/walltime': 1076.9727773666382, 'training/model_train_time': 17.880243062973022, 'training/other_time': 9.971814155578613, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(45000, dtype=int32), 'model/train_total_loss': Array(-24.726088, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0514091, dtype=float32), 'model/test_total_loss': Array(-22.21372, dtype=float32), 'model/test_mean_loss': Array(1.3349329, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.2790419459342957, 'sac/actor_loss': Array(-0.646759, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.3632346, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(11.697111, dtype=float32), 'eval/episode_distance_from_origin': Array(39.924862, dtype=float32), 'eval/episode_forward_reward': Array(23.211065, dtype=float32), 'eval/episode_reward': Array(23.76832, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-40.869144, dtype=float32), 'eval/episode_reward_forward': Array(23.211065, dtype=float32), 'eval/episode_reward_survive': Array(41., dtype=float32), 'eval/episode_x_position': Array(25.238735, dtype=float32), 'eval/episode_x_velocity': Array(23.211065, dtype=float32), 'eval/episode_y_position': Array(0.19367255, dtype=float32), 'eval/episode_y_velocity': Array(1.7368002, dtype=float32), 'eval/avg_episode_length': Array(42., dtype=float32), 'eval/epoch_eval_time': 3.29465651512146, 'eval/sps': 303.5217769774505, 'steps': Array(45000., dtype=float32)}
Model epoch 0: train total loss -23.43975257873535, train mean loss 1.1591671705245972, test mean loss [1.2974473 1.2248254 1.2332948 1.1854113 1.2459656 1.2925946 1.1479946]
Model epoch 1: train total loss -23.26378059387207, train mean loss 1.1924402713775635, test mean loss [1.2278414 1.2463353 1.1933206 1.2656081 1.2247062 1.5284249 1.2460996]
Model epoch 2: train total loss -24.088544845581055, train mean loss 1.2010905742645264, test mean loss [1.1412836 1.2127336 1.1849246 1.2143624 1.200037  1.3000157 1.2913064]
Model epoch 3: train total loss -24.429746627807617, train mean loss 1.086632251739502, test mean loss [1.1880695 1.2045391 1.2290323 1.2321321 1.3705733 1.262815  1.172664 ]
Model epoch 4: train total loss -24.629709243774414, train mean loss 1.115778923034668, test mean loss [1.1876493 1.1835319 1.1811048 1.2379277 1.2542475 1.2676882 1.2086105]
Model epoch 5: train total loss -24.452661514282227, train mean loss 1.1365159749984741, test mean loss [1.1831298 1.1980935 1.2208129 1.2392584 1.2488124 1.2731824 1.2103634]
Model epoch 6: train total loss -24.710113525390625, train mean loss 1.0806770324707031, test mean loss [1.1731957 1.2272485 1.2385026 1.2758112 1.2411503 1.272367  1.252404 ]
Model epoch 7: train total loss -25.56189727783203, train mean loss 1.0102139711380005, test mean loss [1.2216347 1.243946  1.2435167 1.2675256 1.2290294 1.2649893 1.2152852]
Model epoch 8: train total loss -24.777257919311523, train mean loss 1.0451360940933228, test mean loss [1.1768    1.263547  1.2421038 1.3049217 1.2551666 1.2950732 1.2794892]
Model epoch 9: train total loss -25.438575744628906, train mean loss 1.046385407447815, test mean loss [1.1947577 1.2749505 1.2435505 1.3233172 1.241515  1.3373697 1.234802 ]
Model epoch 10: train total loss -25.389135360717773, train mean loss 1.0111799240112305, test mean loss [1.2707729 1.2497661 1.2686058 1.4565331 1.2429368 1.3018249 1.425984 ]
Model trained in 11 epochs with 45000 transitions.
[2025-01-29 16:58:17,645][absl][INFO] - {'eval/walltime': 158.33748364448547, 'training/sps': 37.49464495220767, 'training/walltime': 1103.6432526111603, 'training/model_train_time': 16.69323754310608, 'training/other_time': 9.966736793518066, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(46000, dtype=int32), 'model/train_total_loss': Array(-25.389135, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0111799, dtype=float32), 'model/test_total_loss': Array(-21.86876, dtype=float32), 'model/test_mean_loss': Array(1.316632, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.3113788474689831, 'sac/actor_loss': Array(-2.051798, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.3234451, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(11.864953, dtype=float32), 'eval/episode_distance_from_origin': Array(70.19056, dtype=float32), 'eval/episode_forward_reward': Array(24.040384, dtype=float32), 'eval/episode_reward': Array(38.812443, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-58.33032, dtype=float32), 'eval/episode_reward_forward': Array(24.040384, dtype=float32), 'eval/episode_reward_survive': Array(74., dtype=float32), 'eval/episode_x_position': Array(41.772297, dtype=float32), 'eval/episode_x_velocity': Array(24.040384, dtype=float32), 'eval/episode_y_position': Array(-30.538176, dtype=float32), 'eval/episode_y_velocity': Array(-11.1941395, dtype=float32), 'eval/avg_episode_length': Array(75., dtype=float32), 'eval/epoch_eval_time': 3.2809081077575684, 'eval/sps': 304.79366296042923}
46000.0 {'eval/walltime': 158.33748364448547, 'training/sps': 37.49464495220767, 'training/walltime': 1103.6432526111603, 'training/model_train_time': 16.69323754310608, 'training/other_time': 9.966736793518066, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(46000, dtype=int32), 'model/train_total_loss': Array(-25.389135, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0111799, dtype=float32), 'model/test_total_loss': Array(-21.86876, dtype=float32), 'model/test_mean_loss': Array(1.316632, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.3113788474689831, 'sac/actor_loss': Array(-2.051798, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.3234451, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(11.864953, dtype=float32), 'eval/episode_distance_from_origin': Array(70.19056, dtype=float32), 'eval/episode_forward_reward': Array(24.040384, dtype=float32), 'eval/episode_reward': Array(38.812443, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-58.33032, dtype=float32), 'eval/episode_reward_forward': Array(24.040384, dtype=float32), 'eval/episode_reward_survive': Array(74., dtype=float32), 'eval/episode_x_position': Array(41.772297, dtype=float32), 'eval/episode_x_velocity': Array(24.040384, dtype=float32), 'eval/episode_y_position': Array(-30.538176, dtype=float32), 'eval/episode_y_velocity': Array(-11.1941395, dtype=float32), 'eval/avg_episode_length': Array(75., dtype=float32), 'eval/epoch_eval_time': 3.2809081077575684, 'eval/sps': 304.79366296042923, 'steps': Array(46000., dtype=float32)}
Model epoch 0: train total loss -24.072790145874023, train mean loss 1.127722978591919, test mean loss [1.1237999 1.1937618 1.1953206 1.1376487 1.1806252 1.1833016 1.1955943]
Model epoch 1: train total loss -24.724254608154297, train mean loss 1.105871558189392, test mean loss [1.1658022 1.1490928 1.173864  1.1590209 1.1822455 1.2124554 1.2077404]
Model epoch 2: train total loss -25.393455505371094, train mean loss 1.0561186075210571, test mean loss [1.1895291 1.1753664 1.1515929 1.1700598 1.1720558 1.2308284 1.1668378]
Model epoch 3: train total loss -24.1954345703125, train mean loss 1.0965263843536377, test mean loss [1.1895734 1.2014347 1.1700093 1.2566761 1.2233807 1.2076553 1.228987 ]
Model epoch 4: train total loss -25.337932586669922, train mean loss 1.0365409851074219, test mean loss [1.1579587 1.1891418 1.1783986 1.2188768 1.1867205 1.2110953 1.3551356]
Model epoch 5: train total loss -24.863683700561523, train mean loss 1.0654795169830322, test mean loss [1.2270757 1.2876774 1.1828724 1.2174412 1.1813093 1.2373792 1.1974201]
Model epoch 6: train total loss -24.59202766418457, train mean loss 1.0961010456085205, test mean loss [1.2102312 1.2160127 1.2084569 1.3265139 1.1950213 1.3682566 1.2128892]
Model epoch 7: train total loss -24.623456954956055, train mean loss 1.0958495140075684, test mean loss [1.2161826 1.3401701 1.2068164 1.2513891 1.21621   1.2759092 1.2728739]
Model epoch 8: train total loss -25.16123390197754, train mean loss 1.0756179094314575, test mean loss [1.2295483 1.3023711 1.2391636 1.2424581 1.2248775 1.306395  1.2371061]
Model trained in 9 epochs with 46000 transitions.
[2025-01-29 16:58:44,979][absl][INFO] - {'eval/walltime': 161.62980103492737, 'training/sps': 41.61128116144323, 'training/walltime': 1127.6751971244812, 'training/model_train_time': 14.058405876159668, 'training/other_time': 9.963257789611816, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(47000, dtype=int32), 'model/train_total_loss': Array(-25.161234, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0756179, dtype=float32), 'model/test_total_loss': Array(-22.861061, dtype=float32), 'model/test_mean_loss': Array(1.25456, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.3290229108598497, 'sac/actor_loss': Array(-3.367294, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.2953998, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(12.725415, dtype=float32), 'eval/episode_distance_from_origin': Array(199.821, dtype=float32), 'eval/episode_forward_reward': Array(34.768284, dtype=float32), 'eval/episode_reward': Array(54.342117, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-95.91103, dtype=float32), 'eval/episode_reward_forward': Array(34.768284, dtype=float32), 'eval/episode_reward_survive': Array(116., dtype=float32), 'eval/episode_x_position': Array(103.44092, dtype=float32), 'eval/episode_x_velocity': Array(34.768284, dtype=float32), 'eval/episode_y_position': Array(-146.01224, dtype=float32), 'eval/episode_y_velocity': Array(-53.266712, dtype=float32), 'eval/avg_episode_length': Array(117., dtype=float32), 'eval/epoch_eval_time': 3.2923173904418945, 'eval/sps': 303.73742303921074}
47000.0 {'eval/walltime': 161.62980103492737, 'training/sps': 41.61128116144323, 'training/walltime': 1127.6751971244812, 'training/model_train_time': 14.058405876159668, 'training/other_time': 9.963257789611816, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(47000, dtype=int32), 'model/train_total_loss': Array(-25.161234, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0756179, dtype=float32), 'model/test_total_loss': Array(-22.861061, dtype=float32), 'model/test_mean_loss': Array(1.25456, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.3290229108598497, 'sac/actor_loss': Array(-3.367294, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.2953998, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(12.725415, dtype=float32), 'eval/episode_distance_from_origin': Array(199.821, dtype=float32), 'eval/episode_forward_reward': Array(34.768284, dtype=float32), 'eval/episode_reward': Array(54.342117, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-95.91103, dtype=float32), 'eval/episode_reward_forward': Array(34.768284, dtype=float32), 'eval/episode_reward_survive': Array(116., dtype=float32), 'eval/episode_x_position': Array(103.44092, dtype=float32), 'eval/episode_x_velocity': Array(34.768284, dtype=float32), 'eval/episode_y_position': Array(-146.01224, dtype=float32), 'eval/episode_y_velocity': Array(-53.266712, dtype=float32), 'eval/avg_episode_length': Array(117., dtype=float32), 'eval/epoch_eval_time': 3.2923173904418945, 'eval/sps': 303.73742303921074, 'steps': Array(47000., dtype=float32)}
Model epoch 0: train total loss -23.436077117919922, train mean loss 1.2564853429794312, test mean loss [1.1114374 1.1727353 1.1480935 1.0878433 1.1201963 1.1622684 1.2658768]
Model epoch 1: train total loss -23.939708709716797, train mean loss 1.150681734085083, test mean loss [1.112042  1.1478814 1.1144823 1.1933048 1.1159289 1.2344192 1.3338337]
Model epoch 2: train total loss -25.27902603149414, train mean loss 1.0285069942474365, test mean loss [1.1100736 1.1285856 1.145548  1.1635299 1.1374664 1.1995834 1.2419698]
Model epoch 3: train total loss -24.466760635375977, train mean loss 1.1554415225982666, test mean loss [1.1203978 1.1407093 1.1363997 1.1596227 1.1920472 1.1986097 1.2114487]
Model epoch 4: train total loss -25.668476104736328, train mean loss 1.0113415718078613, test mean loss [1.1409597 1.184531  1.1554358 1.1565759 1.1909857 1.2038    1.2154176]
Model epoch 5: train total loss -24.868728637695312, train mean loss 1.1381328105926514, test mean loss [1.1405249 1.195534  1.1475939 1.1770254 1.1951473 1.3118542 1.1915648]
Model epoch 6: train total loss -24.20237159729004, train mean loss 1.1726679801940918, test mean loss [1.1934657 1.175006  1.2587113 1.197197  1.2271696 1.2389667 1.1984191]
Model epoch 7: train total loss -25.12024688720703, train mean loss 1.0734522342681885, test mean loss [1.5288801 1.1603394 1.1994324 1.2184947 1.2179625 1.2353361 1.188473 ]
Model epoch 8: train total loss -25.376535415649414, train mean loss 1.0364347696304321, test mean loss [1.2790396 1.1886497 1.1711068 1.208532  1.1860435 1.2236829 1.1996679]
Model epoch 9: train total loss -25.0225887298584, train mean loss 1.0514494180679321, test mean loss [1.5214714 1.199596  1.190914  1.2072964 1.1766162 1.258168  1.1910444]
Model epoch 10: train total loss -25.37139320373535, train mean loss 1.0215420722961426, test mean loss [1.2680099 1.1979305 1.1859636 1.2180376 1.1934565 1.2615675 1.1973833]
Model epoch 11: train total loss -25.781503677368164, train mean loss 0.998817503452301, test mean loss [1.2292616 1.2176741 1.203113  1.2242264 1.1833471 1.230004  1.2010432]
Model trained in 12 epochs with 47000 transitions.
[2025-01-29 16:59:16,910][absl][INFO] - {'eval/walltime': 164.90905237197876, 'training/sps': 34.91320651562736, 'training/walltime': 1156.3176536560059, 'training/model_train_time': 18.675980806350708, 'training/other_time': 9.955122470855713, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(48000, dtype=int32), 'model/train_total_loss': Array(-25.781504, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.9988175, dtype=float32), 'model/test_total_loss': Array(-23.574543, dtype=float32), 'model/test_mean_loss': Array(1.2126671, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.3523132999738057, 'sac/actor_loss': Array(-4.7479653, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.266751, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(13.34444, dtype=float32), 'eval/episode_distance_from_origin': Array(1075.5321, dtype=float32), 'eval/episode_forward_reward': Array(99.40405, dtype=float32), 'eval/episode_reward': Array(113.27986, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-300.32907, dtype=float32), 'eval/episode_reward_forward': Array(99.40405, dtype=float32), 'eval/episode_reward_survive': Array(314., dtype=float32), 'eval/episode_x_position': Array(892.14026, dtype=float32), 'eval/episode_x_velocity': Array(99.40405, dtype=float32), 'eval/episode_y_position': Array(-396.90958, dtype=float32), 'eval/episode_y_velocity': Array(1.4338329, dtype=float32), 'eval/avg_episode_length': Array(315., dtype=float32), 'eval/epoch_eval_time': 3.2792513370513916, 'eval/sps': 304.94765335650396}
48000.0 {'eval/walltime': 164.90905237197876, 'training/sps': 34.91320651562736, 'training/walltime': 1156.3176536560059, 'training/model_train_time': 18.675980806350708, 'training/other_time': 9.955122470855713, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(48000, dtype=int32), 'model/train_total_loss': Array(-25.781504, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.9988175, dtype=float32), 'model/test_total_loss': Array(-23.574543, dtype=float32), 'model/test_mean_loss': Array(1.2126671, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.3523132999738057, 'sac/actor_loss': Array(-4.7479653, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.266751, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(13.34444, dtype=float32), 'eval/episode_distance_from_origin': Array(1075.5321, dtype=float32), 'eval/episode_forward_reward': Array(99.40405, dtype=float32), 'eval/episode_reward': Array(113.27986, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-300.32907, dtype=float32), 'eval/episode_reward_forward': Array(99.40405, dtype=float32), 'eval/episode_reward_survive': Array(314., dtype=float32), 'eval/episode_x_position': Array(892.14026, dtype=float32), 'eval/episode_x_velocity': Array(99.40405, dtype=float32), 'eval/episode_y_position': Array(-396.90958, dtype=float32), 'eval/episode_y_velocity': Array(1.4338329, dtype=float32), 'eval/avg_episode_length': Array(315., dtype=float32), 'eval/epoch_eval_time': 3.2792513370513916, 'eval/sps': 304.94765335650396, 'steps': Array(48000., dtype=float32)}
Model epoch 0: train total loss -23.39516830444336, train mean loss 1.2078826427459717, test mean loss [1.0803987 1.2397224 1.0559247 1.4066875 1.1190348 1.1652987 1.10903  ]
Model epoch 1: train total loss -24.091285705566406, train mean loss 1.0798479318618774, test mean loss [1.1016519 1.187007  1.1546999 1.2146529 1.1267201 1.2285262 1.1827759]
Model epoch 2: train total loss -24.595582962036133, train mean loss 1.0760325193405151, test mean loss [1.1271329 1.1330836 1.1085202 1.2192835 1.112038  1.1945249 1.1550663]
Model epoch 3: train total loss -26.20016860961914, train mean loss 0.9838403463363647, test mean loss [1.114846  1.1426316 1.1117145 1.1682941 1.1383501 1.1615855 1.1394705]
Model epoch 4: train total loss -25.499488830566406, train mean loss 1.007527232170105, test mean loss [1.1231923 1.1451085 1.1064781 1.1837645 1.098563  1.3146808 1.1210598]
Model epoch 5: train total loss -24.88292694091797, train mean loss 1.004937767982483, test mean loss [1.1252478 1.1349212 1.3281387 1.1874955 1.1245414 1.3318123 1.131824 ]
Model epoch 6: train total loss -25.532752990722656, train mean loss 1.004475474357605, test mean loss [1.1917322 1.1716021 1.5154563 1.1689515 1.1463716 1.2211821 1.1458204]
Model epoch 7: train total loss -26.247835159301758, train mean loss 0.9375858902931213, test mean loss [1.1576685 1.161815  1.2236595 1.2229456 1.1438446 1.2370441 1.1532861]
Model epoch 8: train total loss -26.63479232788086, train mean loss 0.907252848148346, test mean loss [1.2264721 1.1788568 1.1712993 1.2509108 1.160505  1.2310257 1.1769507]
Model epoch 9: train total loss -26.343082427978516, train mean loss 0.9701970219612122, test mean loss [1.1979954 1.1566526 1.1800894 1.2072357 1.1661209 1.2693511 1.1794268]
Model epoch 10: train total loss -25.604001998901367, train mean loss 1.0602915287017822, test mean loss [1.2124127 1.1619202 1.1855545 1.2566106 1.2137685 1.2918572 1.1737205]
Model trained in 11 epochs with 48000 transitions.
[2025-01-29 16:59:47,989][absl][INFO] - {'eval/walltime': 168.1953582763672, 'training/sps': 35.994478706423706, 'training/walltime': 1184.0996923446655, 'training/model_train_time': 17.81067991256714, 'training/other_time': 9.960902690887451, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(49000, dtype=int32), 'model/train_total_loss': Array(-25.604002, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0602915, dtype=float32), 'model/test_total_loss': Array(-23.522362, dtype=float32), 'model/test_mean_loss': Array(1.2136921, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.3956576260653408, 'sac/actor_loss': Array(-6.27379, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.2510219, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(13.330218, dtype=float32), 'eval/episode_distance_from_origin': Array(7.3138504, dtype=float32), 'eval/episode_forward_reward': Array(0.89935815, dtype=float32), 'eval/episode_reward': Array(-0.15087646, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-13.20832, dtype=float32), 'eval/episode_reward_forward': Array(0.89935815, dtype=float32), 'eval/episode_reward_survive': Array(12., dtype=float32), 'eval/episode_x_position': Array(-0.715028, dtype=float32), 'eval/episode_x_velocity': Array(0.89935815, dtype=float32), 'eval/episode_y_position': Array(-0.10335898, dtype=float32), 'eval/episode_y_velocity': Array(-1.2446016, dtype=float32), 'eval/avg_episode_length': Array(13., dtype=float32), 'eval/epoch_eval_time': 3.2863059043884277, 'eval/sps': 304.2930357349363}
49000.0 {'eval/walltime': 168.1953582763672, 'training/sps': 35.994478706423706, 'training/walltime': 1184.0996923446655, 'training/model_train_time': 17.81067991256714, 'training/other_time': 9.960902690887451, 'training/model_horizon': 3, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(49000, dtype=int32), 'model/train_total_loss': Array(-25.604002, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0602915, dtype=float32), 'model/test_total_loss': Array(-23.522362, dtype=float32), 'model/test_mean_loss': Array(1.2136921, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.3956576260653408, 'sac/actor_loss': Array(-6.27379, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.2510219, dtype=float32), 'sac/buffer_current_size': Array(601599., dtype=float32), 'sac/critic_loss': Array(13.330218, dtype=float32), 'eval/episode_distance_from_origin': Array(7.3138504, dtype=float32), 'eval/episode_forward_reward': Array(0.89935815, dtype=float32), 'eval/episode_reward': Array(-0.15087646, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-13.20832, dtype=float32), 'eval/episode_reward_forward': Array(0.89935815, dtype=float32), 'eval/episode_reward_survive': Array(12., dtype=float32), 'eval/episode_x_position': Array(-0.715028, dtype=float32), 'eval/episode_x_velocity': Array(0.89935815, dtype=float32), 'eval/episode_y_position': Array(-0.10335898, dtype=float32), 'eval/episode_y_velocity': Array(-1.2446016, dtype=float32), 'eval/avg_episode_length': Array(13., dtype=float32), 'eval/epoch_eval_time': 3.2863059043884277, 'eval/sps': 304.2930357349363, 'steps': Array(49000., dtype=float32)}
Model horizon updated to 4.
Hallucination updates per training step updated to 1000.
SAC buffer resized to 1600000 samples.
Model epoch 0: train total loss -24.786481857299805, train mean loss 1.126829743385315, test mean loss [1.1636192 1.0852127 1.1343869 1.1206583 1.0858544 1.1861086 1.1052881]
Model epoch 1: train total loss -24.94147300720215, train mean loss 1.0842348337173462, test mean loss [1.1068145 1.1511245 1.1164783 1.2568198 1.1339198 1.1524296 1.0973083]
Model epoch 2: train total loss -24.503190994262695, train mean loss 1.0541945695877075, test mean loss [1.3353139 1.2181871 1.1095165 1.1794049 1.1517866 1.1941664 1.1003653]
Model epoch 3: train total loss -24.73946762084961, train mean loss 1.1131585836410522, test mean loss [1.2014197 1.1386414 1.0948641 1.329798  1.1369087 1.2341149 1.0948362]
Model epoch 4: train total loss -25.330482482910156, train mean loss 1.0327208042144775, test mean loss [1.1742742 1.1453696 1.1031564 1.174121  1.1209718 1.1856148 1.1654288]
Model epoch 5: train total loss -26.48082160949707, train mean loss 0.9150964021682739, test mean loss [1.2093366 1.1891878 1.1291376 1.1620209 1.1280985 1.1864114 1.1643507]
Model epoch 6: train total loss -25.681243896484375, train mean loss 1.0043152570724487, test mean loss [1.193001  1.1990672 1.1290432 1.2534074 1.1324939 1.1890155 1.1594473]
Model epoch 7: train total loss -25.5744571685791, train mean loss 0.9921119213104248, test mean loss [1.1795461 1.2663523 1.1754783 1.1936381 1.1533967 1.2406083 1.1478045]
Model epoch 8: train total loss -26.6909236907959, train mean loss 0.9557757377624512, test mean loss [1.1852481 1.2737802 1.1668823 1.1952598 1.3067796 1.2119807 1.1634833]
Model epoch 9: train total loss -25.69309425354004, train mean loss 1.021509051322937, test mean loss [1.2325207 1.232019  1.1825829 1.2002676 1.226051  1.2412815 1.1901883]
Model trained in 10 epochs with 49000 transitions.
[2025-01-29 17:00:37,121][absl][INFO] - {'eval/walltime': 171.47943592071533, 'training/sps': 21.831241607988527, 'training/walltime': 1229.9056074619293, 'training/model_train_time': 16.428722381591797, 'training/other_time': 29.366607189178467, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(50000, dtype=int32), 'model/train_total_loss': Array(-25.693094, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.021509, dtype=float32), 'model/test_total_loss': Array(-23.58481, dtype=float32), 'model/test_mean_loss': Array(1.2149873, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 1.4116641759872437, 'sac/actor_loss': Array(-8.4492855, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.2536443, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(14.789486, dtype=float32), 'eval/episode_distance_from_origin': Array(619.7584, dtype=float32), 'eval/episode_forward_reward': Array(13.003908, dtype=float32), 'eval/episode_reward': Array(20.194828, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-178.67294, dtype=float32), 'eval/episode_reward_forward': Array(13.003908, dtype=float32), 'eval/episode_reward_survive': Array(182., dtype=float32), 'eval/episode_x_position': Array(233.8595, dtype=float32), 'eval/episode_x_velocity': Array(13.003908, dtype=float32), 'eval/episode_y_position': Array(-520.9349, dtype=float32), 'eval/episode_y_velocity': Array(-140.21901, dtype=float32), 'eval/avg_episode_length': Array(183., dtype=float32), 'eval/epoch_eval_time': 3.2840776443481445, 'eval/sps': 304.49949979745065}
50000.0 {'eval/walltime': 171.47943592071533, 'training/sps': 21.831241607988527, 'training/walltime': 1229.9056074619293, 'training/model_train_time': 16.428722381591797, 'training/other_time': 29.366607189178467, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(50000, dtype=int32), 'model/train_total_loss': Array(-25.693094, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.021509, dtype=float32), 'model/test_total_loss': Array(-23.58481, dtype=float32), 'model/test_mean_loss': Array(1.2149873, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 1.4116641759872437, 'sac/actor_loss': Array(-8.4492855, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.2536443, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(14.789486, dtype=float32), 'eval/episode_distance_from_origin': Array(619.7584, dtype=float32), 'eval/episode_forward_reward': Array(13.003908, dtype=float32), 'eval/episode_reward': Array(20.194828, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-178.67294, dtype=float32), 'eval/episode_reward_forward': Array(13.003908, dtype=float32), 'eval/episode_reward_survive': Array(182., dtype=float32), 'eval/episode_x_position': Array(233.8595, dtype=float32), 'eval/episode_x_velocity': Array(13.003908, dtype=float32), 'eval/episode_y_position': Array(-520.9349, dtype=float32), 'eval/episode_y_velocity': Array(-140.21901, dtype=float32), 'eval/avg_episode_length': Array(183., dtype=float32), 'eval/epoch_eval_time': 3.2840776443481445, 'eval/sps': 304.49949979745065, 'steps': Array(50000., dtype=float32)}
Model epoch 0: train total loss -24.63231658935547, train mean loss 1.0523607730865479, test mean loss [1.09897   1.0538349 1.1332278 1.0517074 1.0843439 1.2140696 1.1009647]
Model epoch 1: train total loss -25.401397705078125, train mean loss 1.074337363243103, test mean loss [1.0685276 1.1089939 1.2217343 1.1279327 1.1061755 1.161864  1.0923485]
Model epoch 2: train total loss -25.598791122436523, train mean loss 0.9763426780700684, test mean loss [1.0673221 1.1103859 1.1620148 1.1777657 1.0889223 1.1400121 1.1121553]
Model epoch 3: train total loss -26.399391174316406, train mean loss 1.0034732818603516, test mean loss [1.0733207 1.0910987 1.1507667 1.0978664 1.1308206 1.1702425 1.1029037]
Model epoch 4: train total loss -26.776456832885742, train mean loss 0.924171507358551, test mean loss [1.0879223 1.0813063 1.1333816 1.1070445 1.1086062 1.1963658 1.1010973]
Model epoch 5: train total loss -25.632343292236328, train mean loss 0.95306396484375, test mean loss [1.1576413 1.2007836 1.150612  1.118881  1.1902107 1.1783618 1.10578  ]
Model epoch 6: train total loss -26.040708541870117, train mean loss 0.975304126739502, test mean loss [1.1050682 1.2981354 1.1420087 1.1340362 1.1817361 1.2066602 1.2813162]
Model epoch 7: train total loss -25.333057403564453, train mean loss 1.1462763547897339, test mean loss [1.1389388 1.2296113 1.1440529 1.1653833 1.1516157 1.7732102 1.1507086]
Model epoch 8: train total loss -27.37510871887207, train mean loss 0.8841832876205444, test mean loss [1.109456  1.169268  1.1659703 1.1417075 1.1328547 1.2699314 1.173639 ]
Model trained in 9 epochs with 50000 transitions.
[2025-01-29 17:01:06,816][absl][INFO] - {'eval/walltime': 174.76376867294312, 'training/sps': 37.8789950457432, 'training/walltime': 1256.3054630756378, 'training/model_train_time': 15.723246097564697, 'training/other_time': 10.664389848709106, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(51000, dtype=int32), 'model/train_total_loss': Array(-27.375109, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8841833, dtype=float32), 'model/test_total_loss': Array(-24.142733, dtype=float32), 'model/test_mean_loss': Array(1.1661181, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.4820300738016765, 'sac/actor_loss': Array(-9.4723835, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.2169353, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(16.211452, dtype=float32), 'eval/episode_distance_from_origin': Array(7.1243925, dtype=float32), 'eval/episode_forward_reward': Array(-0.55550957, dtype=float32), 'eval/episode_reward': Array(-3.0147965, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-12.414078, dtype=float32), 'eval/episode_reward_forward': Array(-0.55550957, dtype=float32), 'eval/episode_reward_survive': Array(10., dtype=float32), 'eval/episode_x_position': Array(-0.7248948, dtype=float32), 'eval/episode_x_velocity': Array(-0.55550957, dtype=float32), 'eval/episode_y_position': Array(1.7511597, dtype=float32), 'eval/episode_y_velocity': Array(4.490697, dtype=float32), 'eval/avg_episode_length': Array(11., dtype=float32), 'eval/epoch_eval_time': 3.284332752227783, 'eval/sps': 304.4758480460586}
51000.0 {'eval/walltime': 174.76376867294312, 'training/sps': 37.8789950457432, 'training/walltime': 1256.3054630756378, 'training/model_train_time': 15.723246097564697, 'training/other_time': 10.664389848709106, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(51000, dtype=int32), 'model/train_total_loss': Array(-27.375109, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8841833, dtype=float32), 'model/test_total_loss': Array(-24.142733, dtype=float32), 'model/test_mean_loss': Array(1.1661181, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.4820300738016765, 'sac/actor_loss': Array(-9.4723835, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.2169353, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(16.211452, dtype=float32), 'eval/episode_distance_from_origin': Array(7.1243925, dtype=float32), 'eval/episode_forward_reward': Array(-0.55550957, dtype=float32), 'eval/episode_reward': Array(-3.0147965, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-12.414078, dtype=float32), 'eval/episode_reward_forward': Array(-0.55550957, dtype=float32), 'eval/episode_reward_survive': Array(10., dtype=float32), 'eval/episode_x_position': Array(-0.7248948, dtype=float32), 'eval/episode_x_velocity': Array(-0.55550957, dtype=float32), 'eval/episode_y_position': Array(1.7511597, dtype=float32), 'eval/episode_y_velocity': Array(4.490697, dtype=float32), 'eval/avg_episode_length': Array(11., dtype=float32), 'eval/epoch_eval_time': 3.284332752227783, 'eval/sps': 304.4758480460586, 'steps': Array(51000., dtype=float32)}
Model epoch 0: train total loss -25.793859481811523, train mean loss 1.0400875806808472, test mean loss [1.0185809 1.0545062 1.0190665 0.9896782 1.0138428 1.0489631 0.9836094]
Model epoch 1: train total loss -25.199989318847656, train mean loss 1.092609167098999, test mean loss [0.9972828 1.0184358 1.0352781 1.0980929 1.0612406 1.1399945 1.0073646]
Model epoch 2: train total loss -25.67322540283203, train mean loss 1.0136096477508545, test mean loss [1.024794   1.134057   1.0186107  1.024291   1.0329863  1.0675341
 0.97876465]
Model epoch 3: train total loss -25.829204559326172, train mean loss 1.0162687301635742, test mean loss [1.206326  1.064544  1.0242081 1.0713297 1.0408982 1.0761119 1.0002527]
Model epoch 4: train total loss -25.88218879699707, train mean loss 0.9699675440788269, test mean loss [1.1423353 1.0936723 1.0493319 1.0784329 1.0673497 1.0779275 1.002055 ]
Model epoch 5: train total loss -26.724679946899414, train mean loss 0.921038031578064, test mean loss [1.0817775 1.0590944 1.048777  1.0441396 1.0967544 1.0960249 1.0467905]
Model epoch 6: train total loss -26.322185516357422, train mean loss 0.9045678973197937, test mean loss [1.1426791 1.0506628 1.0722924 1.0517695 1.0533179 1.1430694 1.041505 ]
Model epoch 7: train total loss -25.635921478271484, train mean loss 1.0642756223678589, test mean loss [1.1473879 1.0698432 1.317771  1.1079042 1.1219826 1.1027889 1.0461301]
Model trained in 8 epochs with 51000 transitions.
[2025-01-29 17:01:34,810][absl][INFO] - {'eval/walltime': 178.05088686943054, 'training/sps': 40.492454115812244, 'training/walltime': 1281.001422405243, 'training/model_train_time': 14.03319525718689, 'training/other_time': 10.650647640228271, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(52000, dtype=int32), 'model/train_total_loss': Array(-25.635921, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0642756, dtype=float32), 'model/test_total_loss': Array(-24.60166, dtype=float32), 'model/test_mean_loss': Array(1.1305441, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 1.483540117740631, 'sac/actor_loss': Array(-12.405878, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.1511828, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(17.601948, dtype=float32), 'eval/episode_distance_from_origin': Array(4633.9, dtype=float32), 'eval/episode_forward_reward': Array(189.47363, dtype=float32), 'eval/episode_reward': Array(232.56992, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-440.42178, dtype=float32), 'eval/episode_reward_forward': Array(189.47363, dtype=float32), 'eval/episode_reward_survive': Array(484., dtype=float32), 'eval/episode_x_position': Array(1966.1405, dtype=float32), 'eval/episode_x_velocity': Array(189.47363, dtype=float32), 'eval/episode_y_position': Array(-4158.4575, dtype=float32), 'eval/episode_y_velocity': Array(-316.16077, dtype=float32), 'eval/avg_episode_length': Array(485., dtype=float32), 'eval/epoch_eval_time': 3.2871181964874268, 'eval/sps': 304.21784074226093}
52000.0 {'eval/walltime': 178.05088686943054, 'training/sps': 40.492454115812244, 'training/walltime': 1281.001422405243, 'training/model_train_time': 14.03319525718689, 'training/other_time': 10.650647640228271, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(52000, dtype=int32), 'model/train_total_loss': Array(-25.635921, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(1.0642756, dtype=float32), 'model/test_total_loss': Array(-24.60166, dtype=float32), 'model/test_mean_loss': Array(1.1305441, dtype=float32), 'model/train_epochs': 8, 'model/sec_per_epoch': 1.483540117740631, 'sac/actor_loss': Array(-12.405878, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.1511828, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(17.601948, dtype=float32), 'eval/episode_distance_from_origin': Array(4633.9, dtype=float32), 'eval/episode_forward_reward': Array(189.47363, dtype=float32), 'eval/episode_reward': Array(232.56992, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-440.42178, dtype=float32), 'eval/episode_reward_forward': Array(189.47363, dtype=float32), 'eval/episode_reward_survive': Array(484., dtype=float32), 'eval/episode_x_position': Array(1966.1405, dtype=float32), 'eval/episode_x_velocity': Array(189.47363, dtype=float32), 'eval/episode_y_position': Array(-4158.4575, dtype=float32), 'eval/episode_y_velocity': Array(-316.16077, dtype=float32), 'eval/avg_episode_length': Array(485., dtype=float32), 'eval/epoch_eval_time': 3.2871181964874268, 'eval/sps': 304.21784074226093, 'steps': Array(52000., dtype=float32)}
Model epoch 0: train total loss -24.630756378173828, train mean loss 1.0837821960449219, test mean loss [1.0419639 1.0548315 1.1564171 1.2160941 1.0101355 1.089111  1.1048216]
Model epoch 1: train total loss -25.86562728881836, train mean loss 0.9798053503036499, test mean loss [1.1899724 1.0842968 1.0769821 1.1096585 1.0413204 1.3872923 1.0486912]
Model epoch 2: train total loss -25.071592330932617, train mean loss 1.0744109153747559, test mean loss [1.1335412 1.1765993 1.1074096 1.0944605 1.1353279 1.1847051 1.1609799]
Model epoch 3: train total loss -26.40362548828125, train mean loss 0.9752367734909058, test mean loss [1.1237478 1.0851201 1.0850726 1.1291747 1.1014755 1.1215199 1.1658156]
Model epoch 4: train total loss -26.397621154785156, train mean loss 0.9642003178596497, test mean loss [1.1407032 1.0797364 1.084374  1.1048912 1.0541441 1.148749  1.2936939]
Model epoch 5: train total loss -26.975120544433594, train mean loss 0.9202349185943604, test mean loss [1.0907667 1.0978348 1.098226  1.125446  1.1042898 1.1167711 1.14769  ]
Model epoch 6: train total loss -27.642807006835938, train mean loss 0.8791916370391846, test mean loss [1.0858991 1.0920682 1.1390893 1.1426729 1.0752946 1.1282859 1.1138467]
Model epoch 7: train total loss -26.36170768737793, train mean loss 0.9544184803962708, test mean loss [1.093656  1.1079066 1.1191323 1.1770507 1.237481  1.1917169 1.1180847]
Model epoch 8: train total loss -27.60814666748047, train mean loss 0.8896595239639282, test mean loss [1.0987953 1.111022  1.1169486 1.1678323 1.1668429 1.1518778 1.1220531]
Model trained in 9 epochs with 52000 transitions.
[2025-01-29 17:02:04,863][absl][INFO] - {'eval/walltime': 181.33859825134277, 'training/sps': 37.37577154789637, 'training/walltime': 1307.7567229270935, 'training/model_train_time': 16.087090730667114, 'training/other_time': 10.65561580657959, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(53000, dtype=int32), 'model/train_total_loss': Array(-27.608147, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8896595, dtype=float32), 'model/test_total_loss': Array(-24.86838, dtype=float32), 'model/test_mean_loss': Array(1.1336247, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.5300317605336506, 'sac/actor_loss': Array(-10.996413, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.1538064, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(20.519968, dtype=float32), 'eval/episode_distance_from_origin': Array(281.15546, dtype=float32), 'eval/episode_forward_reward': Array(40.998245, dtype=float32), 'eval/episode_reward': Array(43.103558, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-141.53679, dtype=float32), 'eval/episode_reward_forward': Array(40.998245, dtype=float32), 'eval/episode_reward_survive': Array(144., dtype=float32), 'eval/episode_x_position': Array(239.27054, dtype=float32), 'eval/episode_x_velocity': Array(40.998245, dtype=float32), 'eval/episode_y_position': Array(-19.182756, dtype=float32), 'eval/episode_y_velocity': Array(35.193512, dtype=float32), 'eval/avg_episode_length': Array(145., dtype=float32), 'eval/epoch_eval_time': 3.2877113819122314, 'eval/sps': 304.16295222920996}
53000.0 {'eval/walltime': 181.33859825134277, 'training/sps': 37.37577154789637, 'training/walltime': 1307.7567229270935, 'training/model_train_time': 16.087090730667114, 'training/other_time': 10.65561580657959, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(53000, dtype=int32), 'model/train_total_loss': Array(-27.608147, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8896595, dtype=float32), 'model/test_total_loss': Array(-24.86838, dtype=float32), 'model/test_mean_loss': Array(1.1336247, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.5300317605336506, 'sac/actor_loss': Array(-10.996413, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.1538064, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(20.519968, dtype=float32), 'eval/episode_distance_from_origin': Array(281.15546, dtype=float32), 'eval/episode_forward_reward': Array(40.998245, dtype=float32), 'eval/episode_reward': Array(43.103558, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-141.53679, dtype=float32), 'eval/episode_reward_forward': Array(40.998245, dtype=float32), 'eval/episode_reward_survive': Array(144., dtype=float32), 'eval/episode_x_position': Array(239.27054, dtype=float32), 'eval/episode_x_velocity': Array(40.998245, dtype=float32), 'eval/episode_y_position': Array(-19.182756, dtype=float32), 'eval/episode_y_velocity': Array(35.193512, dtype=float32), 'eval/avg_episode_length': Array(145., dtype=float32), 'eval/epoch_eval_time': 3.2877113819122314, 'eval/sps': 304.16295222920996, 'steps': Array(53000., dtype=float32)}
Model epoch 0: train total loss -26.21108055114746, train mean loss 1.0059337615966797, test mean loss [1.059772  1.124921  1.0385619 1.0144049 1.094157  1.1147226 1.6700803]
Model epoch 1: train total loss -26.013671875, train mean loss 0.9706189036369324, test mean loss [1.0611228 1.0752352 1.1426947 1.3070912 1.0882407 1.0983107 1.1451485]
Model epoch 2: train total loss -26.37099838256836, train mean loss 0.8699833154678345, test mean loss [1.0689929 1.0942135 1.0416701 1.0984931 1.0801328 1.0848567 1.0759143]
Model epoch 3: train total loss -26.467914581298828, train mean loss 0.886811375617981, test mean loss [1.0780253 1.0495839 1.110344  1.1245064 1.1628454 1.096118  1.057529 ]
Model epoch 4: train total loss -27.219650268554688, train mean loss 0.9001857042312622, test mean loss [1.0801868 1.099441  1.074375  1.0674297 1.1382    1.0549804 1.0814407]
Model epoch 5: train total loss -27.053762435913086, train mean loss 0.8936258554458618, test mean loss [1.0821229 1.0330961 1.0996743 1.0988108 1.1005416 1.0420578 1.1021125]
Model epoch 6: train total loss -27.55414581298828, train mean loss 0.8866051435470581, test mean loss [1.1196572 1.0932262 1.1099042 1.0569949 1.124668  1.0479249 1.0606484]
Model epoch 7: train total loss -27.35599136352539, train mean loss 0.913082480430603, test mean loss [1.1268674 1.0835168 1.0932792 1.0710449 1.251178  1.0832158 1.1391116]
Model epoch 8: train total loss -27.42803192138672, train mean loss 0.9146325588226318, test mean loss [1.1763552 1.157428  1.0942752 1.183064  1.2029716 1.033648  1.114056 ]
Model epoch 9: train total loss -26.540557861328125, train mean loss 0.9268045425415039, test mean loss [1.3484333 1.0603685 1.1116667 1.0463915 1.1672558 1.0782769 1.1720343]
Model epoch 10: train total loss -27.135900497436523, train mean loss 0.865878164768219, test mean loss [1.1824636 1.0882812 1.1264155 1.061388  1.1998252 1.1239536 1.118936 ]
Model epoch 11: train total loss -27.263019561767578, train mean loss 0.9241219758987427, test mean loss [1.1591749 1.2777278 1.1198618 1.1081849 1.2204051 1.1197274 1.1414081]
Model trained in 12 epochs with 53000 transitions.
[2025-01-29 17:02:38,809][absl][INFO] - {'eval/walltime': 184.62340450286865, 'training/sps': 32.624675721471114, 'training/walltime': 1338.408368587494, 'training/model_train_time': 19.98490309715271, 'training/other_time': 10.654537916183472, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(54000, dtype=int32), 'model/train_total_loss': Array(-27.26302, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.924122, dtype=float32), 'model/test_total_loss': Array(-24.405338, dtype=float32), 'model/test_mean_loss': Array(1.1637844, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.4716932773590088, 'sac/actor_loss': Array(-11.932056, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.1454334, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(18.447088, dtype=float32), 'eval/episode_distance_from_origin': Array(1501.9081, dtype=float32), 'eval/episode_forward_reward': Array(196.88284, dtype=float32), 'eval/episode_reward': Array(203.50378, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-327.19495, dtype=float32), 'eval/episode_reward_forward': Array(196.88284, dtype=float32), 'eval/episode_reward_survive': Array(334., dtype=float32), 'eval/episode_x_position': Array(1389.5181, dtype=float32), 'eval/episode_x_velocity': Array(196.88284, dtype=float32), 'eval/episode_y_position': Array(-477.74802, dtype=float32), 'eval/episode_y_velocity': Array(-86.27895, dtype=float32), 'eval/avg_episode_length': Array(335., dtype=float32), 'eval/epoch_eval_time': 3.284806251525879, 'eval/sps': 304.4319583645074}
54000.0 {'eval/walltime': 184.62340450286865, 'training/sps': 32.624675721471114, 'training/walltime': 1338.408368587494, 'training/model_train_time': 19.98490309715271, 'training/other_time': 10.654537916183472, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(54000, dtype=int32), 'model/train_total_loss': Array(-27.26302, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.924122, dtype=float32), 'model/test_total_loss': Array(-24.405338, dtype=float32), 'model/test_mean_loss': Array(1.1637844, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.4716932773590088, 'sac/actor_loss': Array(-11.932056, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.1454334, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(18.447088, dtype=float32), 'eval/episode_distance_from_origin': Array(1501.9081, dtype=float32), 'eval/episode_forward_reward': Array(196.88284, dtype=float32), 'eval/episode_reward': Array(203.50378, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-327.19495, dtype=float32), 'eval/episode_reward_forward': Array(196.88284, dtype=float32), 'eval/episode_reward_survive': Array(334., dtype=float32), 'eval/episode_x_position': Array(1389.5181, dtype=float32), 'eval/episode_x_velocity': Array(196.88284, dtype=float32), 'eval/episode_y_position': Array(-477.74802, dtype=float32), 'eval/episode_y_velocity': Array(-86.27895, dtype=float32), 'eval/avg_episode_length': Array(335., dtype=float32), 'eval/epoch_eval_time': 3.284806251525879, 'eval/sps': 304.4319583645074, 'steps': Array(54000., dtype=float32)}
Model epoch 0: train total loss -24.787446975708008, train mean loss 1.0362491607666016, test mean loss [1.0486511 1.2531489 1.0117748 1.0319943 1.1738539 1.0732405 1.0994483]
Model epoch 1: train total loss -26.087343215942383, train mean loss 1.0026510953903198, test mean loss [1.0475746 1.0665123 1.0369202 1.0829927 1.0639982 1.0885864 1.0860168]
Model epoch 2: train total loss -25.941986083984375, train mean loss 0.9997451305389404, test mean loss [1.0890987 1.043374  1.0286881 1.0636849 1.1937842 1.0824084 1.0665362]
Model epoch 3: train total loss -26.139612197875977, train mean loss 0.9133701920509338, test mean loss [1.0762657 1.0230534 1.1427739 1.1895938 1.0936036 1.285759  1.0523003]
Model epoch 4: train total loss -27.261978149414062, train mean loss 0.8748687505722046, test mean loss [1.0335876 1.0542333 1.2809285 1.0685933 1.1638879 1.2633721 1.0946857]
Model epoch 5: train total loss -26.81770896911621, train mean loss 0.9213515520095825, test mean loss [1.0754167 1.0868721 1.1075094 1.1046939 1.1205078 1.1774403 1.0446503]
Model epoch 6: train total loss -26.107961654663086, train mean loss 0.9623970985412598, test mean loss [1.0875564 1.0784719 1.1127446 1.1548584 1.0810808 1.190283  1.0760113]
Model epoch 7: train total loss -27.433393478393555, train mean loss 0.8534458875656128, test mean loss [1.1023004 1.08459   1.1003481 1.1138722 1.0963299 1.1643658 1.0721996]
Model epoch 8: train total loss -26.79439926147461, train mean loss 0.9070035219192505, test mean loss [1.0919293 1.160835  1.092053  1.1825641 1.1190151 1.1555004 1.0456883]
Model epoch 9: train total loss -27.108055114746094, train mean loss 0.9316469430923462, test mean loss [1.0755372 1.1556159 1.0930465 1.1253891 1.0874306 1.171104  1.1089181]
Model epoch 10: train total loss -27.05672836303711, train mean loss 0.9083935022354126, test mean loss [1.0816946 1.1496212 1.1187491 1.1419593 1.0858185 1.1522814 1.1293368]
Model trained in 11 epochs with 54000 transitions.
[2025-01-29 17:03:12,020][absl][INFO] - {'eval/walltime': 187.90549540519714, 'training/sps': 33.423393580065735, 'training/walltime': 1368.3275327682495, 'training/model_train_time': 19.25937032699585, 'training/other_time': 10.647529602050781, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(55000, dtype=int32), 'model/train_total_loss': Array(-27.056728, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.9083935, dtype=float32), 'model/test_total_loss': Array(-24.964779, dtype=float32), 'model/test_mean_loss': Array(1.1227802, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.5299706675789573, 'sac/actor_loss': Array(-14.446977, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.0993011, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(20.042303, dtype=float32), 'eval/episode_distance_from_origin': Array(700.7542, dtype=float32), 'eval/episode_forward_reward': Array(140.81125, dtype=float32), 'eval/episode_reward': Array(155.3183, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-194.566, dtype=float32), 'eval/episode_reward_forward': Array(140.81125, dtype=float32), 'eval/episode_reward_survive': Array(208., dtype=float32), 'eval/episode_x_position': Array(644.13104, dtype=float32), 'eval/episode_x_velocity': Array(140.81125, dtype=float32), 'eval/episode_y_position': Array(-224.50787, dtype=float32), 'eval/episode_y_velocity': Array(-45.906105, dtype=float32), 'eval/avg_episode_length': Array(209., dtype=float32), 'eval/epoch_eval_time': 3.282090902328491, 'eval/sps': 304.68382191685987}
55000.0 {'eval/walltime': 187.90549540519714, 'training/sps': 33.423393580065735, 'training/walltime': 1368.3275327682495, 'training/model_train_time': 19.25937032699585, 'training/other_time': 10.647529602050781, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(55000, dtype=int32), 'model/train_total_loss': Array(-27.056728, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.9083935, dtype=float32), 'model/test_total_loss': Array(-24.964779, dtype=float32), 'model/test_mean_loss': Array(1.1227802, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.5299706675789573, 'sac/actor_loss': Array(-14.446977, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.0993011, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(20.042303, dtype=float32), 'eval/episode_distance_from_origin': Array(700.7542, dtype=float32), 'eval/episode_forward_reward': Array(140.81125, dtype=float32), 'eval/episode_reward': Array(155.3183, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-194.566, dtype=float32), 'eval/episode_reward_forward': Array(140.81125, dtype=float32), 'eval/episode_reward_survive': Array(208., dtype=float32), 'eval/episode_x_position': Array(644.13104, dtype=float32), 'eval/episode_x_velocity': Array(140.81125, dtype=float32), 'eval/episode_y_position': Array(-224.50787, dtype=float32), 'eval/episode_y_velocity': Array(-45.906105, dtype=float32), 'eval/avg_episode_length': Array(209., dtype=float32), 'eval/epoch_eval_time': 3.282090902328491, 'eval/sps': 304.68382191685987, 'steps': Array(55000., dtype=float32)}
Model epoch 0: train total loss -26.089740753173828, train mean loss 0.9466093182563782, test mean loss [1.0470915  0.97359514 0.93741083 1.0946295  0.99909943 0.9927722
 1.0408946 ]
Model epoch 1: train total loss -25.11143684387207, train mean loss 1.0266790390014648, test mean loss [1.0358791 1.3840607 1.0291727 1.0824258 1.0071065 1.0273263 1.0885315]
Model epoch 2: train total loss -25.491594314575195, train mean loss 1.0606015920639038, test mean loss [1.0378239 1.1843028 1.0201517 1.056921  1.0035425 1.0334811 1.0118481]
Model epoch 3: train total loss -26.190702438354492, train mean loss 1.010082483291626, test mean loss [0.9693206  1.1475292  0.98386705 1.0201966  0.9943523  1.0248382
 0.98335   ]
Model epoch 4: train total loss -26.020793914794922, train mean loss 0.9872218370437622, test mean loss [1.0228655  1.262635   0.9885696  1.0115488  0.97734106 1.1081434
 1.0513906 ]
Model epoch 5: train total loss -27.001483917236328, train mean loss 0.9808211326599121, test mean loss [1.0152597 1.0648364 1.0181395 1.0290416 1.0782733 1.0452492 1.1053576]
Model epoch 6: train total loss -27.434894561767578, train mean loss 0.9302809238433838, test mean loss [0.9812532 1.0837445 1.0252901 1.016681  1.001771  1.170431  1.0369594]
Model epoch 7: train total loss -26.976303100585938, train mean loss 0.9733693599700928, test mean loss [0.99142   1.0587336 1.0243231 1.0329326 1.0154786 1.0742612 1.0368391]
Model epoch 8: train total loss -27.776872634887695, train mean loss 0.8775611519813538, test mean loss [1.0288105 1.0425787 1.0924327 1.0945944 1.0053587 1.0580801 1.0148441]
Model epoch 9: train total loss -27.45441246032715, train mean loss 0.8982426524162292, test mean loss [1.0005282 1.1001747 1.0615897 1.0440428 1.0355368 1.0987537 1.0172282]
Model epoch 10: train total loss -26.349567413330078, train mean loss 0.9680391550064087, test mean loss [1.1559572 1.0642204 1.0627749 1.0767212 1.2415564 1.0701128 1.0393449]
Model trained in 11 epochs with 55000 transitions.
[2025-01-29 17:03:45,261][absl][INFO] - {'eval/walltime': 191.20454263687134, 'training/sps': 33.40861701743464, 'training/walltime': 1398.2599301338196, 'training/model_train_time': 19.251688241958618, 'training/other_time': 10.668731212615967, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(56000, dtype=int32), 'model/train_total_loss': Array(-26.349567, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.96803916, dtype=float32), 'model/test_total_loss': Array(-24.851257, dtype=float32), 'model/test_mean_loss': Array(1.1015267, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.5440179651433772, 'sac/actor_loss': Array(-15.485567, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.0665696, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(21.004969, dtype=float32), 'eval/episode_distance_from_origin': Array(1961.5695, dtype=float32), 'eval/episode_forward_reward': Array(264.19885, dtype=float32), 'eval/episode_reward': Array(302.78384, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-220.11566, dtype=float32), 'eval/episode_reward_forward': Array(264.19885, dtype=float32), 'eval/episode_reward_survive': Array(257., dtype=float32), 'eval/episode_x_position': Array(1836.9253, dtype=float32), 'eval/episode_x_velocity': Array(264.19885, dtype=float32), 'eval/episode_y_position': Array(-631.8295, dtype=float32), 'eval/episode_y_velocity': Array(-143.7363, dtype=float32), 'eval/avg_episode_length': Array(258., dtype=float32), 'eval/epoch_eval_time': 3.2990472316741943, 'eval/sps': 303.1178185019564}
56000.0 {'eval/walltime': 191.20454263687134, 'training/sps': 33.40861701743464, 'training/walltime': 1398.2599301338196, 'training/model_train_time': 19.251688241958618, 'training/other_time': 10.668731212615967, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(56000, dtype=int32), 'model/train_total_loss': Array(-26.349567, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.96803916, dtype=float32), 'model/test_total_loss': Array(-24.851257, dtype=float32), 'model/test_mean_loss': Array(1.1015267, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.5440179651433772, 'sac/actor_loss': Array(-15.485567, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.0665696, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(21.004969, dtype=float32), 'eval/episode_distance_from_origin': Array(1961.5695, dtype=float32), 'eval/episode_forward_reward': Array(264.19885, dtype=float32), 'eval/episode_reward': Array(302.78384, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-220.11566, dtype=float32), 'eval/episode_reward_forward': Array(264.19885, dtype=float32), 'eval/episode_reward_survive': Array(257., dtype=float32), 'eval/episode_x_position': Array(1836.9253, dtype=float32), 'eval/episode_x_velocity': Array(264.19885, dtype=float32), 'eval/episode_y_position': Array(-631.8295, dtype=float32), 'eval/episode_y_velocity': Array(-143.7363, dtype=float32), 'eval/avg_episode_length': Array(258., dtype=float32), 'eval/epoch_eval_time': 3.2990472316741943, 'eval/sps': 303.1178185019564, 'steps': Array(56000., dtype=float32)}
Model epoch 0: train total loss -26.460344314575195, train mean loss 0.9706775546073914, test mean loss [1.163681  1.0853186 0.9960103 1.152478  1.0627447 1.0466627 1.02571  ]
Model epoch 1: train total loss -26.933162689208984, train mean loss 0.9115365743637085, test mean loss [1.1265408 1.1150578 1.0256913 1.0320125 1.0535424 1.047249  0.9964473]
Model epoch 2: train total loss -27.693185806274414, train mean loss 0.8786618709564209, test mean loss [1.0586259 1.0350906 1.0153857 1.0269533 1.0367919 1.0718037 1.0228401]
Model epoch 3: train total loss -26.740097045898438, train mean loss 0.8801376223564148, test mean loss [1.0357869 1.1182485 1.019985  1.080866  1.0189987 1.120026  1.3993227]
Model epoch 4: train total loss -25.854230880737305, train mean loss 0.995019257068634, test mean loss [1.1129223 1.0736691 1.0403706 1.0352241 1.4621131 1.075083  1.1296647]
Model epoch 5: train total loss -27.070171356201172, train mean loss 0.8950302600860596, test mean loss [1.0786432 1.0329516 1.116086  1.0503294 1.1573211 1.0825586 1.1171734]
Model epoch 6: train total loss -27.362253189086914, train mean loss 0.881915807723999, test mean loss [1.0877984 1.0656846 1.1887287 1.1146004 1.11715   1.1135975 1.09101  ]
Model epoch 7: train total loss -27.51004409790039, train mean loss 0.9070792198181152, test mean loss [1.039285  1.0485325 1.136178  1.1625451 1.1192318 1.3523585 1.0922295]
Model epoch 8: train total loss -28.164470672607422, train mean loss 0.8547896146774292, test mean loss [1.0368345 1.063362  1.0937767 1.1456367 1.0888554 1.1041423 1.0650306]
Model epoch 9: train total loss -26.957000732421875, train mean loss 0.8981638550758362, test mean loss [1.13865   1.050767  1.1097784 1.2369751 1.116434  1.1352227 1.0879719]
Model trained in 10 epochs with 56000 transitions.
[2025-01-29 17:04:16,887][absl][INFO] - {'eval/walltime': 194.48860430717468, 'training/sps': 35.29400459246567, 'training/walltime': 1426.5933542251587, 'training/model_train_time': 17.657975435256958, 'training/other_time': 10.66319227218628, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(57000, dtype=int32), 'model/train_total_loss': Array(-26.957, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.89816386, dtype=float32), 'model/test_total_loss': Array(-25.04605, dtype=float32), 'model/test_mean_loss': Array(1.1251142, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 1.5534077405929565, 'sac/actor_loss': Array(-17.480453, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.0286889, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(23.136694, dtype=float32), 'eval/episode_distance_from_origin': Array(107.08521, dtype=float32), 'eval/episode_forward_reward': Array(56.173054, dtype=float32), 'eval/episode_reward': Array(58.558666, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-66.21546, dtype=float32), 'eval/episode_reward_forward': Array(56.173054, dtype=float32), 'eval/episode_reward_survive': Array(68., dtype=float32), 'eval/episode_x_position': Array(89.47974, dtype=float32), 'eval/episode_x_velocity': Array(56.173054, dtype=float32), 'eval/episode_y_position': Array(17.660402, dtype=float32), 'eval/episode_y_velocity': Array(-2.2680743, dtype=float32), 'eval/avg_episode_length': Array(69., dtype=float32), 'eval/epoch_eval_time': 3.2840616703033447, 'eval/sps': 304.5009809172162}
57000.0 {'eval/walltime': 194.48860430717468, 'training/sps': 35.29400459246567, 'training/walltime': 1426.5933542251587, 'training/model_train_time': 17.657975435256958, 'training/other_time': 10.66319227218628, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(57000, dtype=int32), 'model/train_total_loss': Array(-26.957, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.89816386, dtype=float32), 'model/test_total_loss': Array(-25.04605, dtype=float32), 'model/test_mean_loss': Array(1.1251142, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 1.5534077405929565, 'sac/actor_loss': Array(-17.480453, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(1.0286889, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(23.136694, dtype=float32), 'eval/episode_distance_from_origin': Array(107.08521, dtype=float32), 'eval/episode_forward_reward': Array(56.173054, dtype=float32), 'eval/episode_reward': Array(58.558666, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-66.21546, dtype=float32), 'eval/episode_reward_forward': Array(56.173054, dtype=float32), 'eval/episode_reward_survive': Array(68., dtype=float32), 'eval/episode_x_position': Array(89.47974, dtype=float32), 'eval/episode_x_velocity': Array(56.173054, dtype=float32), 'eval/episode_y_position': Array(17.660402, dtype=float32), 'eval/episode_y_velocity': Array(-2.2680743, dtype=float32), 'eval/avg_episode_length': Array(69., dtype=float32), 'eval/epoch_eval_time': 3.2840616703033447, 'eval/sps': 304.5009809172162, 'steps': Array(57000., dtype=float32)}
Model epoch 0: train total loss -26.307859420776367, train mean loss 0.9603645205497742, test mean loss [1.0007045  0.9561293  0.945473   1.0160186  1.0337328  1.0724825
 0.98394305]
Model epoch 1: train total loss -27.064592361450195, train mean loss 0.9045087099075317, test mean loss [0.97040766 0.936938   0.9510325  1.0070384  0.9958275  1.043905
 0.9827302 ]
Model epoch 2: train total loss -28.50701332092285, train mean loss 0.8454282879829407, test mean loss [0.980403   1.1388355  0.97885615 1.0028839  0.9675061  1.0064465
 0.9595736 ]
Model epoch 3: train total loss -27.541236877441406, train mean loss 0.8682706356048584, test mean loss [1.0032076  1.0299195  0.98493564 0.9877962  1.0145493  1.009441
 0.9873534 ]
Model epoch 4: train total loss -27.650938034057617, train mean loss 0.8716108798980713, test mean loss [0.98540634 0.97264534 0.9897506  1.0011429  1.0176429  1.1915355
 0.9836098 ]
Model epoch 5: train total loss -26.966678619384766, train mean loss 0.9884492754936218, test mean loss [1.0280377 1.1452726 0.9914897 1.0479224 0.99559   1.0882574 1.0646924]
Model epoch 6: train total loss -28.238183975219727, train mean loss 0.8768805265426636, test mean loss [1.0465598 1.0223492 0.9746394 1.0236427 1.0340351 1.1191739 1.0055201]
Model epoch 7: train total loss -26.914840698242188, train mean loss 0.9673099517822266, test mean loss [1.0679389 1.0494503 1.0203507 1.0343986 1.0637186 1.1083794 1.0117949]
Model epoch 8: train total loss -26.171825408935547, train mean loss 0.9935365915298462, test mean loss [1.038092  1.481314  1.0741504 1.1639234 1.037114  1.2420347 1.0455192]
Model epoch 9: train total loss -26.390764236450195, train mean loss 0.8893393278121948, test mean loss [1.2213213 1.2405676 1.0656811 1.1256952 1.0823748 1.2313247 1.0224459]
Model trained in 10 epochs with 57000 transitions.
[2025-01-29 17:04:49,152][absl][INFO] - {'eval/walltime': 197.78241324424744, 'training/sps': 34.52910436521538, 'training/walltime': 1455.554429769516, 'training/model_train_time': 18.264551639556885, 'training/other_time': 10.684226274490356, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(58000, dtype=int32), 'model/train_total_loss': Array(-26.390764, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8893393, dtype=float32), 'model/test_total_loss': Array(-24.277529, dtype=float32), 'model/test_mean_loss': Array(1.1413444, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 1.6061277389526367, 'sac/actor_loss': Array(-22.58987, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.9900984, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(24.14322, dtype=float32), 'eval/episode_distance_from_origin': Array(4366.679, dtype=float32), 'eval/episode_forward_reward': Array(415.47925, dtype=float32), 'eval/episode_reward': Array(444.66492, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-381.71402, dtype=float32), 'eval/episode_reward_forward': Array(415.47925, dtype=float32), 'eval/episode_reward_survive': Array(411., dtype=float32), 'eval/episode_x_position': Array(4267.1353, dtype=float32), 'eval/episode_x_velocity': Array(415.47925, dtype=float32), 'eval/episode_y_position': Array(551.5797, dtype=float32), 'eval/episode_y_velocity': Array(45.830063, dtype=float32), 'eval/avg_episode_length': Array(412., dtype=float32), 'eval/epoch_eval_time': 3.293808937072754, 'eval/sps': 303.5998805955975}
58000.0 {'eval/walltime': 197.78241324424744, 'training/sps': 34.52910436521538, 'training/walltime': 1455.554429769516, 'training/model_train_time': 18.264551639556885, 'training/other_time': 10.684226274490356, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(58000, dtype=int32), 'model/train_total_loss': Array(-26.390764, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8893393, dtype=float32), 'model/test_total_loss': Array(-24.277529, dtype=float32), 'model/test_mean_loss': Array(1.1413444, dtype=float32), 'model/train_epochs': 10, 'model/sec_per_epoch': 1.6061277389526367, 'sac/actor_loss': Array(-22.58987, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.9900984, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(24.14322, dtype=float32), 'eval/episode_distance_from_origin': Array(4366.679, dtype=float32), 'eval/episode_forward_reward': Array(415.47925, dtype=float32), 'eval/episode_reward': Array(444.66492, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-381.71402, dtype=float32), 'eval/episode_reward_forward': Array(415.47925, dtype=float32), 'eval/episode_reward_survive': Array(411., dtype=float32), 'eval/episode_x_position': Array(4267.1353, dtype=float32), 'eval/episode_x_velocity': Array(415.47925, dtype=float32), 'eval/episode_y_position': Array(551.5797, dtype=float32), 'eval/episode_y_velocity': Array(45.830063, dtype=float32), 'eval/avg_episode_length': Array(412., dtype=float32), 'eval/epoch_eval_time': 3.293808937072754, 'eval/sps': 303.5998805955975, 'steps': Array(58000., dtype=float32)}
Model epoch 0: train total loss -26.056825637817383, train mean loss 0.9244238138198853, test mean loss [1.0839612  1.0538608  0.9087914  1.2499347  0.93502533 1.0248405
 0.92432266]
Model epoch 1: train total loss -26.613561630249023, train mean loss 0.9728830456733704, test mean loss [0.9636997  1.008327   0.98662096 1.000286   0.9958489  1.1153135
 1.0095972 ]
Model epoch 2: train total loss -26.12180519104004, train mean loss 0.9900579452514648, test mean loss [0.9539224  0.97364765 0.9496797  0.9536662  0.9348295  1.0221987
 1.4542253 ]
Model epoch 3: train total loss -25.862424850463867, train mean loss 1.0726186037063599, test mean loss [1.1571902  1.108028   0.9500253  1.2060299  0.97628087 1.0366377
 1.2337067 ]
Model epoch 4: train total loss -27.22696304321289, train mean loss 0.9027044177055359, test mean loss [1.1935625  1.0332748  0.98703825 1.0566344  0.9530417  1.0279499
 1.1074389 ]
Model epoch 5: train total loss -27.75374984741211, train mean loss 0.8925540447235107, test mean loss [1.1030679  1.0470141  0.96002275 1.019028   0.97526443 1.0449474
 1.09594   ]
Model epoch 6: train total loss -27.67125701904297, train mean loss 0.9035742282867432, test mean loss [1.0677768 1.0328643 0.9657353 1.0275089 1.0155432 1.0439237 1.0301545]
Model epoch 7: train total loss -27.612777709960938, train mean loss 0.8624642491340637, test mean loss [1.0645895 1.0367821 1.0160575 1.0071723 1.0815549 1.0702106 1.0182204]
Model epoch 8: train total loss -28.092134475708008, train mean loss 0.8546909093856812, test mean loss [1.0453547 1.0188186 0.9848999 1.0191636 1.0221677 1.0472924 1.0306983]
Model trained in 9 epochs with 58000 transitions.
[2025-01-29 17:05:20,767][absl][INFO] - {'eval/walltime': 201.05553221702576, 'training/sps': 35.295162297845074, 'training/walltime': 1483.8869245052338, 'training/model_train_time': 17.66033697128296, 'training/other_time': 10.659777879714966, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(59000, dtype=int32), 'model/train_total_loss': Array(-28.092134, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8546909, dtype=float32), 'model/test_total_loss': Array(-26.006863, dtype=float32), 'model/test_mean_loss': Array(1.0240564, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.699984868367513, 'sac/actor_loss': Array(-25.160202, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.9481904, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(25.266754, dtype=float32), 'eval/episode_distance_from_origin': Array(13129.673, dtype=float32), 'eval/episode_forward_reward': Array(808.59265, dtype=float32), 'eval/episode_reward': Array(847.8512, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-560.8917, dtype=float32), 'eval/episode_reward_forward': Array(808.59265, dtype=float32), 'eval/episode_reward_survive': Array(600., dtype=float32), 'eval/episode_x_position': Array(11285.159, dtype=float32), 'eval/episode_x_velocity': Array(808.59265, dtype=float32), 'eval/episode_y_position': Array(-6670.763, dtype=float32), 'eval/episode_y_velocity': Array(-432.1889, dtype=float32), 'eval/avg_episode_length': Array(601., dtype=float32), 'eval/epoch_eval_time': 3.2731189727783203, 'eval/sps': 305.5189891711056}
59000.0 {'eval/walltime': 201.05553221702576, 'training/sps': 35.295162297845074, 'training/walltime': 1483.8869245052338, 'training/model_train_time': 17.66033697128296, 'training/other_time': 10.659777879714966, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(59000, dtype=int32), 'model/train_total_loss': Array(-28.092134, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8546909, dtype=float32), 'model/test_total_loss': Array(-26.006863, dtype=float32), 'model/test_mean_loss': Array(1.0240564, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.699984868367513, 'sac/actor_loss': Array(-25.160202, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.9481904, dtype=float32), 'sac/buffer_current_size': Array(801799.06, dtype=float32), 'sac/critic_loss': Array(25.266754, dtype=float32), 'eval/episode_distance_from_origin': Array(13129.673, dtype=float32), 'eval/episode_forward_reward': Array(808.59265, dtype=float32), 'eval/episode_reward': Array(847.8512, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-560.8917, dtype=float32), 'eval/episode_reward_forward': Array(808.59265, dtype=float32), 'eval/episode_reward_survive': Array(600., dtype=float32), 'eval/episode_x_position': Array(11285.159, dtype=float32), 'eval/episode_x_velocity': Array(808.59265, dtype=float32), 'eval/episode_y_position': Array(-6670.763, dtype=float32), 'eval/episode_y_velocity': Array(-432.1889, dtype=float32), 'eval/avg_episode_length': Array(601., dtype=float32), 'eval/epoch_eval_time': 3.2731189727783203, 'eval/sps': 305.5189891711056, 'steps': Array(59000., dtype=float32)}
Model horizon updated to 5.
Hallucination updates per training step updated to 1000.
SAC buffer resized to 2000000 samples.
Model epoch 0: train total loss -26.673431396484375, train mean loss 0.9381595849990845, test mean loss [0.9641236  0.9036731  0.9192129  1.0424286  0.88660794 0.993939
 1.021805  ]
Model epoch 1: train total loss -27.93087387084961, train mean loss 0.8612725734710693, test mean loss [0.99472296 1.0414844  0.8916954  0.99342966 0.89028305 0.9632647
 0.9447691 ]
Model epoch 2: train total loss -26.181352615356445, train mean loss 1.0868425369262695, test mean loss [0.95486814 0.9194227  1.5663472  0.9473561  0.90854156 0.9740512
 0.9278541 ]
Model epoch 3: train total loss -26.415973663330078, train mean loss 0.9583011269569397, test mean loss [0.9714138  0.96680397 1.0749654  1.1950961  0.9419496  1.0140394
 0.9232674 ]
Model epoch 4: train total loss -27.239578247070312, train mean loss 0.8423013091087341, test mean loss [0.95095813 0.9507041  1.0166136  1.1936295  0.94199216 1.0154755
 0.9307976 ]
Model epoch 5: train total loss -27.09125328063965, train mean loss 0.9755938649177551, test mean loss [0.9721785  0.96078545 0.9913391  1.079548   0.92432576 1.0747669
 0.9519513 ]
Model epoch 6: train total loss -28.131153106689453, train mean loss 0.804843008518219, test mean loss [0.9811685  0.94382524 0.9815179  1.0623729  0.99793726 1.0413574
 0.93972075]
Model epoch 7: train total loss -27.930221557617188, train mean loss 0.8663455843925476, test mean loss [0.97691    0.94558316 0.9843675  1.0906093  0.96418685 1.021395
 0.9938326 ]
Model epoch 8: train total loss -27.96837615966797, train mean loss 0.799900472164154, test mean loss [0.9802173  1.0221734  0.9860437  1.0421225  0.9575221  1.0271928
 0.96994364]
Model epoch 9: train total loss -27.97632598876953, train mean loss 0.8645560145378113, test mean loss [0.98658353 1.0070114  0.9953915  1.0426726  0.96807015 1.0306827
 1.0980233 ]
Model epoch 10: train total loss -27.12997055053711, train mean loss 0.9772105813026428, test mean loss [1.0390598  1.2945905  0.9974996  1.1710645  0.99200606 1.0396756
 1.060016  ]
Model trained in 11 epochs with 59000 transitions.
[2025-01-29 17:06:12,355][absl][INFO] - {'eval/walltime': 204.3374001979828, 'training/sps': 20.718968618303425, 'training/walltime': 1532.151875257492, 'training/model_train_time': 20.659245491027832, 'training/other_time': 27.5921528339386, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(60000, dtype=int32), 'model/train_total_loss': Array(-27.12997, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.9772106, dtype=float32), 'model/test_total_loss': Array(-25.61859, dtype=float32), 'model/test_mean_loss': Array(1.0848446, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.650750918821855, 'sac/actor_loss': Array(-29.6794, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.90391755, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(28.591143, dtype=float32), 'eval/episode_distance_from_origin': Array(3009.018, dtype=float32), 'eval/episode_forward_reward': Array(348.8601, dtype=float32), 'eval/episode_reward': Array(340.83072, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-314.49515, dtype=float32), 'eval/episode_reward_forward': Array(348.8601, dtype=float32), 'eval/episode_reward_survive': Array(308., dtype=float32), 'eval/episode_x_position': Array(2985.9705, dtype=float32), 'eval/episode_x_velocity': Array(348.8601, dtype=float32), 'eval/episode_y_position': Array(-0.04523575, dtype=float32), 'eval/episode_y_velocity': Array(19.95521, dtype=float32), 'eval/avg_episode_length': Array(309., dtype=float32), 'eval/epoch_eval_time': 3.2818679809570312, 'eval/sps': 304.70451761084803}
60000.0 {'eval/walltime': 204.3374001979828, 'training/sps': 20.718968618303425, 'training/walltime': 1532.151875257492, 'training/model_train_time': 20.659245491027832, 'training/other_time': 27.5921528339386, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(60000, dtype=int32), 'model/train_total_loss': Array(-27.12997, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.9772106, dtype=float32), 'model/test_total_loss': Array(-25.61859, dtype=float32), 'model/test_mean_loss': Array(1.0848446, dtype=float32), 'model/train_epochs': 11, 'model/sec_per_epoch': 1.650750918821855, 'sac/actor_loss': Array(-29.6794, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.90391755, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(28.591143, dtype=float32), 'eval/episode_distance_from_origin': Array(3009.018, dtype=float32), 'eval/episode_forward_reward': Array(348.8601, dtype=float32), 'eval/episode_reward': Array(340.83072, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-314.49515, dtype=float32), 'eval/episode_reward_forward': Array(348.8601, dtype=float32), 'eval/episode_reward_survive': Array(308., dtype=float32), 'eval/episode_x_position': Array(2985.9705, dtype=float32), 'eval/episode_x_velocity': Array(348.8601, dtype=float32), 'eval/episode_y_position': Array(-0.04523575, dtype=float32), 'eval/episode_y_velocity': Array(19.95521, dtype=float32), 'eval/avg_episode_length': Array(309., dtype=float32), 'eval/epoch_eval_time': 3.2818679809570312, 'eval/sps': 304.70451761084803, 'steps': Array(60000., dtype=float32)}
Model epoch 0: train total loss -27.385543823242188, train mean loss 0.8929439783096313, test mean loss [0.923809   0.9402332  0.878823   0.93924534 0.9968282  0.9562391
 0.88190794]
Model epoch 1: train total loss -27.82538604736328, train mean loss 0.8287657499313354, test mean loss [0.89031595 0.9572828  0.96400046 0.9415938  0.976634   0.980783
 0.89218014]
Model epoch 2: train total loss -26.8303279876709, train mean loss 0.9633524417877197, test mean loss [0.9025683  1.1422772  0.93917084 0.9732591  0.903551   0.9223783
 0.9207027 ]
Model epoch 3: train total loss -26.995689392089844, train mean loss 0.9084054231643677, test mean loss [0.91975987 1.1264132  1.0976665  0.96227187 0.8946063  0.94437337
 0.8936245 ]
Model epoch 4: train total loss -27.234458923339844, train mean loss 0.9065253138542175, test mean loss [0.9176804  1.065639   0.9863956  0.95118487 0.90964663 0.9636335
 1.0978572 ]
Model epoch 5: train total loss -27.700244903564453, train mean loss 0.9014400243759155, test mean loss [0.9384483  0.99982    0.94660103 0.99898934 0.9240869  0.9713377
 0.98660994]
Model epoch 6: train total loss -27.966432571411133, train mean loss 0.813374936580658, test mean loss [0.9212035  0.95544785 0.9541985  1.0307889  0.9306225  0.95385706
 1.0496955 ]
Model epoch 7: train total loss -27.17101287841797, train mean loss 0.9253027439117432, test mean loss [1.2061023  1.0057998  0.94598264 1.0085065  0.9291413  0.9786288
 0.9932654 ]
Model epoch 8: train total loss -27.823970794677734, train mean loss 0.8447452187538147, test mean loss [1.1687813  0.9651811  0.94732916 0.9925403  0.9382531  0.9806335
 0.99240226]
Model trained in 9 epochs with 60000 transitions.
[2025-01-29 17:06:44,456][absl][INFO] - {'eval/walltime': 207.62604975700378, 'training/sps': 34.72154679653825, 'training/walltime': 1560.9524354934692, 'training/model_train_time': 17.431150674819946, 'training/other_time': 11.353823184967041, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(61000, dtype=int32), 'model/train_total_loss': Array(-27.82397, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8447452, dtype=float32), 'model/test_total_loss': Array(-26.335312, dtype=float32), 'model/test_mean_loss': Array(0.99787444, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.6913139820098877, 'sac/actor_loss': Array(-30.635042, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.8903463, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(29.688843, dtype=float32), 'eval/episode_distance_from_origin': Array(45170.758, dtype=float32), 'eval/episode_forward_reward': Array(1826.0482, dtype=float32), 'eval/episode_reward': Array(1807.6819, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-1019.50977, dtype=float32), 'eval/episode_reward_forward': Array(1826.0482, dtype=float32), 'eval/episode_reward_survive': Array(999., dtype=float32), 'eval/episode_x_position': Array(44368.945, dtype=float32), 'eval/episode_x_velocity': Array(1826.0482, dtype=float32), 'eval/episode_y_position': Array(-8191.84, dtype=float32), 'eval/episode_y_velocity': Array(-272.33636, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 3.288649559020996, 'eval/sps': 304.0761814395608}
61000.0 {'eval/walltime': 207.62604975700378, 'training/sps': 34.72154679653825, 'training/walltime': 1560.9524354934692, 'training/model_train_time': 17.431150674819946, 'training/other_time': 11.353823184967041, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(61000, dtype=int32), 'model/train_total_loss': Array(-27.82397, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8447452, dtype=float32), 'model/test_total_loss': Array(-26.335312, dtype=float32), 'model/test_mean_loss': Array(0.99787444, dtype=float32), 'model/train_epochs': 9, 'model/sec_per_epoch': 1.6913139820098877, 'sac/actor_loss': Array(-30.635042, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.8903463, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(29.688843, dtype=float32), 'eval/episode_distance_from_origin': Array(45170.758, dtype=float32), 'eval/episode_forward_reward': Array(1826.0482, dtype=float32), 'eval/episode_reward': Array(1807.6819, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-1019.50977, dtype=float32), 'eval/episode_reward_forward': Array(1826.0482, dtype=float32), 'eval/episode_reward_survive': Array(999., dtype=float32), 'eval/episode_x_position': Array(44368.945, dtype=float32), 'eval/episode_x_velocity': Array(1826.0482, dtype=float32), 'eval/episode_y_position': Array(-8191.84, dtype=float32), 'eval/episode_y_velocity': Array(-272.33636, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 3.288649559020996, 'eval/sps': 304.0761814395608, 'steps': Array(61000., dtype=float32)}
Model epoch 0: train total loss -26.12221336364746, train mean loss 0.9327235817909241, test mean loss [1.1255794  0.884853   0.9052706  0.9206781  0.8389628  1.3071662
 0.87948024]
Model epoch 1: train total loss -26.75601577758789, train mean loss 0.909209132194519, test mean loss [0.99763936 0.87211746 0.88627994 0.90477365 0.8898561  1.1686345
 0.87621367]
Model epoch 2: train total loss -27.531309127807617, train mean loss 0.8716175556182861, test mean loss [0.9593375  0.8993604  0.93361974 0.94411266 1.0019448  1.1057048
 0.8869982 ]
Model epoch 3: train total loss -26.561534881591797, train mean loss 0.9560456275939941, test mean loss [0.9466018  0.9045744  1.037543   0.90720344 1.2370884  1.0115522
 0.89036655]
Model epoch 4: train total loss -27.940052032470703, train mean loss 0.8629786968231201, test mean loss [0.9252925  0.8862089  0.9358875  0.94779193 1.0182118  1.0521294
 0.91038316]
Model epoch 5: train total loss -28.18822479248047, train mean loss 0.7978612184524536, test mean loss [0.91868854 0.90760696 0.93681276 0.9235015  0.9717455  0.99167246
 0.90284127]
Model epoch 6: train total loss -28.19763946533203, train mean loss 0.8421370387077332, test mean loss [0.9458327  0.9110469  0.91587275 1.0301749  0.96266073 1.0052204
 0.90411127]
Model epoch 7: train total loss -28.515928268432617, train mean loss 0.8193458318710327, test mean loss [0.9271657  0.9032929  0.9294867  1.109877   0.98350644 0.9739466
 0.9143526 ]
Model epoch 8: train total loss -28.289928436279297, train mean loss 0.8300700783729553, test mean loss [1.0134721  0.9514579  0.9605457  0.97575754 0.9471886  0.96145034
 0.97467536]
Model epoch 9: train total loss -27.919946670532227, train mean loss 0.9207671284675598, test mean loss [1.0322454  0.93403673 0.9521155  0.97628987 0.9785106  0.9667062
 0.930637  ]
Model epoch 10: train total loss -28.79851722717285, train mean loss 0.8180900812149048, test mean loss [0.9486685 1.0341687 0.9443655 1.0521647 0.9500816 0.9661796 0.9303273]
Model epoch 11: train total loss -28.47115707397461, train mean loss 0.7773903608322144, test mean loss [0.93653035 0.96223164 0.9641019  1.0019152  0.9594567  0.9773176
 0.9404193 ]
Model epoch 12: train total loss -28.66579246520996, train mean loss 0.8118498921394348, test mean loss [0.94337416 0.9782897  0.9817254  1.0390441  0.9931915  0.96790546
 0.96984524]
Model epoch 13: train total loss -28.712162017822266, train mean loss 0.8278409242630005, test mean loss [0.9679198  0.9614875  0.9600706  0.99099463 0.9680183  0.9750337
 0.967775  ]
Model epoch 14: train total loss -28.559123992919922, train mean loss 0.8156870007514954, test mean loss [0.9609666 1.0104841 0.983245  1.0182626 0.9964185 1.0416275 1.0083694]
Model trained in 15 epochs with 61000 transitions.
[2025-01-29 17:07:25,456][absl][INFO] - {'eval/walltime': 210.91599464416504, 'training/sps': 26.52440799712763, 'training/walltime': 1598.6535596847534, 'training/model_train_time': 26.325434923171997, 'training/other_time': 11.361738681793213, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(62000, dtype=int32), 'model/train_total_loss': Array(-28.559124, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.815687, dtype=float32), 'model/test_total_loss': Array(-26.193012, dtype=float32), 'model/test_mean_loss': Array(1.0027677, dtype=float32), 'model/train_epochs': 15, 'model/sec_per_epoch': 1.6125421365102133, 'sac/actor_loss': Array(-35.9817, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.85952204, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(30.403217, dtype=float32), 'eval/episode_distance_from_origin': Array(3131.7266, dtype=float32), 'eval/episode_forward_reward': Array(479.1332, dtype=float32), 'eval/episode_reward': Array(455.95132, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-279.79398, dtype=float32), 'eval/episode_reward_forward': Array(479.1332, dtype=float32), 'eval/episode_reward_survive': Array(257., dtype=float32), 'eval/episode_x_position': Array(3081.3218, dtype=float32), 'eval/episode_x_velocity': Array(479.1332, dtype=float32), 'eval/episode_y_position': Array(-441.71582, dtype=float32), 'eval/episode_y_velocity': Array(-19.535975, dtype=float32), 'eval/avg_episode_length': Array(258., dtype=float32), 'eval/epoch_eval_time': 3.289944887161255, 'eval/sps': 303.9564595450883}
62000.0 {'eval/walltime': 210.91599464416504, 'training/sps': 26.52440799712763, 'training/walltime': 1598.6535596847534, 'training/model_train_time': 26.325434923171997, 'training/other_time': 11.361738681793213, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(62000, dtype=int32), 'model/train_total_loss': Array(-28.559124, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.815687, dtype=float32), 'model/test_total_loss': Array(-26.193012, dtype=float32), 'model/test_mean_loss': Array(1.0027677, dtype=float32), 'model/train_epochs': 15, 'model/sec_per_epoch': 1.6125421365102133, 'sac/actor_loss': Array(-35.9817, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.85952204, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(30.403217, dtype=float32), 'eval/episode_distance_from_origin': Array(3131.7266, dtype=float32), 'eval/episode_forward_reward': Array(479.1332, dtype=float32), 'eval/episode_reward': Array(455.95132, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-279.79398, dtype=float32), 'eval/episode_reward_forward': Array(479.1332, dtype=float32), 'eval/episode_reward_survive': Array(257., dtype=float32), 'eval/episode_x_position': Array(3081.3218, dtype=float32), 'eval/episode_x_velocity': Array(479.1332, dtype=float32), 'eval/episode_y_position': Array(-441.71582, dtype=float32), 'eval/episode_y_velocity': Array(-19.535975, dtype=float32), 'eval/avg_episode_length': Array(258., dtype=float32), 'eval/epoch_eval_time': 3.289944887161255, 'eval/sps': 303.9564595450883, 'steps': Array(62000., dtype=float32)}
Model epoch 0: train total loss -27.838027954101562, train mean loss 0.830020546913147, test mean loss [0.87546706 0.88392866 0.89502364 0.88586813 0.92846864 0.8572704
 1.0934457 ]
Model epoch 1: train total loss -27.95456314086914, train mean loss 0.838699221611023, test mean loss [0.86372435 1.0673507  0.9322457  0.89538276 0.9038705  0.9743246
 0.966981  ]
Model epoch 2: train total loss -27.297481536865234, train mean loss 0.8512222766876221, test mean loss [0.8834876  1.3442031  0.92797613 0.9013267  0.9142785  0.9818997
 0.92193204]
Model epoch 3: train total loss -28.842912673950195, train mean loss 0.7645617723464966, test mean loss [0.89509636 1.0034449  0.9451976  0.9281958  0.8959951  0.88649774
 0.9330196 ]
Model epoch 4: train total loss -28.586164474487305, train mean loss 0.8446601033210754, test mean loss [0.8828418  1.0275403  0.91917264 0.9153825  0.95467234 1.0036545
 0.9139224 ]
Model epoch 5: train total loss -27.261178970336914, train mean loss 0.9722968935966492, test mean loss [1.4424658  0.9332187  0.97277313 0.92756706 0.9442116  0.9253286
 0.90352917]
Model epoch 6: train total loss -28.919530868530273, train mean loss 0.7696393132209778, test mean loss [1.0793583  1.0105942  0.94410706 0.9345212  0.9275224  0.888273
 0.94397926]
Model epoch 7: train total loss -28.027267456054688, train mean loss 0.8440766930580139, test mean loss [1.0521281  0.9734129  0.93087476 0.99195075 0.9335253  0.90656155
 0.91687584]
Model epoch 8: train total loss -28.906984329223633, train mean loss 0.7889188528060913, test mean loss [0.96722895 0.9547182  0.9653807  1.1565666  0.94636106 0.8746484
 1.0176787 ]
Model epoch 9: train total loss -28.220958709716797, train mean loss 0.8605764508247375, test mean loss [1.004472   0.97746915 0.9874835  1.0408645  0.95298165 0.9316986
 0.91857094]
Model epoch 10: train total loss -28.372238159179688, train mean loss 0.8034703731536865, test mean loss [1.1308061  0.94928676 0.9647099  0.98800594 0.99682087 0.92196995
 0.9015865 ]
Model epoch 11: train total loss -28.277297973632812, train mean loss 0.7774420380592346, test mean loss [1.0133481  0.9386814  0.9948833  1.1520051  0.97087306 0.97297996
 0.94791806]
Model trained in 12 epochs with 62000 transitions.
[2025-01-29 17:08:03,420][absl][INFO] - {'eval/walltime': 214.20677495002747, 'training/sps': 28.849843678059024, 'training/walltime': 1633.3157925605774, 'training/model_train_time': 23.28962779045105, 'training/other_time': 11.3577721118927, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(63000, dtype=int32), 'model/train_total_loss': Array(-28.277298, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.77744204, dtype=float32), 'model/test_total_loss': Array(-26.069445, dtype=float32), 'model/test_mean_loss': Array(0.99866986, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.7075148622194927, 'sac/actor_loss': Array(-42.406857, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.8153261, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(33.32893, dtype=float32), 'eval/episode_distance_from_origin': Array(42898.973, dtype=float32), 'eval/episode_forward_reward': Array(1749.2095, dtype=float32), 'eval/episode_reward': Array(1665.53, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-1082.9208, dtype=float32), 'eval/episode_reward_forward': Array(1749.2095, dtype=float32), 'eval/episode_reward_survive': Array(999., dtype=float32), 'eval/episode_x_position': Array(42474.938, dtype=float32), 'eval/episode_x_velocity': Array(1749.2095, dtype=float32), 'eval/episode_y_position': Array(-5793.662, dtype=float32), 'eval/episode_y_velocity': Array(-189.63504, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 3.2907803058624268, 'eval/sps': 303.87929519893197}
63000.0 {'eval/walltime': 214.20677495002747, 'training/sps': 28.849843678059024, 'training/walltime': 1633.3157925605774, 'training/model_train_time': 23.28962779045105, 'training/other_time': 11.3577721118927, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(63000, dtype=int32), 'model/train_total_loss': Array(-28.277298, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.77744204, dtype=float32), 'model/test_total_loss': Array(-26.069445, dtype=float32), 'model/test_mean_loss': Array(0.99866986, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.7075148622194927, 'sac/actor_loss': Array(-42.406857, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.8153261, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(33.32893, dtype=float32), 'eval/episode_distance_from_origin': Array(42898.973, dtype=float32), 'eval/episode_forward_reward': Array(1749.2095, dtype=float32), 'eval/episode_reward': Array(1665.53, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-1082.9208, dtype=float32), 'eval/episode_reward_forward': Array(1749.2095, dtype=float32), 'eval/episode_reward_survive': Array(999., dtype=float32), 'eval/episode_x_position': Array(42474.938, dtype=float32), 'eval/episode_x_velocity': Array(1749.2095, dtype=float32), 'eval/episode_y_position': Array(-5793.662, dtype=float32), 'eval/episode_y_velocity': Array(-189.63504, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 3.2907803058624268, 'eval/sps': 303.87929519893197, 'steps': Array(63000., dtype=float32)}
Model epoch 0: train total loss -27.313589096069336, train mean loss 0.9272456169128418, test mean loss [0.95003194 0.8990817  0.94993305 0.89387685 0.959257   0.8912111
 0.97693413]
Model epoch 1: train total loss -28.417924880981445, train mean loss 0.8031374216079712, test mean loss [0.9026568  0.86374944 0.8572176  0.8710834  1.0230713  0.8857835
 0.92795914]
Model epoch 2: train total loss -28.877548217773438, train mean loss 0.8568156957626343, test mean loss [0.9032526  0.83951247 0.86699456 0.87766325 0.9423171  0.87142205
 0.8963943 ]
Model epoch 3: train total loss -27.806255340576172, train mean loss 0.8611422777175903, test mean loss [0.9168412  0.84925795 1.0855967  0.85919005 0.99907196 0.9108367
 0.8837786 ]
Model epoch 4: train total loss -27.914974212646484, train mean loss 0.8806079626083374, test mean loss [0.911399   0.8906716  0.9167244  0.9567217  0.9667992  0.89284587
 0.88727486]
Model epoch 5: train total loss -28.396615982055664, train mean loss 0.8482487797737122, test mean loss [0.8890727  0.90434825 0.8817249  1.0092205  0.93362933 0.9086724
 0.89571315]
Model epoch 6: train total loss -28.441444396972656, train mean loss 0.813707709312439, test mean loss [0.9016157  0.93277144 0.88432074 0.93958694 0.927003   0.93045735
 0.8834466 ]
Model epoch 7: train total loss -28.564254760742188, train mean loss 0.8160396814346313, test mean loss [0.91364807 0.93059903 0.8670088  0.97961235 0.9448441  0.9525327
 0.8711407 ]
Model epoch 8: train total loss -28.832643508911133, train mean loss 0.7744748592376709, test mean loss [0.9089568  0.92095274 0.89296764 0.9439923  0.9234391  0.99301726
 0.91373146]
Model epoch 9: train total loss -27.469396591186523, train mean loss 0.9587827920913696, test mean loss [0.92527443 1.0825745  0.8979868  0.9377456  0.97951794 1.1051385
 1.099324  ]
Model epoch 10: train total loss -28.216793060302734, train mean loss 0.7633334398269653, test mean loss [1.0786817  1.0112994  0.89514756 0.9394824  0.92721856 1.0238125
 0.95376706]
Model epoch 11: train total loss -29.07697868347168, train mean loss 0.7854624390602112, test mean loss [1.1116433  0.928275   0.9194175  0.9402526  0.9283833  0.97560596
 0.88940936]
Model epoch 12: train total loss -28.575279235839844, train mean loss 0.8686428666114807, test mean loss [0.9604499 0.9953258 0.9715204 0.9454402 0.9257905 0.9529954 0.8945297]
Model epoch 13: train total loss -29.476741790771484, train mean loss 0.7573383450508118, test mean loss [0.9484555  0.9227741  0.91179603 0.9836201  0.92238635 0.99223346
 0.9314372 ]
Model trained in 14 epochs with 63000 transitions.
[2025-01-29 17:08:44,270][absl][INFO] - {'eval/walltime': 217.49575114250183, 'training/sps': 26.630600162792984, 'training/walltime': 1670.8665797710419, 'training/model_train_time': 26.190387964248657, 'training/other_time': 11.345591068267822, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(64000, dtype=int32), 'model/train_total_loss': Array(-29.476742, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.75733835, dtype=float32), 'model/test_total_loss': Array(-27.283535, dtype=float32), 'model/test_mean_loss': Array(0.9446718, dtype=float32), 'model/train_epochs': 14, 'model/sec_per_epoch': 1.7010619129453386, 'sac/actor_loss': Array(-43.99929, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.81475216, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(34.129894, dtype=float32), 'eval/episode_distance_from_origin': Array(43789.598, dtype=float32), 'eval/episode_forward_reward': Array(1770.8378, dtype=float32), 'eval/episode_reward': Array(1713.5076, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-1056.522, dtype=float32), 'eval/episode_reward_forward': Array(1770.8378, dtype=float32), 'eval/episode_reward_survive': Array(999., dtype=float32), 'eval/episode_x_position': Array(42555.297, dtype=float32), 'eval/episode_x_velocity': Array(1770.8378, dtype=float32), 'eval/episode_y_position': Array(9801.453, dtype=float32), 'eval/episode_y_velocity': Array(289.19327, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 3.2889761924743652, 'eval/sps': 304.0459831506652}
64000.0 {'eval/walltime': 217.49575114250183, 'training/sps': 26.630600162792984, 'training/walltime': 1670.8665797710419, 'training/model_train_time': 26.190387964248657, 'training/other_time': 11.345591068267822, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(64000, dtype=int32), 'model/train_total_loss': Array(-29.476742, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.75733835, dtype=float32), 'model/test_total_loss': Array(-27.283535, dtype=float32), 'model/test_mean_loss': Array(0.9446718, dtype=float32), 'model/train_epochs': 14, 'model/sec_per_epoch': 1.7010619129453386, 'sac/actor_loss': Array(-43.99929, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.81475216, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(34.129894, dtype=float32), 'eval/episode_distance_from_origin': Array(43789.598, dtype=float32), 'eval/episode_forward_reward': Array(1770.8378, dtype=float32), 'eval/episode_reward': Array(1713.5076, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-1056.522, dtype=float32), 'eval/episode_reward_forward': Array(1770.8378, dtype=float32), 'eval/episode_reward_survive': Array(999., dtype=float32), 'eval/episode_x_position': Array(42555.297, dtype=float32), 'eval/episode_x_velocity': Array(1770.8378, dtype=float32), 'eval/episode_y_position': Array(9801.453, dtype=float32), 'eval/episode_y_velocity': Array(289.19327, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 3.2889761924743652, 'eval/sps': 304.0459831506652, 'steps': Array(64000., dtype=float32)}
Model epoch 0: train total loss -26.065378189086914, train mean loss 1.0285471677780151, test mean loss [0.87881184 0.857468   0.9354789  0.9104561  0.8199581  0.8368397
 1.9134959 ]
Model epoch 1: train total loss -27.969066619873047, train mean loss 0.8427294492721558, test mean loss [0.84241396 0.89120156 0.82628286 0.85077566 1.0239401  0.8602518
 0.99554116]
Model epoch 2: train total loss -28.65432357788086, train mean loss 0.8574807643890381, test mean loss [0.8498672  0.8163352  0.8568955  0.84645885 0.90236527 0.878591
 0.95423734]
Model epoch 3: train total loss -27.8189640045166, train mean loss 0.9414318799972534, test mean loss [0.8623564  0.82387376 1.0871705  0.96813285 1.082876   0.87155426
 0.906736  ]
Model epoch 4: train total loss -27.344945907592773, train mean loss 0.9194352626800537, test mean loss [0.8476822 0.8415623 0.9625278 0.8631983 1.0168668 1.0865755 0.8989371]
Model epoch 5: train total loss -28.062795639038086, train mean loss 0.8737815022468567, test mean loss [0.8914181  0.82580125 0.8647337  1.1390743  0.9337409  0.9897779
 0.872638  ]
Model epoch 6: train total loss -28.61417579650879, train mean loss 0.8223866820335388, test mean loss [0.8795713  0.84069234 0.8404387  0.8902229  0.87977    0.9889736
 0.89341307]
Model epoch 7: train total loss -28.50472068786621, train mean loss 0.8383191823959351, test mean loss [0.8747178  0.8260855  0.84063435 0.8777238  0.8886309  0.9448555
 0.8878445 ]
Model epoch 8: train total loss -29.318819046020508, train mean loss 0.8023731112480164, test mean loss [0.8974497  0.8341736  0.8351778  0.8702773  0.8863601  0.95307994
 0.88138044]
Model epoch 9: train total loss -28.968955993652344, train mean loss 0.7824575304985046, test mean loss [0.8768062  0.8849216  0.8556347  0.8654617  0.9031471  0.9346947
 0.89187896]
Model epoch 10: train total loss -27.2995662689209, train mean loss 0.9024953842163086, test mean loss [0.8763499  0.93125105 1.5960491  0.95597816 0.8763882  1.1980281
 0.88242733]
Model epoch 11: train total loss -29.17200469970703, train mean loss 0.7620537877082825, test mean loss [0.9003783  0.9642811  1.0117952  0.8645447  0.87109184 0.9905661
 0.92210907]
Model trained in 12 epochs with 64000 transitions.
[2025-01-29 17:09:22,226][absl][INFO] - {'eval/walltime': 220.77614092826843, 'training/sps': 28.862012523925642, 'training/walltime': 1705.5141983032227, 'training/model_train_time': 23.274035215377808, 'training/other_time': 11.359488487243652, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(65000, dtype=int32), 'model/train_total_loss': Array(-29.172005, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.7620538, dtype=float32), 'model/test_total_loss': Array(-27.026167, dtype=float32), 'model/test_mean_loss': Array(0.93210953, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.74655948082606, 'sac/actor_loss': Array(-49.33542, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.78630555, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(35.20235, dtype=float32), 'eval/episode_distance_from_origin': Array(154.31204, dtype=float32), 'eval/episode_forward_reward': Array(109.30128, dtype=float32), 'eval/episode_reward': Array(92.23024, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-72.53958, dtype=float32), 'eval/episode_reward_forward': Array(109.30128, dtype=float32), 'eval/episode_reward_survive': Array(55., dtype=float32), 'eval/episode_x_position': Array(144.32632, dtype=float32), 'eval/episode_x_velocity': Array(109.30128, dtype=float32), 'eval/episode_y_position': Array(-6.297086, dtype=float32), 'eval/episode_y_velocity': Array(5.8207464, dtype=float32), 'eval/avg_episode_length': Array(56., dtype=float32), 'eval/epoch_eval_time': 3.2803897857666016, 'eval/sps': 304.8418222550671}
65000.0 {'eval/walltime': 220.77614092826843, 'training/sps': 28.862012523925642, 'training/walltime': 1705.5141983032227, 'training/model_train_time': 23.274035215377808, 'training/other_time': 11.359488487243652, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(65000, dtype=int32), 'model/train_total_loss': Array(-29.172005, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.7620538, dtype=float32), 'model/test_total_loss': Array(-27.026167, dtype=float32), 'model/test_mean_loss': Array(0.93210953, dtype=float32), 'model/train_epochs': 12, 'model/sec_per_epoch': 1.74655948082606, 'sac/actor_loss': Array(-49.33542, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.78630555, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(35.20235, dtype=float32), 'eval/episode_distance_from_origin': Array(154.31204, dtype=float32), 'eval/episode_forward_reward': Array(109.30128, dtype=float32), 'eval/episode_reward': Array(92.23024, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-72.53958, dtype=float32), 'eval/episode_reward_forward': Array(109.30128, dtype=float32), 'eval/episode_reward_survive': Array(55., dtype=float32), 'eval/episode_x_position': Array(144.32632, dtype=float32), 'eval/episode_x_velocity': Array(109.30128, dtype=float32), 'eval/episode_y_position': Array(-6.297086, dtype=float32), 'eval/episode_y_velocity': Array(5.8207464, dtype=float32), 'eval/avg_episode_length': Array(56., dtype=float32), 'eval/epoch_eval_time': 3.2803897857666016, 'eval/sps': 304.8418222550671, 'steps': Array(65000., dtype=float32)}
Model epoch 0: train total loss -26.25926971435547, train mean loss 0.9314748048782349, test mean loss [1.2890141  0.8472536  0.91025656 0.95958847 0.86980087 0.8980908
 0.9056818 ]
Model epoch 1: train total loss -27.85588264465332, train mean loss 0.8136366605758667, test mean loss [1.2718581  0.9698961  0.96949756 1.0818949  0.8708511  0.90320474
 0.910277  ]
Model epoch 2: train total loss -28.12015724182129, train mean loss 0.8219640254974365, test mean loss [1.0176952  0.9138563  0.9915299  0.96592677 0.8713476  0.9163899
 0.87546325]
Model epoch 3: train total loss -28.56395149230957, train mean loss 0.7711896300315857, test mean loss [0.97024274 0.8624479  0.9914111  0.9758475  0.85793513 0.88467747
 0.8796067 ]
Model epoch 4: train total loss -29.205541610717773, train mean loss 0.7782456278800964, test mean loss [0.981511   0.88086    0.90024906 0.90375257 0.8532056  0.9067312
 0.86004364]
Model epoch 5: train total loss -28.83029556274414, train mean loss 0.7847678661346436, test mean loss [0.9673628  0.9540368  0.89381766 0.9055717  0.85227305 0.9126186
 0.8738719 ]
Model epoch 6: train total loss -28.025901794433594, train mean loss 0.8265135884284973, test mean loss [0.96944666 0.97386634 0.8984901  0.92087406 0.9334215  0.96991175
 1.0048143 ]
Model epoch 7: train total loss -28.499889373779297, train mean loss 0.7769896984100342, test mean loss [0.98339355 0.96098685 0.9011645  0.9815539  0.9278302  0.9598715
 0.99157196]
Model epoch 8: train total loss -28.950584411621094, train mean loss 0.8149393796920776, test mean loss [0.9396965 0.9451671 0.9145719 0.9221072 1.0072281 0.9380054 0.9870063]
Model epoch 9: train total loss -28.949310302734375, train mean loss 0.8039071559906006, test mean loss [0.9149958  1.076906   0.9072276  0.89582074 0.8953395  0.9452536
 0.91845727]
Model epoch 10: train total loss -29.482810974121094, train mean loss 0.7514008283615112, test mean loss [0.9248518  0.98262495 0.93171924 0.9138531  0.8901103  0.94283855
 0.9263015 ]
Model epoch 11: train total loss -28.91890525817871, train mean loss 0.8523567318916321, test mean loss [1.2959307  0.96035236 0.9095767  1.4382347  0.94838214 0.9588476
 0.91692436]
Model epoch 12: train total loss -28.747709274291992, train mean loss 0.7912830710411072, test mean loss [1.121668   0.9871352  0.956331   1.0127847  1.0135107  0.99388593
 0.92702407]
Model epoch 13: train total loss -28.3577938079834, train mean loss 0.8384186029434204, test mean loss [1.2370389  1.0335352  0.9181176  1.0135046  0.9463459  1.0159372
 0.93075997]
Model epoch 14: train total loss -27.624855041503906, train mean loss 0.7865867018699646, test mean loss [1.0315843  0.9597242  0.9285582  0.954935   0.94066405 1.0803671
 1.1826211 ]
Model epoch 15: train total loss -28.822771072387695, train mean loss 0.7943326234817505, test mean loss [1.0001547 0.950665  0.9729456 0.9271647 0.9434088 1.0745046 1.074269 ]
Model trained in 16 epochs with 65000 transitions.
[2025-01-29 17:10:06,697][absl][INFO] - {'eval/walltime': 224.0623061656952, 'training/sps': 24.28670642974349, 'training/walltime': 1746.6889867782593, 'training/model_train_time': 29.81504988670349, 'training/other_time': 11.345587253570557, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(66000, dtype=int32), 'model/train_total_loss': Array(-28.822771, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.7943326, dtype=float32), 'model/test_total_loss': Array(-26.260359, dtype=float32), 'model/test_mean_loss': Array(0.99187326, dtype=float32), 'model/train_epochs': 16, 'model/sec_per_epoch': 1.7152621746063232, 'sac/actor_loss': Array(-49.487263, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.7886201, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(33.20386, dtype=float32), 'eval/episode_distance_from_origin': Array(3944.1018, dtype=float32), 'eval/episode_forward_reward': Array(632.89746, dtype=float32), 'eval/episode_reward': Array(591.9691, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-302.01892, dtype=float32), 'eval/episode_reward_forward': Array(632.89746, dtype=float32), 'eval/episode_reward_survive': Array(259., dtype=float32), 'eval/episode_x_position': Array(3929.5952, dtype=float32), 'eval/episode_x_velocity': Array(632.89746, dtype=float32), 'eval/episode_y_position': Array(65.593544, dtype=float32), 'eval/episode_y_velocity': Array(-19.78022, dtype=float32), 'eval/avg_episode_length': Array(260., dtype=float32), 'eval/epoch_eval_time': 3.286165237426758, 'eval/sps': 304.30606124452015}
66000.0 {'eval/walltime': 224.0623061656952, 'training/sps': 24.28670642974349, 'training/walltime': 1746.6889867782593, 'training/model_train_time': 29.81504988670349, 'training/other_time': 11.345587253570557, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(66000, dtype=int32), 'model/train_total_loss': Array(-28.822771, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.7943326, dtype=float32), 'model/test_total_loss': Array(-26.260359, dtype=float32), 'model/test_mean_loss': Array(0.99187326, dtype=float32), 'model/train_epochs': 16, 'model/sec_per_epoch': 1.7152621746063232, 'sac/actor_loss': Array(-49.487263, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.7886201, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(33.20386, dtype=float32), 'eval/episode_distance_from_origin': Array(3944.1018, dtype=float32), 'eval/episode_forward_reward': Array(632.89746, dtype=float32), 'eval/episode_reward': Array(591.9691, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-302.01892, dtype=float32), 'eval/episode_reward_forward': Array(632.89746, dtype=float32), 'eval/episode_reward_survive': Array(259., dtype=float32), 'eval/episode_x_position': Array(3929.5952, dtype=float32), 'eval/episode_x_velocity': Array(632.89746, dtype=float32), 'eval/episode_y_position': Array(65.593544, dtype=float32), 'eval/episode_y_velocity': Array(-19.78022, dtype=float32), 'eval/avg_episode_length': Array(260., dtype=float32), 'eval/epoch_eval_time': 3.286165237426758, 'eval/sps': 304.30606124452015, 'steps': Array(66000., dtype=float32)}
Model epoch 0: train total loss -28.48345947265625, train mean loss 0.8839331269264221, test mean loss [0.9194629  0.8238503  0.79161155 0.7878995  0.92307675 0.8785559
 1.0451473 ]
Model epoch 1: train total loss -28.835121154785156, train mean loss 0.8260437846183777, test mean loss [0.88707656 0.7868394  0.8060672  0.8353087  0.7954157  0.83112174
 0.87822944]
Model epoch 2: train total loss -29.291311264038086, train mean loss 0.7955124378204346, test mean loss [0.8879612  1.072587   0.8165623  0.82990605 0.7950768  0.89440835
 0.852533  ]
Model epoch 3: train total loss -27.770252227783203, train mean loss 0.865212082862854, test mean loss [0.8840274  0.85875255 0.8996354  0.8871439  0.7971706  1.4323804
 0.9170378 ]
Model epoch 4: train total loss -28.160953521728516, train mean loss 0.8955481648445129, test mean loss [0.859927   0.8567406  0.8771977  0.83643234 0.8987324  1.2109494
 0.857114  ]
Model epoch 5: train total loss -29.339553833007812, train mean loss 0.775534451007843, test mean loss [0.8564375 0.8453753 0.8148547 0.9447946 0.9133876 1.1242052 0.8404963]
Model epoch 6: train total loss -29.1143798828125, train mean loss 0.8290361166000366, test mean loss [0.8269578  0.83874637 0.8438306  0.9002017  0.8733132  1.0861454
 0.8929524 ]
Model epoch 7: train total loss -28.05394744873047, train mean loss 0.8408854603767395, test mean loss [1.1033596  0.8595547  0.89264166 0.9127841  0.859882   1.0687895
 0.9011014 ]
Model epoch 8: train total loss -28.51308822631836, train mean loss 0.7959204912185669, test mean loss [0.91174865 0.831795   0.83796257 1.0493629  0.9374801  1.058362
 0.8856015 ]
Model epoch 9: train total loss -28.018299102783203, train mean loss 0.8761699199676514, test mean loss [0.88583976 0.9626641  0.8531075  1.3506995  0.87657386 1.0423597
 0.8721011 ]
Model epoch 10: train total loss -28.39389991760254, train mean loss 0.8365590572357178, test mean loss [0.9314512  0.93000054 0.8596072  1.0586044  0.88564026 1.0123119
 0.9329145 ]
Model epoch 11: train total loss -29.16642189025879, train mean loss 0.754944384098053, test mean loss [0.89306724 0.9658762  0.8624448  1.0331305  0.8846155  1.0287743
 0.89202577]
Model epoch 12: train total loss -29.076974868774414, train mean loss 0.8125792741775513, test mean loss [0.8808741  0.89387465 0.9686526  0.97825485 0.88198644 1.0157429
 0.8957475 ]
Model trained in 13 epochs with 66000 transitions.
[2025-01-29 17:10:46,420][absl][INFO] - {'eval/walltime': 227.34462976455688, 'training/sps': 27.44889698640285, 'training/walltime': 1783.1203231811523, 'training/model_train_time': 25.055099725723267, 'training/other_time': 11.36210322380066, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(67000, dtype=int32), 'model/train_total_loss': Array(-29.076975, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8125793, dtype=float32), 'model/test_total_loss': Array(-27.440641, dtype=float32), 'model/test_mean_loss': Array(0.9307334, dtype=float32), 'model/train_epochs': 13, 'model/sec_per_epoch': 1.7477041574624868, 'sac/actor_loss': Array(-49.68333, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.7793558, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(35.232212, dtype=float32), 'eval/episode_distance_from_origin': Array(341.96368, dtype=float32), 'eval/episode_forward_reward': Array(168.78699, dtype=float32), 'eval/episode_reward': Array(158.33026, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-105.63673, dtype=float32), 'eval/episode_reward_forward': Array(168.78699, dtype=float32), 'eval/episode_reward_survive': Array(92., dtype=float32), 'eval/episode_x_position': Array(313.6747, dtype=float32), 'eval/episode_x_velocity': Array(168.78699, dtype=float32), 'eval/episode_y_position': Array(-108.052734, dtype=float32), 'eval/episode_y_velocity': Array(-50.157227, dtype=float32), 'eval/avg_episode_length': Array(93., dtype=float32), 'eval/epoch_eval_time': 3.2823235988616943, 'eval/sps': 304.6622217098883}
67000.0 {'eval/walltime': 227.34462976455688, 'training/sps': 27.44889698640285, 'training/walltime': 1783.1203231811523, 'training/model_train_time': 25.055099725723267, 'training/other_time': 11.36210322380066, 'training/model_horizon': 5, 'training/hallucination_updates_per_training_step': 1000, 'training/env_buffer_size': Array(67000, dtype=int32), 'model/train_total_loss': Array(-29.076975, dtype=float32, weak_type=True), 'model/train_mean_loss': Array(0.8125793, dtype=float32), 'model/test_total_loss': Array(-27.440641, dtype=float32), 'model/test_mean_loss': Array(0.9307334, dtype=float32), 'model/train_epochs': 13, 'model/sec_per_epoch': 1.7477041574624868, 'sac/actor_loss': Array(-49.68333, dtype=float32), 'sac/alpha': Array(0.19999999, dtype=float32), 'sac/alpha_loss': Array(0.7793558, dtype=float32), 'sac/buffer_current_size': Array(1001999., dtype=float32), 'sac/critic_loss': Array(35.232212, dtype=float32), 'eval/episode_distance_from_origin': Array(341.96368, dtype=float32), 'eval/episode_forward_reward': Array(168.78699, dtype=float32), 'eval/episode_reward': Array(158.33026, dtype=float32), 'eval/episode_reward_contact': Array(0., dtype=float32), 'eval/episode_reward_ctrl': Array(-105.63673, dtype=float32), 'eval/episode_reward_forward': Array(168.78699, dtype=float32), 'eval/episode_reward_survive': Array(92., dtype=float32), 'eval/episode_x_position': Array(313.6747, dtype=float32), 'eval/episode_x_velocity': Array(168.78699, dtype=float32), 'eval/episode_y_position': Array(-108.052734, dtype=float32), 'eval/episode_y_velocity': Array(-50.157227, dtype=float32), 'eval/avg_episode_length': Array(93., dtype=float32), 'eval/epoch_eval_time': 3.2823235988616943, 'eval/sps': 304.6622217098883, 'steps': Array(67000., dtype=float32)}
Traceback (most recent call last):
  File "/home/ant/ssrl/ssrl/scripts/rl_benchmarks.py", line 111, in <module>
    train()
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/ant/ssrl/ssrl/scripts/rl_benchmarks.py", line 89, in train
    ssrl_fn(
  File "/home/ant/ssrl/ssrl/brax/training/agents/ssrl/train.py", line 263, in train
    env_state) = sim_training_epoch_with_timing(
  File "/home/ant/ssrl/ssrl/brax/training/agents/ssrl/train.py", line 947, in sim_training_epoch_with_timing
    training_state, model_metrics = train_model(
  File "/home/ant/ssrl/ssrl/brax/training/agents/ssrl/train.py", line 661, in train_model
    test_total_loss, test_mean_loss) = model_training_epoch(
  File "/home/ant/ssrl/ssrl/brax/training/agents/ssrl/train.py", line 741, in model_training_epoch
    transitions, test_transitions = prepare_data(
  File "/home/ant/ssrl/ssrl/brax/training/agents/ssrl/train.py", line 787, in prepare_data
    transitions = jax.tree_map(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/tree_util.py", line 343, in tree_map
    return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/tree_util.py", line 343, in <genexpr>
    return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))
  File "/home/ant/ssrl/ssrl/brax/training/agents/ssrl/train.py", line 788, in <lambda>
    lambda x: x.reshape(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/numpy/array_methods.py", line 151, in _reshape
    return lax.reshape(a, newshape, None)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/lax/lax.py", line 919, in reshape
    return reshape_p.bind(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/core.py", line 416, in bind
    return self.bind_with_trace(find_top_trace(args), args, params)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/core.py", line 420, in bind_with_trace
    out = trace.process_primitive(self, map(trace.full_raise, args), params)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/core.py", line 921, in process_primitive
    return primitive.impl(*tracers, **params)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/dispatch.py", line 87, in apply_primitive
    outs = fun(*args)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/traceback_util.py", line 179, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/pjit.py", line 327, in cache_miss
    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/pjit.py", line 185, in _python_pjit_helper
    out_flat = pjit_p.bind(*args_flat, **params)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/core.py", line 2834, in bind
    return self.bind_with_trace(top_trace, args, params)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/core.py", line 420, in bind_with_trace
    out = trace.process_primitive(self, map(trace.full_raise, args), params)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/core.py", line 921, in process_primitive
    return primitive.impl(*tracers, **params)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/pjit.py", line 1635, in _pjit_call_impl
    return xc._xla.pjit(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/pjit.py", line 1614, in call_impl_cache_miss
    out_flat, compiled = _pjit_call_impl_python(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/pjit.py", line 1536, in _pjit_call_impl_python
    compiled = _resolve_and_lower(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py", line 2496, in compile
    executable = UnloadedMeshExecutable.from_hlo(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py", line 2995, in from_hlo
    xla_executable = _cached_compilation(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py", line 2807, in _cached_compilation
    with dispatch.log_elapsed_time(
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/contextlib.py", line 263, in helper
    return _GeneratorContextManager(func, args, kwds)
  File "/home/ant/miniforge3/envs/ssrl/lib/python3.9/contextlib.py", line 88, in __init__
    self.func, self.args, self.kwds = func, args, kwds
KeyboardInterrupt